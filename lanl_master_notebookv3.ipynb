{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Master jupyter notebook for LANL - SlimBros Team"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correctly predicting earthquakes is very important for preventing deaths and damage to infrastructure. In this competition we try to predict time left to the next laboratory earthquake based on seismic signal data. Training data represents one huge signal, but in test data we have many separate chunks, for each of which we need to predict time to failure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries\n",
    "Let's import everything we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import eli5\n",
    "import csv\n",
    "import dill\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import datetime\n",
    "import warnings\n",
    "import feather\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "import xgboost as xgb\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import HTML\n",
    "from tqdm import tqdm_notebook\n",
    "from altair.vega import v3\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn import svm, neighbors, linear_model\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, AdaBoostRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import NuSVR, SVR\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.linear_model import Ridge, RidgeCV\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold, GridSearchCV, cross_val_score, ParameterGrid, train_test_split\n",
    "from sklearn.feature_selection import GenericUnivariateSelect, SelectPercentile, SelectKBest, f_classif, mutual_info_classif, RFE\n",
    "from utils import generate_segment_start_ids, compare_methods\n",
    "from features import gpi, create_all_features_extended\n",
    "from features import gpi_new, gpii_new, gpiii_new\n",
    "\n",
    "from hyperparam_search import quick_hyperopt\n",
    "\n",
    "#Configure the environment\n",
    "%matplotlib inline\n",
    "pd.options.display.precision = 15\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "GLOBAL_SEED = 42\n",
    "BAGGING_SEED = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing altair. I use code from this great kernel: https://www.kaggle.com/notslush/altair-visualization-2018-stackoverflow-survey\n",
    "vega_url = 'https://cdn.jsdelivr.net/npm/vega@' + v3.SCHEMA_VERSION\n",
    "vega_lib_url = 'https://cdn.jsdelivr.net/npm/vega-lib'\n",
    "vega_lite_url = 'https://cdn.jsdelivr.net/npm/vega-lite@' + alt.SCHEMA_VERSION\n",
    "vega_embed_url = 'https://cdn.jsdelivr.net/npm/vega-embed@3'\n",
    "noext = \"?noext\"\n",
    "\n",
    "paths = {\n",
    "    'vega': vega_url + noext,\n",
    "    'vega-lib': vega_lib_url + noext,\n",
    "    'vega-lite': vega_lite_url + noext,\n",
    "    'vega-embed': vega_embed_url + noext\n",
    "}\n",
    "\n",
    "workaround = \"\"\"\n",
    "requirejs.config({{\n",
    "    baseUrl: 'https://cdn.jsdelivr.net/npm/',\n",
    "    paths: {}\n",
    "}});\n",
    "\"\"\"\n",
    "\n",
    "#------------------------------------------------ Defs for future rendering\n",
    "def add_autoincrement(render_func):\n",
    "    # Keep track of unique <div/> IDs\n",
    "    cache = {}\n",
    "    def wrapped(chart, id=\"vega-chart\", autoincrement=True):\n",
    "        if autoincrement:\n",
    "            if id in cache:\n",
    "                counter = 1 + cache[id]\n",
    "                cache[id] = counter\n",
    "            else:\n",
    "                cache[id] = 0\n",
    "            actual_id = id if cache[id] == 0 else id + '-' + str(cache[id])\n",
    "        else:\n",
    "            if id not in cache:\n",
    "                cache[id] = 0\n",
    "            actual_id = id\n",
    "        return render_func(chart, id=actual_id)\n",
    "    # Cache will stay outside and \n",
    "    return wrapped\n",
    "            \n",
    "@add_autoincrement\n",
    "def render(chart, id=\"vega-chart\"):\n",
    "    chart_str = \"\"\"\n",
    "    <div id=\"{id}\"></div><script>\n",
    "    require([\"vega-embed\"], function(vg_embed) {{\n",
    "        const spec = {chart};     \n",
    "        vg_embed(\"#{id}\", spec, {{defaultStyle: true}}).catch(console.warn);\n",
    "        console.log(\"anything?\");\n",
    "    }});\n",
    "    console.log(\"really...anything?\");\n",
    "    </script>\n",
    "    \"\"\"\n",
    "    return HTML(\n",
    "        chart_str.format(\n",
    "            id=id,\n",
    "            chart=json.dumps(chart) if isinstance(chart, dict) else chart.to_json(indent=None)\n",
    "        )\n",
    "    )\n",
    "\n",
    "HTML(\"\".join((\n",
    "    \"<script>\",\n",
    "    workaround.format(json.dumps(paths)),\n",
    "    \"</script>\",\n",
    ")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load/compute the necessary features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_features = False \n",
    "# The computed features are saved in an hdf file along with the time_to_failure to \n",
    "# save the time spend reading the training data and the feature computation\n",
    "#train_data_format = 'csv'\n",
    "train_data_format = 'feather'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_data(file_format):\n",
    "    \"\"\"Loads the training dataset.\"\"\"\n",
    "    print(f'Loading data from {file_format} file:', end=\"\")\n",
    "    if file_format.lower() == 'feather':\n",
    "        train_df = feather.read_dataframe('../input/train.feather')\n",
    "    else:\n",
    "        train_df = pd.read_csv('../input/train.csv', dtype={'acoustic_data': np.int16,\n",
    "                                                            'time_to_failure': np.float32})\n",
    "        feather.write_dataframe(train_df, '../input/train.feather')\n",
    "    print(\"Done\")\n",
    "    return train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = load_train_data(train_data_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_to_failure_delta = np.diff(train['time_to_failure'])\n",
    "init_times = np.where(time_to_failure_delta > 5)[0].tolist()\n",
    "print(f'There are {len(init_times)} quakes on the training set.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature generation\n",
    "- Usual aggregations: mean, std, min and max;\n",
    "- Average difference between the consequitive values in absolute and percent values;\n",
    "- Absolute min and max vallues;\n",
    "- Aforementioned aggregations for first and last 10000 and 50000 values - I think these data should be useful;\n",
    "- Max value to min value and their differencem also count of values bigger that 500 (arbitrary threshold);\n",
    "- Quantile features from this kernel: https://www.kaggle.com/andrekos/basic-feature-benchmark-with-quantiles\n",
    "- Trend features from this kernel: https://www.kaggle.com/jsaguiar/baseline-with-abs-and-trend-features\n",
    "- Rolling features from this kernel: https://www.kaggle.com/wimwim/rolling-quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#features_version = ''\n",
    "#features_version = 'v2'\n",
    "features_version = 'v3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_files_present = (os.path.isfile(f'../tmp_results/X{features_version}_tr.hdf') and \n",
    "                       os.path.isfile(f'../tmp_results/X{features_version}_test.hdf') and \n",
    "                       os.path.isfile(f'../tmp_results/y{features_version}_tr.hdf') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if saved_files_present:\n",
    "    print(f'The files containing the features ({features_version}) already exist.')\n",
    "else:\n",
    "    print(f'The files containing the features ({features_version}) do not exist')\n",
    "    print(f'In the following step, the features will be computed. This may take several hours.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (not compute_features) and saved_files_present:\n",
    "    print(f'Reading hdf files ({features_version}): ', end=\"\")\n",
    "    X_tr = pd.read_hdf(f'../tmp_results/X{features_version}_tr.hdf', 'data')\n",
    "    X_test = pd.read_hdf(f'../tmp_results/X{features_version}_test.hdf', 'data')\n",
    "    y_tr = pd.read_hdf(f'../tmp_results/y{features_version}_tr.hdf', 'data')  \n",
    "    print(\"Done\")\n",
    "else:\n",
    "    fs = 4000000 #Sampling frequency of the raw signal\n",
    "\n",
    "    #Compute features for the training data\n",
    "    segment_size = 150000\n",
    "    segment_start_ids = generate_segment_start_ids('uniform_no_jump', segment_size, train)\n",
    "    X_tr = pd.DataFrame(index=range(len(segment_start_ids)), dtype=np.float64)\n",
    "    y_tr = pd.DataFrame(index=range(len(segment_start_ids)), dtype=np.float64, columns=['time_to_failure'])\n",
    "    for idx in tqdm_notebook(range(len(segment_start_ids))):        \n",
    "        seg_id = segment_start_ids[idx]\n",
    "        seg = train.iloc[seg_id:seg_id + segment_size]\n",
    "        create_all_features_extended(idx, seg, X_tr, fs)\n",
    "        y_tr.loc[idx, 'time_to_failure'] = seg['time_to_failure'].values[-1]\n",
    "    # Sanity check\n",
    "    means_dict = {}\n",
    "    for col in X_tr.columns:\n",
    "        if X_tr[col].isnull().any():\n",
    "            print(col)\n",
    "            mean_value = X_tr.loc[X_tr[col] != -np.inf, col].mean()\n",
    "            X_tr.loc[X_tr[col] == -np.inf, col] = mean_value\n",
    "            X_tr[col] = X_tr[col].fillna(mean_value)\n",
    "            means_dict[col] = mean_value\n",
    "\n",
    "    #Compute features for the test data\n",
    "    submission = pd.read_csv('../input/sample_submission.csv', index_col='seg_id')\n",
    "    X_test = pd.DataFrame(columns=X_tr.columns, dtype=np.float64, index=submission.index)\n",
    "    for i, seg_id in enumerate(tqdm_notebook(X_test.index)):\n",
    "        seg = pd.read_csv('../input/test/' + seg_id + '.csv')\n",
    "        create_all_features_extended(seg_id, seg, X_test, fs)\n",
    "\n",
    "    # Sanity check\n",
    "    for col in X_test.columns:\n",
    "        if X_test[col].isnull().any():\n",
    "            X_test.loc[X_test[col] == -np.inf, col] = means_dict[col]\n",
    "            X_test[col] = X_test[col].fillna(means_dict[col])\n",
    "            \n",
    "    X_tr.to_hdf(f'../tmp_results/X{features_version}_tr.hdf', 'data')\n",
    "    X_test.to_hdf(f'../tmp_results/X{features_version}_test.hdf', 'data')\n",
    "    y_tr.to_hdf(f'../tmp_results/y{features_version}_tr.hdf', 'data')\n",
    "    \n",
    "    del segment_start_ids\n",
    "    del means_dict\n",
    "    del submission\n",
    "    \n",
    "    print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldata = pd.concat([X_tr, X_test])\n",
    "scaler = StandardScaler()\n",
    "alldata = pd.DataFrame(scaler.fit_transform(alldata), columns=alldata.columns)\n",
    "X_train_scaled = alldata[:X_tr.shape[0]]\n",
    "X_test_scaled = alldata[X_tr.shape[0]:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X, X_test, y, folds, params=None, model_type='lgb',\n",
    "                model=None, show_scatter=False, force_positive=False,\n",
    "                compute_feature_importance=True):\n",
    "\n",
    "    oof = np.zeros(len(X))\n",
    "    prediction = np.zeros(len(X_test))\n",
    "    scores = []\n",
    "    errors = []\n",
    "    n_fold = folds.get_n_splits()\n",
    "    feature_importance = pd.DataFrame()\n",
    "    for fold_n, (train_index, valid_index) in enumerate(folds.split(X)):\n",
    "        print('Fold', fold_n, 'started at', time.ctime())\n",
    "\n",
    "        if type(X) == np.ndarray:\n",
    "            X_train, X_valid = X[train_index], X[valid_index]\n",
    "            y_train, y_valid = y[train_index], y[valid_index]\n",
    "        else:\n",
    "            X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n",
    "            y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n",
    "           \n",
    "        if model_type == 'lgb':\n",
    "            model = lgb.LGBMRegressor(**params, n_estimators = 50000, n_jobs = -1)\n",
    "            model.fit(X_train, y_train, \n",
    "                      eval_set=[(X_train, y_train), (X_valid, y_valid)],\n",
    "                      eval_metric='mae',\n",
    "                      verbose=0,\n",
    "                      early_stopping_rounds=100)\n",
    "            \n",
    "            y_pred_valid = model.predict(X_valid)\n",
    "            if force_positive:\n",
    "                y_pred_valid = y_pred_valid.clip(min=0)\n",
    "          \n",
    "            y_pred = model.predict(X_test, num_iteration=model.best_iteration_)\n",
    "\n",
    "        if model_type == 'lgb2':\n",
    "            y_train = np.sqrt(y_train)\n",
    "            y_valid = np.sqrt(y_valid)\n",
    "\n",
    "            model = lgb.LGBMRegressor(**params, n_estimators = 50000, n_jobs = -1)\n",
    "            model.fit(X_train, y_train, \n",
    "                      eval_set=[(X_train, y_train), (X_valid, y_valid)],\n",
    "                      eval_metric='mse',\n",
    "                      verbose=0,\n",
    "                      early_stopping_rounds=100)\n",
    "            \n",
    "            y_pred_valid = model.predict(X_valid)**2\n",
    "            if force_positive:\n",
    "                y_pred_valid = y_pred_valid.clip(min=0)\n",
    "          \n",
    "            y_pred = model.predict(X_test, num_iteration=model.best_iteration_)**2\n",
    "\n",
    "        if model_type == 'xgb':\n",
    "            train_data = xgb.DMatrix(data=X_train, label=y_train, feature_names=X.columns)\n",
    "            valid_data = xgb.DMatrix(data=X_valid, label=y_valid, feature_names=X.columns)\n",
    "\n",
    "            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]\n",
    "            model = xgb.train(dtrain=train_data,\n",
    "                              num_boost_round=20000,\n",
    "                              evals=watchlist,\n",
    "                              early_stopping_rounds=100,\n",
    "                              verbose_eval=0,\n",
    "                              params=params)\n",
    "\n",
    "            y_pred_valid = model.predict(xgb.DMatrix(X_valid, feature_names=X.columns),\n",
    "                                         ntree_limit=model.best_ntree_limit)\n",
    "            if force_positive:\n",
    "                y_pred_valid = y_pred_valid.clip(min=0)\n",
    "          \n",
    "            y_pred = model.predict(xgb.DMatrix(X_test, feature_names=X.columns),\n",
    "                                   ntree_limit=model.best_ntree_limit)\n",
    "\n",
    "        if model_type == 'xgb2':\n",
    "            y_train = np.sqrt(y_train)\n",
    "            y_valid = np.sqrt(y_valid)\n",
    "\n",
    "            train_data = xgb.DMatrix(data=X_train, label=y_train, feature_names=X.columns)\n",
    "            valid_data = xgb.DMatrix(data=X_valid, label=y_valid, feature_names=X.columns)\n",
    "\n",
    "            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]\n",
    "            model = xgb.train(dtrain=train_data,\n",
    "                              num_boost_round=20000,\n",
    "                              evals=watchlist,\n",
    "                              early_stopping_rounds=100,\n",
    "                              verbose_eval=0,\n",
    "                              params=params)\n",
    "\n",
    "            y_pred_valid = model.predict(xgb.DMatrix(X_valid, feature_names=X.columns),\n",
    "                                         ntree_limit=model.best_ntree_limit)**2\n",
    "            if force_positive:\n",
    "                y_pred_valid = y_pred_valid.clip(min=0)\n",
    "          \n",
    "            y_pred = model.predict(xgb.DMatrix(X_test, feature_names=X.columns),\n",
    "                                   ntree_limit=model.best_ntree_limit)**2\n",
    "            \n",
    "        if model_type == 'gpi':\n",
    "            y_pred_valid = gpi(X_valid, activation=params['activation']).values\n",
    "            y_pred = gpi(X_test, activation=params['activation']).values\n",
    "            if force_positive:\n",
    "                y_pred_valid = y_pred_valid.clip(min=0)\n",
    "\n",
    "        if model_type == 'gpi_new':\n",
    "            y_pred_valid = gpi_new(X_valid, activation=params['activation']).values\n",
    "            y_pred = gpi_new(X_test, activation=params['activation']).values\n",
    "            if force_positive:\n",
    "                y_pred_valid = y_pred_valid.clip(min=0)\n",
    "            \n",
    "        if model_type == 'gpii_new':\n",
    "            y_pred_valid = gpii_new(X_valid, activation=params['activation']).values\n",
    "            y_pred = gpii_new(X_test, activation=params['activation']).values\n",
    "            if force_positive:\n",
    "                y_pred_valid = y_pred_valid.clip(min=0)\n",
    "                \n",
    "        if model_type == 'gpiii_new':\n",
    "            y_pred_valid = gpiii_new(X_valid, activation=params['activation']).values\n",
    "            y_pred = gpiii_new(X_test, activation=params['activation']).values\n",
    "            if force_positive:\n",
    "                y_pred_valid = y_pred_valid.clip(min=0)\n",
    "                \n",
    "        oof[valid_index] = y_pred_valid.reshape(-1,)\n",
    "        scores.append(mean_absolute_error(y_valid, y_pred_valid))\n",
    "        errors.append(y_valid['time_to_failure'].values-y_pred_valid)\n",
    "        \n",
    "        prediction += y_pred    \n",
    "        \n",
    "        if (model_type == 'lgb' or model_type == 'lgb2') and compute_feature_importance:\n",
    "            # feature importance\n",
    "            fold_importance = pd.DataFrame()\n",
    "            fold_importance['feature'] = X.columns\n",
    "            fold_importance['importance'] = model.feature_importances_\n",
    "            fold_importance['fold'] = fold_n + 1\n",
    "            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n",
    "\n",
    "    prediction /= n_fold\n",
    "    errors = [y for x in errors for y in x]\n",
    "    \n",
    "    if force_positive:\n",
    "        prediction = prediction.clip(min=0)    \n",
    "    \n",
    "    if show_scatter:\n",
    "        fig, axis = plt.subplots(1, 3, figsize=(15,5))\n",
    "        ax1, ax2, ax3 = axis\n",
    "        \n",
    "        ax1.scatter(y, oof, color='brown')\n",
    "        ax1.plot([(0, 0), (20, 20)], [(0, 0), (20, 20)], color='blue')\n",
    "        ax1.set_xlabel('actual')\n",
    "        ax1.set_ylabel('predicted')\n",
    "\n",
    "        ax2.plot(y, color='blue', label='y_train')\n",
    "        ax2.plot(oof, color='orange')\n",
    "        ax2.plot(y, color='blue', label='y_train')\n",
    "        ax2.plot(oof, color='orange')\n",
    "        ax2.set_xlabel('train index')\n",
    "        ax2.set_ylabel('time to failure')\n",
    "\n",
    "        ax3.hist(errors, bins='auto', alpha=0.5, density=True)\n",
    "        ax3.set_xlabel('error')\n",
    "        ax3.set_ylabel('count')\n",
    "        ax3.set_xlim(-10, 10)\n",
    "\n",
    "    print(f'CV mean score: {np.mean(scores):.4f}, std: {np.std(scores):.4f}.')\n",
    "    \n",
    "    if (model_type == 'lgb' or model_type == 'lgb2') and compute_feature_importance:\n",
    "        feature_importance['importance'] /= n_fold\n",
    "        return oof, prediction, scores, feature_importance\n",
    "    else:\n",
    "        return oof, prediction, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fold = 5\n",
    "folds_models = KFold(n_splits=n_fold, shuffle=True, random_state=GLOBAL_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a few different models and submit the one with the best validation score. The predicted values in the following plots are using a out-of-fold scheme."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LGBM (Gradient Boosting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#TODO: optimize params\n",
    "params_lgb = {\n",
    "    'objective': 'huber',\n",
    "    'boosting': 'gbdt',\n",
    "    'verbosity': -1,\n",
    "    'num_leaves': 12,\n",
    "    'min_data_in_leaf': 40,\n",
    "    'max_depth': 8,\n",
    "    'learning_rate': 0.005,\n",
    "    'bagging_freq': 4,\n",
    "    'bagging_fraction': 0.6,\n",
    "    'bagging_seed': BAGGING_SEED,\n",
    "    'random_seed': GLOBAL_SEED,\n",
    "    'metric': 'mae',\n",
    "    'reg_alpha': 0.47777777777777775,\n",
    "    'reg_lambda': 0.47777777777777775\n",
    "}\n",
    "oof_lgb, prediction_lgb, scores_lgb, feature_importance_lgb = train_model(X=X_train_scaled,\n",
    "                                                                          X_test=X_test_scaled,\n",
    "                                                                          y=y_tr,\n",
    "                                                                          folds=folds_models,\n",
    "                                                                          params=params_lgb,\n",
    "                                                                          model_type='lgb',\n",
    "                                                                          show_scatter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_lgb_opt = quick_hyperopt(X_train_scaled, y_tr, num_evals=100, eval_metric='mae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_lgb_opt, prediction_lgb_opt, scores_lgb_opt, feature_importance_lgb_opt = train_model(X=X_train_scaled,\n",
    "                                                                          X_test=X_test_scaled,\n",
    "                                                                          y=y_tr,\n",
    "                                                                          folds=folds_models,\n",
    "                                                                          params=params_lgb_opt,\n",
    "                                                                          model_type='lgb',\n",
    "                                                                          show_scatter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_lgb = {\n",
    "    'objective': 'huber',\n",
    "    'boosting': 'gbdt',\n",
    "    'verbosity': -1,\n",
    "    'num_leaves': 12,\n",
    "    'min_data_in_leaf': 40,\n",
    "    'max_depth': 8,\n",
    "    'learning_rate': 0.005,\n",
    "    'bagging_freq': 4,\n",
    "    'bagging_fraction': 0.6,\n",
    "    'bagging_seed': BAGGING_SEED,\n",
    "    'random_seed': GLOBAL_SEED,\n",
    "    'metric': 'mse',\n",
    "    'reg_alpha': 0.47777777777777775,\n",
    "    'reg_lambda': 0.47777777777777775\n",
    "}\n",
    "oof_lgb, prediction_lgb, scores_lgb, feature_importance_lgb = train_model(X=X_train_scaled,\n",
    "                                                                          X_test=X_test_scaled,\n",
    "                                                                          y=y_tr,\n",
    "                                                                          folds=folds_models,\n",
    "                                                                          params=params_lgb,\n",
    "                                                                          model_type='lgb2',\n",
    "                                                                          show_scatter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_lgb_opt = quick_hyperopt(X_train_scaled, y_tr, num_evals=100, eval_metric='mae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost (Gradient Boosting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#TODO: optimize params\n",
    "params_xgb = {\n",
    "    'eta': 0.01,\n",
    "    'max_depth': 6,\n",
    "    'subsample': 0.5,\n",
    "    'objective': 'reg:linear',\n",
    "    'eval_metric': 'mae',\n",
    "    'silent': True,\n",
    "    'nthread': 32\n",
    "}\n",
    "oof_xgb, prediction_xgb, scores_xgb = train_model(X=X_train_scaled,\n",
    "                                                  X_test=X_test_scaled,\n",
    "                                                  y=y_tr,\n",
    "                                                  folds=folds_models,\n",
    "                                                  params=params_xgb,\n",
    "                                                  model_type='xgb',\n",
    "                                                  show_scatter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: optimize params\n",
    "params_xgb = {\n",
    "    'eta': 0.01,\n",
    "    'max_depth': 6,\n",
    "    'subsample': 0.5,\n",
    "    'objective': 'reg:linear',\n",
    "    'eval_metric': 'rmse',\n",
    "    'silent': True,\n",
    "    'nthread': 32\n",
    "}\n",
    "oof_xgb, prediction_xgb, scores_xgb = train_model(X=X_train_scaled,\n",
    "                                                  X_test=X_test_scaled,\n",
    "                                                  y=y_tr,\n",
    "                                                  folds=folds_models,\n",
    "                                                  params=params_xgb,\n",
    "                                                  model_type='xgb2',\n",
    "                                                  show_scatter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Genetic Program Models (borrowed from Kaggle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_gpi = {\n",
    "    'activation':'atan'\n",
    "}\n",
    "oof_gpi, prediction_gpi, scores_gpi = train_model(X=X_train_scaled,\n",
    "                                                  X_test=X_test_scaled,\n",
    "                                                  y=y_tr,\n",
    "                                                  folds=folds_models,\n",
    "                                                  params=params_gpi,\n",
    "                                                  model_type='gpi',\n",
    "                                                  show_scatter=True)\n",
    "\n",
    "params_gpi_new = {\n",
    "    'activation':'atan'\n",
    "}\n",
    "oof_gpi_new, prediction_gpi_new, scores_gpi_new = train_model(X=X_train_scaled,\n",
    "                                                              X_test=X_test_scaled,\n",
    "                                                              y=y_tr,\n",
    "                                                              folds=folds_models,\n",
    "                                                              params=params_gpi_new,\n",
    "                                                              model_type='gpi_new',\n",
    "                                                              show_scatter=True)\n",
    "\n",
    "params_gpii_new = {\n",
    "    'activation':'atan'\n",
    "}\n",
    "oof_gpii_new, prediction_gpii_new, scores_gpii_new = train_model(X=X_train_scaled,\n",
    "                                                                 X_test=X_test_scaled,\n",
    "                                                                 y=y_tr,\n",
    "                                                                 folds=folds_models,\n",
    "                                                                 params=params_gpii_new,\n",
    "                                                                 model_type='gpii_new',\n",
    "                                                                 show_scatter=True)\n",
    "\n",
    "params_gpiii_new = {\n",
    "    'activation':'atan'\n",
    "}\n",
    "oof_gpiii_new, prediction_gpiii_new, scores_gpiii_new = train_model(X=X_train_scaled,\n",
    "                                                                    X_test=X_test_scaled,\n",
    "                                                                    y=y_tr,\n",
    "                                                                    folds=folds_models,\n",
    "                                                                    params=params_gpiii_new,\n",
    "                                                                    model_type='gpiii_new',\n",
    "                                                                    show_scatter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 8));\n",
    "scores_df = pd.DataFrame()\n",
    "\n",
    "try:\n",
    "    scores_df['LGB'] = scores_lgb\n",
    "except NameError:\n",
    "    print('LGB not computed')\n",
    "try:\n",
    "    scores_df['LGB-opt'] = scores_lgb_opt\n",
    "except NameError:\n",
    "    print('LGB-opt not computed')\n",
    "try:\n",
    "    scores_df['XGB'] = scores_xgb\n",
    "except NameError:\n",
    "    print('XGB not computed')\n",
    "try:\n",
    "    scores_df['GPI'] = scores_gpi\n",
    "except NameError:\n",
    "    print('GPI not computed')\n",
    "try:\n",
    "    scores_df['GPI_new'] = scores_gpi_new\n",
    "except NameError:\n",
    "    print('GPI_new not computed')\n",
    "try:\n",
    "    scores_df['GPII_new'] = scores_gpii_new\n",
    "except NameError:\n",
    "    print('GPII_new not computed')    \n",
    "try:\n",
    "    scores_df['GPIII_new'] = scores_gpiii_new\n",
    "except NameError:\n",
    "    print('GPIII_new not computed')     \n",
    "    \n",
    "ax = sns.boxplot(data=scores_df.reindex(scores_df.mean().sort_values().index, axis=1));\n",
    "ax = sns.swarmplot(data=scores_df.reindex(scores_df.mean().sort_values().index, axis=1), color=\".25\")\n",
    "\n",
    "plt.xticks(rotation=45);\n",
    "plt.xlabel('Method');\n",
    "plt.ylabel('CV Score');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping highly correlated features\n",
    "Due to the huge number of features there are certainly some highly correlated features, let's try droping them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled_uncorr = X_train_scaled.copy()\n",
    "X_test_scaled_uncorr = X_test_scaled.copy()\n",
    "# https://chrisalbon.com/machine_learning/feature_selection/drop_highly_correlated_features/\n",
    "corr_matrix = X_train_scaled_uncorr.corr().abs()\n",
    "# Select upper triangle of correlation matrix\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "# Find index of feature columns with correlation greater than 0.99\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.99)]\n",
    "X_train_scaled_uncorr = X_train_scaled_uncorr.drop(to_drop, axis=1)\n",
    "X_test_scaled_uncorr = X_test_scaled_uncorr.drop(to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Optimize\n",
    "params_lgb_uncorr = {\n",
    "    'bagging_fraction': 0.6,\n",
    "    'bagging_freq': 4,\n",
    "    'bagging_seed': BAGGING_SEED,\n",
    "    'boosting': 'gbdt',\n",
    "    'reg_alpha': 0.47777777777777775,\n",
    "    'reg_lambda': 0.47777777777777775,\n",
    "    'learning_rate': 0.005,\n",
    "    'max_depth': 8,\n",
    "    'metric': 'mae',\n",
    "    'min_data_in_leaf': 40,\n",
    "    'num_leaves': 12,    \n",
    "    'objective': 'huber',\n",
    "    'verbosity': -1,\n",
    "    'random_seed': GLOBAL_SEED\n",
    "}\n",
    "oof_lgb_uncorr, prediction_lgb_uncorr, scores_lgb_uncorr = train_model(\n",
    "    X=X_train_scaled_uncorr,\n",
    "    X_test=X_test_scaled_uncorr,\n",
    "    y=y_tr,\n",
    "    folds=folds_models,\n",
    "    params=params_lgb_uncorr,\n",
    "    model_type='lgb',\n",
    "    compute_feature_importance=False,\n",
    "    show_scatter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_lgb_uncorr_opt = quick_hyperopt(X_train_scaled_uncorr, y_tr, num_evals=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_lgb_uncorr_opt, prediction_lgb_uncorr_opt, scores_lgb_uncorr_opt = train_model(\n",
    "    X=X_train_scaled_uncorr,\n",
    "    X_test=X_test_scaled_uncorr,\n",
    "    y=y_tr,\n",
    "    folds=folds_models,\n",
    "    params=params_lgb_uncorr_opt,\n",
    "    model_type='lgb',\n",
    "    compute_feature_importance=False,\n",
    "    show_scatter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ELI5 and permutation importance\n",
    "ELI5 is a package with provides explanations for ML models. It can do this not only for linear models, but also for tree based like Random Forest or lightgbm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recompute_eli5 = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_lgb = {\n",
    "    'objective': 'huber',\n",
    "    'boosting': 'gbdt',\n",
    "    'verbosity': -1,\n",
    "    'num_leaves': 12,\n",
    "    'min_data_in_leaf': 40,\n",
    "    'max_depth': 8,\n",
    "    'learning_rate': 0.005,\n",
    "    'bagging_freq': 4,\n",
    "    'bagging_fraction': 0.6,\n",
    "    'bagging_seed': BAGGING_SEED,\n",
    "    'random_seed': GLOBAL_SEED,\n",
    "    'metric': 'mae',\n",
    "    'reg_alpha': 0.47777777777777775,\n",
    "    'reg_lambda': 0.47777777777777775\n",
    "}\n",
    "if recompute_eli5:\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_train_scaled, y_tr, test_size=0.2)\n",
    "    model = lgb.LGBMRegressor(**params_lgb, n_estimators = 50000, n_jobs = -1, verbose=-1)\n",
    "    model.fit(X_train,\n",
    "              y_train, \n",
    "              eval_set=[(X_train, y_train), (X_valid, y_valid)],\n",
    "              eval_metric='mae',\n",
    "              verbose=10000,\n",
    "              early_stopping_rounds=200)\n",
    "    perm = eli5.sklearn.PermutationImportance(model, random_state=1).fit(X_train, y_train)\n",
    "    #eli5.show_weights(perm, top=200, feature_names=X_train_scaled.columns.values.tolist())\n",
    "    top_num_features = 150\n",
    "    top_features = [i for i in eli5.formatters.as_dataframe.explain_weights_df(model).feature if 'BIAS' not in i][:top_num_features]\n",
    "\n",
    "    top_features_df = pd.DataFrame(data=top_features)\n",
    "    top_features_df.columns = ['top_features']\n",
    "    top_features_df.to_csv(f'../tmp_results/top_features_eli5.csv', index=False)\n",
    "else:\n",
    "    top_features = pd.read_csv(f'../tmp_results/top_features_eli5.csv')['top_features'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_lgb_eli5_top, prediction_lgb_eli5_top, scores_lgb_eli5_top = train_model(\n",
    "    X=X_train_scaled[top_features],\n",
    "    X_test=X_test_scaled[top_features],\n",
    "    y=y_tr,\n",
    "    folds=folds_models,\n",
    "    params=params_lgb,\n",
    "    model_type='lgb',\n",
    "    compute_feature_importance=False,\n",
    "    show_scatter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_lgb_opt = quick_hyperopt(X_train_scaled[top_features], y_tr, num_evals=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_lgb_eli5_top_opt, prediction_lgb_eli5_top_opt, scores_lgb_eli5_top_opt = train_model(\n",
    "    X=X_train_scaled[top_features],\n",
    "    X_test=X_test_scaled[top_features],\n",
    "    y=y_tr,\n",
    "    folds=folds_models,\n",
    "    params=params_lgb_opt,\n",
    "    model_type='lgb',\n",
    "    compute_feature_importance=False,\n",
    "    show_scatter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluation = pd.DataFrame(dtype=np.float64)\n",
    "#submission = pd.read_csv('../input/sample_submission.csv', index_col='seg_id')\n",
    "#submission['time_to_failure'] = prediction_lgb_eli5_top\n",
    "#\n",
    "#submission.to_csv('../output/submission_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_lgb_eli5_top, prediction_lgb_eli5_top, scores_lgb_eli5_top = train_model(\n",
    "    X=X_train_scaled[top_features],\n",
    "    X_test=X_test_scaled[top_features],\n",
    "    y=y_tr,\n",
    "    folds=folds_models,\n",
    "    params=params_lgb,\n",
    "    model_type='lgb2',\n",
    "    compute_feature_importance=False,\n",
    "    show_scatter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluation = pd.DataFrame(dtype=np.float64)\n",
    "#submission = pd.read_csv('../input/magic_submission.csv', index_col='seg_id')\n",
    "#prediction_magic = submission['time_to_failure']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction_blend = (prediction_magic + prediction_lgb_eli5_top) / 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluation = pd.DataFrame(dtype=np.float64)\n",
    "#submission = pd.read_csv('../input/sample_submission.csv', index_col='seg_id')\n",
    "#submission['time_to_failure'] = prediction_blend\n",
    "#submission.to_csv('../output/submission_magic_and_lgb_eli5_top.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: optimize params\n",
    "params_xgb = {\n",
    "    'eta': 0.01,\n",
    "    'max_depth': 6,\n",
    "    'subsample': 0.5,\n",
    "    'objective': 'reg:linear',\n",
    "    'eval_metric': 'rmse',\n",
    "    'silent': True,\n",
    "    'nthread': 32\n",
    "}\n",
    "oof_xgb_eli5_top, prediction_xgb_eli5_top, scores_xgb_eli5_top = train_model(X=X_train_scaled[top_features],\n",
    "                                                  X_test=X_test_scaled[top_features],\n",
    "                                                  y=y_tr,\n",
    "                                                  folds=folds_models,\n",
    "                                                  params=params_xgb,\n",
    "                                                  model_type='xgb2',\n",
    "                                                  show_scatter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = pd.DataFrame(dtype=np.float64)\n",
    "submission = pd.read_csv('../input/sample_submission.csv', index_col='seg_id')\n",
    "submission['time_to_failure'] = prediction_xgb_eli5_top\n",
    "submission.to_csv('../output/submission_xgb_eli5_top_rmse.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recompute_eli5_uncorr = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_lgb_uncorr = {\n",
    "    'objective': 'huber',\n",
    "    'boosting': 'gbdt',\n",
    "    'verbosity': -1,\n",
    "    'num_leaves': 12,\n",
    "    'min_data_in_leaf': 40,\n",
    "    'max_depth': 8,\n",
    "    'learning_rate': 0.005,\n",
    "    'bagging_freq': 4,\n",
    "    'bagging_fraction': 0.6,\n",
    "    'bagging_seed': BAGGING_SEED,\n",
    "    'random_seed': GLOBAL_SEED,\n",
    "    'metric': 'mae',\n",
    "    'reg_alpha': 0.47777777777777775,\n",
    "    'reg_lambda': 0.47777777777777775\n",
    "}\n",
    "if recompute_eli5_uncorr:\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_train_scaled_uncorr, y_tr, test_size=0.2)\n",
    "    model = lgb.LGBMRegressor(**params_lgb_uncorr, n_estimators = 50000, n_jobs = -1, verbose=-1)\n",
    "    model.fit(X_train,\n",
    "              y_train, \n",
    "              eval_set=[(X_train, y_train), (X_valid, y_valid)],\n",
    "              eval_metric='mae',\n",
    "              verbose=10000,\n",
    "              early_stopping_rounds=200)\n",
    "    perm = eli5.sklearn.PermutationImportance(model, random_state=1).fit(X_train, y_train)\n",
    "    #eli5.show_weights(perm, top=200, feature_names=X_train_scaled.columns.values.tolist())\n",
    "    top_num_features = 150\n",
    "    top_features = [i for i in eli5.formatters.as_dataframe.explain_weights_df(model).feature if 'BIAS' not in i][:top_num_features]\n",
    "\n",
    "    top_features_df = pd.DataFrame(data=top_features)\n",
    "    top_features_df.columns = ['top_features']\n",
    "    top_features_df.to_csv(f'../tmp_results/top_features_eli5_uncorr.csv', index=False)\n",
    "else:\n",
    "    top_features = pd.read_csv(f'../tmp_results/top_features_eli5_uncorr.csv')['top_features'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_lgb_eli5_uncorr_top, prediction_lgb_eli5_uncorr_top, scores_lgb_eli5_uncorr_top = train_model(\n",
    "    X=X_train_scaled_uncorr[top_features],\n",
    "    X_test=X_test_scaled_uncorr[top_features],\n",
    "    y=y_tr,\n",
    "    folds=folds_models,\n",
    "    params=params_lgb_uncorr,\n",
    "    model_type='lgb',\n",
    "    compute_feature_importance=False,\n",
    "    show_scatter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_lgb_uncorr_opt = quick_hyperopt(X_train_scaled_uncorr[top_features], y_tr, num_evals=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "oof_lgb_eli5_uncorr_top_opt, prediction_lgb_eli5_uncorr_top_opt, scores_lgb_eli5_uncorr_top_opt = train_model(\n",
    "    X=X_train_scaled_uncorr[top_features],\n",
    "    X_test=X_test_scaled_uncorr[top_features],\n",
    "    y=y_tr,\n",
    "    folds=folds_models,\n",
    "    params=params_lgb_uncorr_opt,\n",
    "    model_type='lgb',\n",
    "    compute_feature_importance=False,\n",
    "    show_scatter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RFE (Feature ranking with recursive feature elimination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recompute_rfe_analysis = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if recompute_rfe_analysis:\n",
    "    scores_dict = {'rfe_score': [], 'n_features': []}\n",
    "    total_num_features = X_train_scaled_uncorr.shape[1]\n",
    "    rfe_feat = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 15, 20, 30, 40, 50,\n",
    "                110, 150, 210, 250, 310, 350, 410, 510,\n",
    "                610, 710, 810, 850, 910, 950, 1010, 1113]\n",
    "    for i in tqdm_notebook(rfe_feat):\n",
    "        model = lgb.LGBMRegressor(**params_lgb_uncorr, n_estimators = 50000, n_jobs = -1, verbose=-1)\n",
    "        s1 = RFE(model, i, step=100)\n",
    "        #s1 = SelectPercentile(f_classif, percentile=i)\n",
    "        #s2 = SelectPercentile(mutual_info_classif, percentile=i)\n",
    "        #s1 = SelectKBest(f_classif, k=i)\n",
    "        X_train1 = s1.fit_transform(X_train_scaled_uncorr, y_tr.values.astype(int))\n",
    "        X_test1 = s1.transform(X_test_scaled_uncorr)    \n",
    "        oof, prediction, scores = train_model(X=X_train1,\n",
    "                                              X_test=X_test1,\n",
    "                                              y=y_tr.values.reshape(-1, ),\n",
    "                                              params=params_lgb_uncorr,\n",
    "                                              folds=folds_models,\n",
    "                                              model_type='lgb',\n",
    "                                              compute_feature_importance=False)\n",
    "        scores_dict['rfe_score'].append(np.mean(scores))    \n",
    "        scores_dict['n_features'].append(X_train1.shape[1])\n",
    "        scores_df = pd.DataFrame(scores_dict)\n",
    "        scores_df.to_csv(f'../tmp_results/rfe_scores.csv', index=False)\n",
    "else:\n",
    "    scores_df = pd.read_csv(f'../tmp_results/rfe_scores.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scores_df = scores_df.melt(id_vars=['n_features'], value_vars=['rfe_score'], var_name='metric', value_name='mae')\n",
    "max_value = scores_df['mae'].max() * 1.01\n",
    "min_value = scores_df['mae'].min() * 0.99\n",
    "render(alt.Chart(scores_df).mark_line().encode(\n",
    "    y=alt.Y('mae:Q', scale=alt.Scale(domain=(min_value, max_value))),\n",
    "    x='n_features:Q',\n",
    "    color='metric:N',\n",
    "    tooltip=['metric:N', 'n:O', 'mae:Q']\n",
    ").properties(\n",
    "    title='Top N features by RFE vs CV'\n",
    ").interactive())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_lgb_rfe_uncorr_top = {\n",
    "    'objective': 'huber',\n",
    "    'boosting': 'gbdt',\n",
    "    'verbosity': -1,\n",
    "    'num_leaves': 12,\n",
    "    'min_data_in_leaf': 40,\n",
    "    'max_depth': 8,\n",
    "    'learning_rate': 0.005,\n",
    "    'bagging_freq': 4,\n",
    "    'bagging_fraction': 0.6,\n",
    "    'bagging_seed': BAGGING_SEED,\n",
    "    'random_seed': GLOBAL_SEED,\n",
    "    'metric': 'mae',\n",
    "    'reg_alpha': 0.47777777777777775,\n",
    "    'reg_lambda': 0.47777777777777775\n",
    "}\n",
    "top_num_features = 150\n",
    "model = lgb.LGBMRegressor(**params_lgb_rfe_uncorr_top, n_estimators = 50000, n_jobs = -1, verbose=-1)\n",
    "s1 = RFE(model, top_num_features, step=100)\n",
    "X_train1 = s1.fit_transform(X_train_scaled_uncorr, y_tr.values.astype(int))\n",
    "X_test1 = s1.transform(X_test_scaled_uncorr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_lgb_rfe_uncorr_top, prediction_lgb_rfe_uncorr_top, scores_lgb_rfe_uncorr_top = train_model(\n",
    "    X=X_train1,\n",
    "    X_test=X_test1,\n",
    "    y=y_tr.values.reshape(-1, ),\n",
    "    params=params_lgb_rfe_uncorr_top,\n",
    "    folds=folds_models,\n",
    "    model_type='lgb',\n",
    "    compute_feature_importance=False,\n",
    "    show_scatter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that a sligh modification on the objective function may yield important gains. The idea is that using sqrt(```time_to_failure```) and using MSE gives much better performance on MAE once squaring the predictions. \n",
    "https://www.kaggle.com/c/LANL-Earthquake-Prediction/discussion/92440"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model compatison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8));\n",
    "scores_df = pd.DataFrame()\n",
    "\n",
    "try:\n",
    "    scores_df['LGB'] = scores_lgb\n",
    "except NameError:\n",
    "    print('LGB not computed')\n",
    "try:\n",
    "    scores_df['LGB-opt'] = scores_lgb_opt\n",
    "except NameError:\n",
    "    print('LGB-opt not computed')\n",
    "try:\n",
    "    scores_df['LGB_uncorr'] = scores_lgb_uncorr\n",
    "except NameError:\n",
    "    print('LGB_uncorr not computed')\n",
    "try:\n",
    "    scores_df['LGB_uncorr-opt'] = scores_lgb_uncorr_opt\n",
    "except NameError:\n",
    "    print('LGB_uncorr-opt not computed')\n",
    "try:\n",
    "    scores_df['LGB_eli5_top'] = scores_lgb_eli5_top\n",
    "except NameError:\n",
    "    print('LGB_eli5_top not computed')\n",
    "try:\n",
    "    scores_df['LGB_eli5_top-opt'] = scores_lgb_eli5_top_opt\n",
    "except NameError:\n",
    "    print('LGB_eli5_top-opt not computed')\n",
    "try:\n",
    "    scores_df['LGB_eli5_uncorr_top'] = scores_lgb_eli5_uncorr_top\n",
    "except NameError:\n",
    "    print('LGB_eli5_uncorr_top not computed')\n",
    "try:\n",
    "    scores_df['LGB_eli5_uncorr_top-opt'] = scores_lgb_eli5_uncorr_top_opt\n",
    "except NameError:\n",
    "    print('LGB_eli5_uncorr_top-opt not computed')\n",
    "try:\n",
    "    scores_df['LGB_rfe_uncorr_top'] = scores_lgb_rfe_uncorr_top\n",
    "except NameError:\n",
    "    print('LGB_rfe_uncorr_top not computed')    \n",
    "try:\n",
    "    scores_df['XGB'] = scores_xgb\n",
    "except NameError:\n",
    "    print('XGB not computed')\n",
    "try:\n",
    "    scores_df['GPI'] = scores_gpi\n",
    "except NameError:\n",
    "    print('GPI not computed')\n",
    "try:\n",
    "    scores_df['GPI_new'] = scores_gpi_new\n",
    "except NameError:\n",
    "    print('GPI_new not computed')\n",
    "try:\n",
    "    scores_df['GPII_new'] = scores_gpii_new\n",
    "except NameError:\n",
    "    print('GPII_new not computed')    \n",
    "try:\n",
    "    scores_df['GPIII_new'] = scores_gpiii_new\n",
    "except NameError:\n",
    "    print('GPIII_new not computed')\n",
    "    \n",
    "ax = sns.boxplot(data=scores_df.reindex(scores_df.mean().sort_values().index, axis=1));\n",
    "#ax.set(yscale=\"log\")\n",
    "\n",
    "plt.xticks(rotation=45);\n",
    "plt.xlabel('Method');\n",
    "plt.ylabel('Score');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_blend_gp = (prediction_gpi +\n",
    "                    prediction_gpi_new + \n",
    "                    prediction_gpii_new + \n",
    "                    prediction_gpiii_new) / 4\n",
    "oof_blend_gp = (oof_gpi +\n",
    "             oof_gpi_new + \n",
    "             oof_gpii_new + \n",
    "             oof_gpiii_new) / 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = pd.DataFrame(dtype=np.float64)\n",
    "submission = pd.read_csv('../input/sample_submission.csv', index_col='seg_id')\n",
    "submission['time_to_failure'] = prediction_lgb_eli5_top\n",
    "submission.to_csv('../output/submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dill.dump_session('notebook_env.db')\n",
    "#dill.load_session('notebook_env.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
