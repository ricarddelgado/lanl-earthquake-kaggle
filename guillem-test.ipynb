{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sample_submission.csv', 'test', 'train.csv']\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Traceback (most recent call last):\n  File \"/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\n    from tensorflow.python.pywrap_tensorflow_internal import *\n  File \"/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\n    _pywrap_tensorflow_internal = swig_import_helper()\n  File \"/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n  File \"/usr/local/anaconda3/lib/python3.6/imp.py\", line 243, in load_module\n    return load_dynamic(name, filename, file)\n  File \"/usr/local/anaconda3/lib/python3.6/imp.py\", line 343, in load_dynamic\n    return _load(spec)\nImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory\n\n\nFailed to load the native TensorFlow runtime.\n\nSee https://www.tensorflow.org/install/errors\n\nfor some common reasons and solutions.  Include the entire stack trace\nabove this error message when asking for help.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m   \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpywrap_tensorflow_internal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m   \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpywrap_tensorflow_internal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_mod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0m_pywrap_tensorflow_internal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mswig_import_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mswig_import_helper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36mswig_import_helper\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m                 \u001b[0m_mod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_pywrap_tensorflow_internal'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.6/imp.py\u001b[0m in \u001b[0;36mload_module\u001b[0;34m(name, file, filename, details)\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mload_dynamic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mtype_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mPKG_DIRECTORY\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.6/imp.py\u001b[0m in \u001b[0;36mload_dynamic\u001b[0;34m(name, path, file)\u001b[0m\n\u001b[1;32m    342\u001b[0m             name=name, loader=loader, origin=path)\n\u001b[0;32m--> 343\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: libcublas.so.9.0: cannot open shared object file: No such file or directory",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-9abc5294d6a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# pylint: disable=g-bad-import-order\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m  \u001b[0;31m# pylint: disable=unused-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow/python/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcomponent_api_helper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msome\u001b[0m \u001b[0mcommon\u001b[0m \u001b[0mreasons\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msolutions\u001b[0m\u001b[0;34m.\u001b[0m  \u001b[0mInclude\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mentire\u001b[0m \u001b[0mstack\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m above this error message when asking for help.\"\"\" % traceback.format_exc()\n\u001b[0;32m---> 74\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;31m# pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: Traceback (most recent call last):\n  File \"/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\n    from tensorflow.python.pywrap_tensorflow_internal import *\n  File \"/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\n    _pywrap_tensorflow_internal = swig_import_helper()\n  File \"/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n  File \"/usr/local/anaconda3/lib/python3.6/imp.py\", line 243, in load_module\n    return load_dynamic(name, filename, file)\n  File \"/usr/local/anaconda3/lib/python3.6/imp.py\", line 343, in load_dynamic\n    return _load(spec)\nImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory\n\n\nFailed to load the native TensorFlow runtime.\n\nSee https://www.tensorflow.org/install/errors\n\nfor some common reasons and solutions.  Include the entire stack trace\nabove this error message when asking for help."
     ]
    }
   ],
   "source": [
    "#This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output.\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "from tensorflow.contrib import rnn\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "class Dataset_Manager():\n",
    "    def __init__(self, dataset_split, TRAIN_VAL_SPLIT, sequence_length, batch_size):\n",
    "        self.downsample = int(150000/sequence_length)\n",
    "        print(\"Sequence lentgh of {} where the original length of 150.000 has been downsampled of: {}\".format(sequence_length, self.downsample))\n",
    "        train_samples = int(round(dataset_split*TRAIN_VAL_SPLIT))\n",
    "        validation_samples = int(dataset_split-train_samples)\n",
    "        print(\"The dataset contains {} train samples and {} validation samples which is a {} ratio\".format(\n",
    "            train_samples, validation_samples, TRAIN_VAL_SPLIT))\n",
    "\n",
    "        index = np.arange(dataset_split)\n",
    "        np.random.shuffle(index)\n",
    "\n",
    "        self.validation_split = index[validation_samples:]\n",
    "        self.train_split = index[:train_samples]\n",
    "        self.seq_length = sequence_length\n",
    "\n",
    "        self.total_batches_train = math.ceil(len(self.train_split)/batch_size)\n",
    "        self.total_batches_validation = math.ceil(len(self.validation_split)/batch_size)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def generate_batch(self):\n",
    "\n",
    "        np.random.shuffle(self.train_split)\n",
    "\n",
    "        np.random.shuffle(self.validation_split)\n",
    "\n",
    "    def get_next_batch(self, data, labels, train, step):\n",
    "        minibatch = []\n",
    "        minilabels = []\n",
    "        if train:\n",
    "            for i in range(self.batch_size):\n",
    "                start_index = int(self.train_split[(self.batch_size*step)+i]*self.seq_length)\n",
    "                train_batch = data[start_index:(start_index+150000):self.downsample]\n",
    "                train_batch = np.expand_dims(train_batch, -1)\n",
    "\n",
    "                # TODO: Review properly which label to use\n",
    "                train_y = labels[start_index+150000-1]\n",
    "                \n",
    "                minibatch.append(train_batch)\n",
    "                print(train_y)\n",
    "                print(train_y.shape)\n",
    "                print(train_batch.shape)\n",
    "                print(\"---\")\n",
    "                minilabels.append(train_y)\n",
    "        else:\n",
    "            for i in range(self.batch_size):\n",
    "                start_index = int(self.validation_split[(self.batch_size*step)+i]*self.seq_length)\n",
    "                val_batch = data[start_index:(start_index+150000):self.downsample]\n",
    "                val_y = labels[start_index+self.seq_length-1]\n",
    "                #val_batch = np.expand_dims(val_batch, -1)\n",
    "                minibatch.append(val_batch)\n",
    "                minilabels.append(val_y)\n",
    "        print(minilabels)\n",
    "        #minilabels = np.expand_dims(minilabels, -1)\n",
    "        print(minilabels)\n",
    "        return minibatch, minilabels\n",
    "        \n",
    "class model(object):\n",
    "\n",
    "    def __init__(self, x, seq_lenght, skip_layer, is_training, whichmodel):\n",
    "        # define constants\n",
    "        # unrolled through 49 time steps\n",
    "        self.seq_lenght = seq_lenght\n",
    "\n",
    "        # hidden LSTM units\n",
    "        self.lstm_units = 512\n",
    "\n",
    "\n",
    "\n",
    "        self.SKIP_LAYER = skip_layer\n",
    "\n",
    "        self.is_training = is_training\n",
    "\n",
    "\n",
    "        if whichmodel == 'LSTM':\n",
    "            self.y = self.create_lstmmodel(x, is_training)\n",
    "        else:\n",
    "            print(\"Error creating the model\")\n",
    "\n",
    "    def create_lstmmodel(self,x, is_training):\n",
    "        with tf.variable_scope('Single_LSTM', reuse=tf.AUTO_REUSE):\n",
    "            print(\"\\n\\nShape of LSTM\")\n",
    "            \n",
    "            print(\"Input X shape: \", x.get_shape())\n",
    "            #print(tf.shape(x))\n",
    "            #print(\"------\")\n",
    "    \n",
    "    \n",
    "            inputs_unstack = tf.unstack(x, axis=1)\n",
    "            #print(len(inputs_unstack))\n",
    "            print(\"Unstack shape: \", tf.shape(inputs_unstack))\n",
    "            #print(\"------\")\n",
    "            fc7_out = []\n",
    "            for i in inputs_unstack:\n",
    "                flattened = tf.reshape(i,[-1, 2])\n",
    "                fc7_out.append(flattened)\n",
    "    \n",
    "            aux = np.array(fc7_out)\n",
    "            #print(len(fc7_out))\n",
    "            #print(fc7_out[1].get_shape())\n",
    "            print(aux.shape)\n",
    "            #print(\"------\")\n",
    "    \n",
    "    \n",
    "    \n",
    "            lstm_layer = rnn.BasicLSTMCell(self.lstm_units, forget_bias=1)\n",
    "            lstm_outputs, _ = rnn.static_rnn(lstm_layer, fc7_out, dtype=\"float32\")\n",
    "    \n",
    "            if is_training is not None:\n",
    "                wd1 = tf.get_variable(\"wd1\", [self.lstm_units, 1], initializer=tf.contrib.layers.xavier_initializer(),trainable=False)\n",
    "                bd1 = tf.get_variable(\"bd1\", [1], initializer=tf.contrib.layers.xavier_initializer(), trainable=False)\n",
    "            else:\n",
    "                wd1 = tf.get_variable(\"wd1\", [self.lstm_units, 1], initializer=tf.contrib.layers.xavier_initializer(),trainable=True)\n",
    "                bd1 = tf.get_variable(\"bd1\", [1], initializer=tf.contrib.layers.xavier_initializer(), trainable=True)\n",
    "                \n",
    "    \n",
    "            # Regression\n",
    "            fc1 = tf.matmul(lstm_outputs[-1], wd1) + bd1\n",
    "            print(\"FC1 shape: \", tf.shape(fc1))\n",
    "            \n",
    "            print(\"---- \\n\\n\")\n",
    "            #y = tf.nn.softmax(fc1)\n",
    "    \n",
    "            return fc1\n",
    "\n",
    "    def load_initial_weights(self, session):\n",
    "\n",
    "        # Load the weights into memory\n",
    "        weights_dict = np.load(\"DEFAULT\", encoding='bytes').item()\n",
    "\n",
    "        # Loop over all layer names stored in the weights dict\n",
    "        for op_name in weights_dict:\n",
    "\n",
    "            # Check if layer should be trained from scratch\n",
    "            if op_name not in self.SKIP_LAYER:\n",
    "\n",
    "                with tf.variable_scope(op_name, reuse=True):\n",
    "\n",
    "                    # Assign weights/biases to their corresponding tf variable\n",
    "                    for data in weights_dict[op_name]:\n",
    "\n",
    "                        # Biases\n",
    "                        if len(data.shape) == 1:\n",
    "                            var = tf.get_variable('biases', trainable=False)\n",
    "                            session.run(var.assign(data))\n",
    "\n",
    "                        # Weights\n",
    "                        else:\n",
    "                            var = tf.get_variable('weights', trainable=False)\n",
    "                            session.run(var.assign(data))\n",
    "                            \n",
    "\n",
    "#Seeds\n",
    "np.random.seed(1234)\n",
    "tf.set_random_seed(1234)\n",
    "\n",
    "CUDA_VISIBLE_DEVICES=0\n",
    "config = tf.ConfigProto()\n",
    "\n",
    "# Variables\n",
    "TRAIN_PATH = \"../input/train.csv\"\n",
    "TARIN_VAL_SPLIT = 0.75\n",
    "filewriter_path = \"\"\n",
    "checkpoint_path = \"\"\n",
    "\n",
    "# Local variables\n",
    "seq_length = 150\n",
    "batch_size = 16\n",
    "n_features = 1\n",
    "starter_learning_rate = 0.0001\n",
    "epochs = 50\n",
    "\n",
    "\n",
    "\n",
    "print(\"Loading data\")\n",
    "train = pd.read_csv(TRAIN_PATH, dtype={'acoustic_data': np.int16, 'time_to_failure': np.float32})\n",
    "\n",
    "\n",
    "print(\"Data with shape {} has the following type of data:\".format(train.shape))\n",
    "\n",
    "\n",
    "\n",
    "dataset_split = np.floor(train.shape[0]/seq_length) #Number of samples used on training\n",
    "acoustic_data = train.acoustic_data.values\n",
    "print(acoustic_data.shape)\n",
    "del(train['acoustic_data'])\n",
    "ttf = train.time_to_failure.values\n",
    "print(ttf.shape)\n",
    "del(train)\n",
    "\n",
    "dataset = Dataset_Manager(dataset_split, TARIN_VAL_SPLIT, seq_length, batch_size)\n",
    "\n",
    "# Model\n",
    "tf.reset_default_graph()\n",
    "x = tf.placeholder(tf.float32, [None, seq_length, n_features])\n",
    "y = tf.placeholder(tf.float32, [None, 1])\n",
    "is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "kernel = model(x, seq_length, [], is_training, 'LSTM')\n",
    "score = kernel.y\n",
    "print(\"Score shape: \", tf.shape(score))\n",
    "# List of trainable variables\n",
    "var_list = tf.trainable_variables()\n",
    "print(\"----\")\n",
    "\n",
    "with tf.name_scope(\"cost_function\"):\n",
    "    print(\"Loading loss\")\n",
    "    print(tf.shape(y),tf.shape(score))\n",
    "    loss = tf.losses.absolute_difference(y, score)\n",
    "    tf.summary.scalar('Absolute_error', loss)\n",
    "\n",
    "with tf.name_scope(\"mean_absolute_error\"):\n",
    "    eval_metrics_ops = tf.metrics.mean_absolute_error(y,score)\n",
    "    tf.summary.scalar('MAE_metric', eval_metrics_ops)\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    # add an optimiser\n",
    "    print(\"Loading gradients\")\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    print(\"Global step\")\n",
    "    learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "                                               100, 0.95, staircase=True)\n",
    "    print(\"learning rate\")\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "    print(\"Optimizer loading\")\n",
    "    grads = tf.gradients(loss, tf.trainable_variables())\n",
    "    grads = list(zip(grads, tf.trainable_variables()))\n",
    "    print(\"Grads\")\n",
    "\n",
    "#for var in var_list:\n",
    "#        tf.summary.histogram(var.name, var)\n",
    "\n",
    "for grad, var in grads:\n",
    "    #print(var)\n",
    "    #print(grad)\n",
    "    #print(\"-----\")\n",
    "    tf.summary.histogram(var.name + '/gradient', grad)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Merge all summaries together\n",
    "merged_summary = tf.summary.merge_all()\n",
    "\n",
    "# Initialize the FileWriter\n",
    "writer_train = tf.summary.FileWriter(filewriter_path + '/train')\n",
    "writer_test = tf.summary.FileWriter(filewriter_path + '/validation')\n",
    "\n",
    "# Initialize a saver for store model checkpoints\n",
    "saver = tf.train.Saver(save_relative_paths=True)\n",
    "\n",
    "\n",
    "\n",
    "#with tf.Session() as sess:\n",
    "with tf.Session(config = config) as sess:\n",
    "    # initialise the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "\n",
    "    #writer_train.add_graph(sess.graph)\n",
    "    #writer_test.add_graph(sess.graph)\n",
    "\n",
    "    print(\"{} Start training...\".format(datetime.now()))\n",
    "    print(\"{} Open Tensorboard at --logdir {}\".format(datetime.now(),\n",
    "                                                     filewriter_path))\n",
    "    checkpoints = 0\n",
    "    for epoch in range(epochs):\n",
    "        num_batches = 0\n",
    "        dataset.generate_batch()\n",
    "        total_batches_train = dataset.total_batches_train\n",
    "        total_batches_test = dataset.total_batches_validation\n",
    "        for step in range(dataset.total_batches_train):\n",
    "            gs = (epoch * dataset.total_batches_train) + step + 1\n",
    "\n",
    "            batch_x, batch_y = dataset.get_next_batch(acoustic_data, ttf , True, step)\n",
    "            print(\"Shape X: \", np.asarray(batch_x).shape)\n",
    "            batch_y = np.expand_dims(batch_y, -1)\n",
    "            print(\"Shape Y: \", np.asarray(batch_y).shape)\n",
    "            s, _, loss_mse = sess.run([merged_summary, optimizer, loss],\n",
    "                               feed_dict={x: batch_x, y: batch_y, global_step: gs, is_training: True})\n",
    "                               \n",
    "            \n",
    "            if (step % 50 == 0):\n",
    "                print(\"MSE: \", loss_mse, \"epoch \", epoch, \" global step: \", gs, \" training: \",\n",
    "                      round(100 * float(step) / float(dataset.total_batches_train), 3), '%')\n",
    "            #writer_train.add_summary(s, (epoch * num_batches) + step)\n",
    "\n",
    "            for i in range(total_batches_test):\n",
    "                if (i % 100 == 0):\n",
    "                    print(\" validating: \", round(100 * float(i) / float(total_batches_test), 3), '%')\n",
    "\n",
    "                batch_test_x, batch_test_y = dataset.get_next_batch(acoustic_data, ttf, True, i)\n",
    "                batch_test_y = np.expand_dims(batch_test_y, -1)\n",
    "                s, mse_val, result = sess.run([merged_summary, eval_metrics_ops, score],\n",
    "                                                 feed_dict={x: batch_test_x, y: batch_test_y, is_training: False})\n",
    "                #writer_test.add_summary(s, (epoch * dataset.total_batches_validation) + i + step)\n",
    "\n",
    "                print(\"{} Saving checkpoint of model...\".format(datetime.now()))\n",
    "                # save checkpoint of the model for each epoch\n",
    "                checkpoint_name = os.path.join(checkpoint_path,\n",
    "                                                   'model_epoch' + str(checkpoints) + '.ckpt')\n",
    "                #save_path = saver.save(sess, checkpoint_name)\n",
    "                checkpoints += 1\n",
    "\n",
    "                print(\"{} Model checkpoint saved at {}\".format(datetime.now(),checkpoint_name))\n",
    "                print(\"{} MSE Accuracy Last = {:.4f}\".format(datetime.now(), mse_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
