{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature-based predictor\n",
    "This notebook is based on [Andrew Lukyanenko](https://www.kaggle.com/artgor)'s [kernel](https://www.kaggle.com/artgor/earthquakes-fe-more-features-and-samples)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General information\n",
    "Correctly predicting earthquakes is very important for preventing deaths and damage to infrastructure. In this competition we try to predict time left to the next laboratory earthquake based on seismic signal data. Training data represents one huge signal, but in test data we have many separate chunks, for each of which we need to predict time to failure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries\n",
    "Let's import everything we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import datetime\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "from catboost import CatBoostRegressor\n",
    "from scipy import stats\n",
    "from scipy.signal import hilbert, hann, convolve\n",
    "\n",
    "#from tensorflow import keras\n",
    "from keras import layers, Sequential, callbacks, backend\n",
    "from keras.utils import multi_gpu_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.regularizers import l1, l2\n",
    "\n",
    "#from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, AdaBoostRegressor\n",
    "from sklearn.svm import NuSVR, SVR\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.linear_model import LinearRegression, Ridge, RidgeCV\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold, GridSearchCV, cross_val_score\n",
    "from utils import freq_from_crossings, freq_from_fft\n",
    "from features import gpi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "pd.options.display.precision = 15\n",
    "warnings.filterwarnings('ignore')\n",
    "random.seed(1013)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/local/user/toolse64/ubuntu/18.04/usr/local/tools/anaconda_2018.12/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/user/toolse64/ubuntu/18.04/usr/local/tools/anaconda_2018.12/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/user/toolse64/ubuntu/18.04/usr/local/tools/anaconda_2018.12/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'nrows'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m         \u001b[0;31m# May alter columns / col_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/user/toolse64/ubuntu/18.04/usr/local/tools/anaconda_2018.12/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1993\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1994\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1995\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1996\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1997\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/local/user/toolse64/ubuntu/18.04/usr/local/tools/anaconda_2018.12/lib/python3.7/site-packages/pandas/core/dtypes/common.py\u001b[0m in \u001b[0;36mis_categorical_dtype\u001b[0;34m(arr_or_dtype)\u001b[0m\n\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mis_categorical_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_or_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m     \"\"\"\n\u001b[1;32m    574\u001b[0m     \u001b[0mCheck\u001b[0m \u001b[0mwhether\u001b[0m \u001b[0man\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlike\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mCategorical\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train = pd.read_csv('../input/train.csv', dtype={'acoustic_data': np.int16, 'time_to_failure': np.float32})\n",
    "fs = 4000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-2b7448ad5ccf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Train: rows:{train.shape[0]} cols:{train.shape[1]}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "print(f'Train: rows:{train.shape[0]} cols:{train.shape[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_acoustic_data_small = train['acoustic_data'].values[::50]\n",
    "train_time_to_failure_small = train['time_to_failure'].values[::50]\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(16, 8))\n",
    "plt.title('Trends of acoustic_data and time_to_failure. 2% of the data (sampled)')\n",
    "plt.plot(train_acoustic_data_small, color='b')\n",
    "ax1.set_ylabel('acoustic_data', color='b')\n",
    "plt.legend(['acoustic_data'])\n",
    "ax2 = ax1.twinx()\n",
    "plt.plot(train_time_to_failure_small, color='g')\n",
    "ax2.set_ylabel('time_to_failure', color='g')\n",
    "plt.legend(['time_to_failure'], loc=(0.875, 0.9))\n",
    "plt.grid(False)\n",
    "\n",
    "del train_acoustic_data_small\n",
    "del train_time_to_failure_small"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can see that usually acoustic data shows huge fluctuations just before the failure and the nature of data is cyclical;\n",
    "- Another important point: visually failures can be predicted as cases when huge fluctuations in signal are followes by small signal values. This could be useful for predicting \"time_to_failure\" changes from 0 to high values;\n",
    "- I thought that comparing max values of signal in a segment to some threshold value (1000 or 2000) could be useful, but it didn't work;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 8))\n",
    "plt.hist(train['time_to_failure'].values[::50], bins='auto', density=True)  # arguments are passed to np.histogram\n",
    "plt.title('Histogram of time_to_failure')\n",
    "plt.xlabel('time_to_failure')\n",
    "plt.ylabel('count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The time to training is not uniforally distribured on the training set. Resampling could be interesting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature generation\n",
    "- Usual aggregations: mean, std, min and max;\n",
    "- Average difference between the consequitive values in absolute and percent values;\n",
    "- Absolute min and max vallues;\n",
    "- Aforementioned aggregations for first and last 10000 and 50000 values - I think these data should be useful;\n",
    "- Max value to min value and their differencem also count of values bigger that 500 (arbitrary threshold);\n",
    "- Quantile features from this kernel: https://www.kaggle.com/andrekos/basic-feature-benchmark-with-quantiles\n",
    "- Trend features from this kernel: https://www.kaggle.com/jsaguiar/baseline-with-abs-and-trend-features\n",
    "- Rolling features from this kernel: https://www.kaggle.com/wimwim/rolling-quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_trend_feature(arr, abs_values=False):\n",
    "    idx = np.array(range(len(arr)))\n",
    "    if abs_values:\n",
    "        arr = np.abs(arr)\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(idx.reshape(-1, 1), arr)\n",
    "    return lr.coef_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sta_lta_ratio(x, length_sta, length_lta, method='original'):\n",
    "    if method=='original':\n",
    "        sta = np.cumsum(x ** 2)\n",
    "        # Convert to float\n",
    "        sta = np.require(sta, dtype=np.float)\n",
    "        # Copy for LTA\n",
    "        lta = sta.copy()\n",
    "        # Compute the STA and the LTA\n",
    "        sta[length_sta:] = sta[length_sta:] - sta[:-length_sta]\n",
    "        sta /= length_sta\n",
    "        lta[length_lta:] = lta[length_lta:] - lta[:-length_lta]\n",
    "        lta /= length_lta\n",
    "        # Pad zeros\n",
    "        sta[:length_lta - 1] = 0\n",
    "        # Avoid division by zero by setting zero values to tiny float\n",
    "        dtiny = np.finfo(0.0).tiny\n",
    "        idx = lta < dtiny\n",
    "        lta[idx] = dtiny\n",
    "        ratio = sta / lta\n",
    "        \n",
    "    elif method == 'modified':\n",
    "        x_abs = np.abs(x)\n",
    "        # Convert to float\n",
    "        x_abs = np.require(x_abs, dtype=np.float)\n",
    "        # Compute the STA and the LTA\n",
    "        sta = np.cumsum(x_abs)\n",
    "        sta[length_sta:] = sta[length_sta:] - sta[:-length_sta]\n",
    "        sta = sta[length_sta - 1:] / length_sta\n",
    "        sta = sta[:-(length_lta-length_sta)]\n",
    "        lta = x_abs.copy()\n",
    "        lta = np.cumsum(lta)\n",
    "        lta[length_lta:] = lta[length_lta:] - lta[:-length_lta]\n",
    "        lta = lta[length_lta - 1:] / length_lta\n",
    "        ratio = sta / lta\n",
    "\n",
    "    return ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_rate(x, method='original'):\n",
    "    if method == 'original':\n",
    "        rate = np.mean(np.nonzero((np.diff(x) / x[:-1]))[0])\n",
    "    if method == 'modified':\n",
    "        change = (np.diff(x) / x[:-1]).values\n",
    "        change = change[np.nonzero(change)[0]]\n",
    "        change = change[~np.isnan(change)]\n",
    "        change = change[change != -np.inf]\n",
    "        change = change[change != np.inf]\n",
    "        rate = np.mean(change)\n",
    "    return rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(seg_id, seg, X):\n",
    "    xc = pd.Series(seg['acoustic_data'].values)\n",
    "    zc = np.fft.fft(xc)\n",
    "    \n",
    "    # Generic stats\n",
    "    X.loc[seg_id, 'mean'] = xc.mean()\n",
    "    X.loc[seg_id, 'std'] = xc.std()\n",
    "    X.loc[seg_id, 'max'] = xc.max()\n",
    "    X.loc[seg_id, 'min'] = xc.min()\n",
    "    \n",
    "    #FFT transform values\n",
    "    realFFT = np.real(zc)\n",
    "    imagFFT = np.imag(zc)\n",
    "    X.loc[seg_id, 'Rmean'] = realFFT.mean()\n",
    "    X.loc[seg_id, 'Rstd'] = realFFT.std()\n",
    "    X.loc[seg_id, 'Rmax'] = realFFT.max()\n",
    "    X.loc[seg_id, 'Rmin'] = realFFT.min()\n",
    "    X.loc[seg_id, 'Imean'] = imagFFT.mean()\n",
    "    X.loc[seg_id, 'Istd'] = imagFFT.std()\n",
    "    X.loc[seg_id, 'Imax'] = imagFFT.max()\n",
    "    X.loc[seg_id, 'Imin'] = imagFFT.min()\n",
    "    X.loc[seg_id, 'Rmean_last_5000'] = realFFT[-5000:].mean()\n",
    "    X.loc[seg_id, 'Rstd__last_5000'] = realFFT[-5000:].std()\n",
    "    X.loc[seg_id, 'Rmax_last_5000'] = realFFT[-5000:].max()\n",
    "    X.loc[seg_id, 'Rmin_last_5000'] = realFFT[-5000:].min()\n",
    "    X.loc[seg_id, 'Rmean_last_15000'] = realFFT[-15000:].mean()\n",
    "    X.loc[seg_id, 'Rstd_last_15000'] = realFFT[-15000:].std()\n",
    "    X.loc[seg_id, 'Rmax_last_15000'] = realFFT[-15000:].max()\n",
    "    X.loc[seg_id, 'Rmin_last_15000'] = realFFT[-15000:].min()\n",
    "    \n",
    "    X.loc[seg_id, 'mean_diff'] = np.mean(np.diff(xc))\n",
    "    X.loc[seg_id, 'mean_abs_diff'] = np.mean(np.abs(np.diff(xc)))\n",
    "    X.loc[seg_id, 'mean_change_rate'] = change_rate(xc, method='original')\n",
    "    X.loc[seg_id, 'mean_change_rate_v2'] = change_rate(xc, method='modified')\n",
    "    X.loc[seg_id, 'abs_max'] = np.abs(xc).max()\n",
    "    X.loc[seg_id, 'abs_min'] = np.abs(xc).min()\n",
    "    \n",
    "    # Classical stats by segment\n",
    "    X.loc[seg_id, 'std_first_50000'] = xc[:50000].std()\n",
    "    X.loc[seg_id, 'std_last_50000'] = xc[-50000:].std()\n",
    "    X.loc[seg_id, 'std_first_10000'] = xc[:10000].std()\n",
    "    X.loc[seg_id, 'std_last_10000'] = xc[-10000:].std()\n",
    "    \n",
    "    X.loc[seg_id, 'avg_first_50000'] = xc[:50000].mean()\n",
    "    X.loc[seg_id, 'avg_last_50000'] = xc[-50000:].mean()\n",
    "    X.loc[seg_id, 'avg_first_10000'] = xc[:10000].mean()\n",
    "    X.loc[seg_id, 'avg_last_10000'] = xc[-10000:].mean()\n",
    "    \n",
    "    X.loc[seg_id, 'min_first_50000'] = xc[:50000].min()\n",
    "    X.loc[seg_id, 'min_last_50000'] = xc[-50000:].min()\n",
    "    X.loc[seg_id, 'min_first_10000'] = xc[:10000].min()\n",
    "    X.loc[seg_id, 'min_last_10000'] = xc[-10000:].min()\n",
    "    \n",
    "    X.loc[seg_id, 'max_first_50000'] = xc[:50000].max()\n",
    "    X.loc[seg_id, 'max_last_50000'] = xc[-50000:].max()\n",
    "    X.loc[seg_id, 'max_first_10000'] = xc[:10000].max()\n",
    "    X.loc[seg_id, 'max_last_10000'] = xc[-10000:].max()\n",
    "    \n",
    "    X.loc[seg_id, 'max_to_min'] = xc.max() / np.abs(xc.min())\n",
    "    X.loc[seg_id, 'max_to_min_diff'] = xc.max() - np.abs(xc.min())\n",
    "    X.loc[seg_id, 'count_big'] = len(xc[np.abs(xc) > 500])\n",
    "    X.loc[seg_id, 'sum'] = xc.sum()\n",
    "    \n",
    "    X.loc[seg_id, 'mean_change_rate_first_50000'] = change_rate(xc[:50000], method='original')\n",
    "    X.loc[seg_id, 'mean_change_rate_last_50000'] = change_rate(xc[-50000:], method='original')\n",
    "    X.loc[seg_id, 'mean_change_rate_first_10000'] = change_rate(xc[:10000], method='original')\n",
    "    X.loc[seg_id, 'mean_change_rate_last_10000'] = change_rate(xc[-10000:], method='original')\n",
    "\n",
    "    X.loc[seg_id, 'mean_change_rate_first_50000_v2'] = change_rate(xc[:50000], method='modified')\n",
    "    X.loc[seg_id, 'mean_change_rate_last_50000_v2'] = change_rate(xc[-50000:], method='modified')\n",
    "    X.loc[seg_id, 'mean_change_rate_first_10000_v2'] = change_rate(xc[:10000], method='modified')\n",
    "    X.loc[seg_id, 'mean_change_rate_last_10000_v2'] = change_rate(xc[-10000:], method='modified')\n",
    "\n",
    "    X.loc[seg_id, 'q95'] = np.quantile(xc, 0.95)\n",
    "    X.loc[seg_id, 'q99'] = np.quantile(xc, 0.99)\n",
    "    X.loc[seg_id, 'q05'] = np.quantile(xc, 0.05)\n",
    "    X.loc[seg_id, 'q01'] = np.quantile(xc, 0.01)\n",
    "    \n",
    "    X.loc[seg_id, 'abs_q95'] = np.quantile(np.abs(xc), 0.95)\n",
    "    X.loc[seg_id, 'abs_q99'] = np.quantile(np.abs(xc), 0.99)\n",
    "    X.loc[seg_id, 'abs_q05'] = np.quantile(np.abs(xc), 0.05)\n",
    "    X.loc[seg_id, 'abs_q01'] = np.quantile(np.abs(xc), 0.01)\n",
    "    \n",
    "    X.loc[seg_id, 'trend'] = add_trend_feature(xc)\n",
    "    X.loc[seg_id, 'abs_trend'] = add_trend_feature(xc, abs_values=True)\n",
    "    X.loc[seg_id, 'abs_mean'] = np.abs(xc).mean()\n",
    "    X.loc[seg_id, 'abs_std'] = np.abs(xc).std()\n",
    "    \n",
    "    X.loc[seg_id, 'mad'] = xc.mad()\n",
    "    X.loc[seg_id, 'kurt'] = xc.kurtosis()\n",
    "    X.loc[seg_id, 'skew'] = xc.skew()\n",
    "    X.loc[seg_id, 'med'] = xc.median()\n",
    "    \n",
    "    X.loc[seg_id, 'Hilbert_mean'] = np.abs(hilbert(xc)).mean()\n",
    "    X.loc[seg_id, 'Hann_window_mean'] = (convolve(xc, hann(150), mode='same') / sum(hann(150))).mean()    \n",
    "    \n",
    "    sta_lta_method = 'original'\n",
    "    classic_sta_lta1 = sta_lta_ratio(xc, 500, 10000, method=sta_lta_method)\n",
    "    classic_sta_lta2 = sta_lta_ratio(xc, 5000, 100000, method=sta_lta_method)\n",
    "    classic_sta_lta3 = sta_lta_ratio(xc, 3333, 6666, method=sta_lta_method)\n",
    "    classic_sta_lta4 = sta_lta_ratio(xc, 10000, 25000, method=sta_lta_method)\n",
    "    classic_sta_lta5 = sta_lta_ratio(xc, 50, 1000, method=sta_lta_method)\n",
    "    classic_sta_lta6 = sta_lta_ratio(xc, 100, 5000, method=sta_lta_method)\n",
    "    classic_sta_lta7 = sta_lta_ratio(xc, 333, 666, method=sta_lta_method)\n",
    "    classic_sta_lta8 = sta_lta_ratio(xc, 4000, 10000, method=sta_lta_method)\n",
    "    \n",
    "    X.loc[seg_id, 'classic_sta_lta1_mean'] = classic_sta_lta1.mean()\n",
    "    X.loc[seg_id, 'classic_sta_lta2_mean'] = classic_sta_lta2.mean()\n",
    "    X.loc[seg_id, 'classic_sta_lta3_mean'] = classic_sta_lta3.mean()\n",
    "    X.loc[seg_id, 'classic_sta_lta4_mean'] = classic_sta_lta4.mean()\n",
    "    X.loc[seg_id, 'classic_sta_lta5_mean'] = classic_sta_lta5.mean()\n",
    "    X.loc[seg_id, 'classic_sta_lta6_mean'] = classic_sta_lta6.mean()\n",
    "    X.loc[seg_id, 'classic_sta_lta7_mean'] = classic_sta_lta7.mean()\n",
    "    X.loc[seg_id, 'classic_sta_lta8_mean'] = classic_sta_lta8.mean()\n",
    "\n",
    "    X.loc[seg_id, 'classic_sta_lta1_q95'] = np.quantile(classic_sta_lta1, 0.95)\n",
    "    X.loc[seg_id, 'classic_sta_lta2_q95'] = np.quantile(classic_sta_lta2, 0.95)\n",
    "    X.loc[seg_id, 'classic_sta_lta3_q95'] = np.quantile(classic_sta_lta3, 0.95)\n",
    "    X.loc[seg_id, 'classic_sta_lta4_q95'] = np.quantile(classic_sta_lta4, 0.95)\n",
    "    X.loc[seg_id, 'classic_sta_lta5_q95'] = np.quantile(classic_sta_lta5, 0.95)\n",
    "    X.loc[seg_id, 'classic_sta_lta6_q95'] = np.quantile(classic_sta_lta6, 0.95)\n",
    "    X.loc[seg_id, 'classic_sta_lta7_q95'] = np.quantile(classic_sta_lta7, 0.95)\n",
    "    X.loc[seg_id, 'classic_sta_lta8_q95'] = np.quantile(classic_sta_lta8, 0.95)   \n",
    "\n",
    "    X.loc[seg_id, 'classic_sta_lta1_q05'] = np.quantile(classic_sta_lta1, 0.05)\n",
    "    X.loc[seg_id, 'classic_sta_lta2_q05'] = np.quantile(classic_sta_lta2, 0.05)\n",
    "    X.loc[seg_id, 'classic_sta_lta3_q05'] = np.quantile(classic_sta_lta3, 0.05)\n",
    "    X.loc[seg_id, 'classic_sta_lta4_q05'] = np.quantile(classic_sta_lta4, 0.05)\n",
    "    X.loc[seg_id, 'classic_sta_lta5_q05'] = np.quantile(classic_sta_lta5, 0.05)\n",
    "    X.loc[seg_id, 'classic_sta_lta6_q05'] = np.quantile(classic_sta_lta6, 0.05)\n",
    "    X.loc[seg_id, 'classic_sta_lta7_q05'] = np.quantile(classic_sta_lta7, 0.05)\n",
    "    X.loc[seg_id, 'classic_sta_lta8_q05'] = np.quantile(classic_sta_lta8, 0.05)\n",
    "\n",
    "    sta_lta_method = 'modified'\n",
    "    classic_sta_lta1 = sta_lta_ratio(xc, 500, 10000, method=sta_lta_method)\n",
    "    classic_sta_lta2 = sta_lta_ratio(xc, 5000, 100000, method=sta_lta_method)\n",
    "    classic_sta_lta3 = sta_lta_ratio(xc, 3333, 6666, method=sta_lta_method)\n",
    "    classic_sta_lta4 = sta_lta_ratio(xc, 10000, 25000, method=sta_lta_method)\n",
    "    classic_sta_lta5 = sta_lta_ratio(xc, 50, 1000, method=sta_lta_method)\n",
    "    classic_sta_lta6 = sta_lta_ratio(xc, 100, 5000, method=sta_lta_method)\n",
    "    classic_sta_lta7 = sta_lta_ratio(xc, 333, 666, method=sta_lta_method)\n",
    "    classic_sta_lta8 = sta_lta_ratio(xc, 4000, 10000, method=sta_lta_method)\n",
    "    \n",
    "    X.loc[seg_id, 'modified_sta_lta1_mean'] = classic_sta_lta1.mean()\n",
    "    X.loc[seg_id, 'modified_sta_lta2_mean'] = classic_sta_lta2.mean()\n",
    "    X.loc[seg_id, 'modified_sta_lta3_mean'] = classic_sta_lta3.mean()\n",
    "    X.loc[seg_id, 'modified_sta_lta4_mean'] = classic_sta_lta4.mean()\n",
    "    X.loc[seg_id, 'modified_sta_lta5_mean'] = classic_sta_lta5.mean()\n",
    "    X.loc[seg_id, 'modified_sta_lta6_mean'] = classic_sta_lta6.mean()\n",
    "    X.loc[seg_id, 'modified_sta_lta7_mean'] = classic_sta_lta7.mean()\n",
    "    X.loc[seg_id, 'modified_sta_lta8_mean'] = classic_sta_lta8.mean()\n",
    "\n",
    "    X.loc[seg_id, 'modified_sta_lta1_q95'] = np.quantile(classic_sta_lta1, 0.95)\n",
    "    X.loc[seg_id, 'modified_sta_lta2_q95'] = np.quantile(classic_sta_lta2, 0.95)\n",
    "    X.loc[seg_id, 'modified_sta_lta3_q95'] = np.quantile(classic_sta_lta3, 0.95)\n",
    "    X.loc[seg_id, 'modified_sta_lta4_q95'] = np.quantile(classic_sta_lta4, 0.95)\n",
    "    X.loc[seg_id, 'modified_sta_lta5_q95'] = np.quantile(classic_sta_lta5, 0.95)\n",
    "    X.loc[seg_id, 'modified_sta_lta6_q95'] = np.quantile(classic_sta_lta6, 0.95)\n",
    "    X.loc[seg_id, 'modified_sta_lta7_q95'] = np.quantile(classic_sta_lta7, 0.95)\n",
    "    X.loc[seg_id, 'modified_sta_lta8_q95'] = np.quantile(classic_sta_lta8, 0.95)   \n",
    "\n",
    "    X.loc[seg_id, 'modified_sta_lta1_q05'] = np.quantile(classic_sta_lta1, 0.05)\n",
    "    X.loc[seg_id, 'modified_sta_lta2_q05'] = np.quantile(classic_sta_lta2, 0.05)\n",
    "    X.loc[seg_id, 'modified_sta_lta3_q05'] = np.quantile(classic_sta_lta3, 0.05)\n",
    "    X.loc[seg_id, 'modified_sta_lta4_q05'] = np.quantile(classic_sta_lta4, 0.05)\n",
    "    X.loc[seg_id, 'modified_sta_lta5_q05'] = np.quantile(classic_sta_lta5, 0.05)\n",
    "    X.loc[seg_id, 'modified_sta_lta6_q05'] = np.quantile(classic_sta_lta6, 0.05)\n",
    "    X.loc[seg_id, 'modified_sta_lta7_q05'] = np.quantile(classic_sta_lta7, 0.05)\n",
    "    X.loc[seg_id, 'modified_sta_lta8_q05'] = np.quantile(classic_sta_lta8, 0.05)\n",
    "\n",
    "    X.loc[seg_id, 'Moving_average_700_mean'] = xc.rolling(window=700).mean().mean(skipna=True)\n",
    "    X.loc[seg_id, 'Moving_average_1500_mean'] = xc.rolling(window=1500).mean().mean(skipna=True)\n",
    "    X.loc[seg_id, 'Moving_average_3000_mean'] = xc.rolling(window=3000).mean().mean(skipna=True)\n",
    "    X.loc[seg_id, 'Moving_average_6000_mean'] = xc.rolling(window=6000).mean().mean(skipna=True)\n",
    "    X.loc[seg_id, 'Moving_average_30000_mean'] = xc.rolling(window=30000).mean().mean(skipna=True)\n",
    "\n",
    "    ewma = pd.Series.ewm\n",
    "    X.loc[seg_id, 'exp_Moving_average_300_mean'] = ewma(xc, span=300).mean().mean(skipna=True)\n",
    "    X.loc[seg_id, 'exp_Moving_average_3000_mean'] = ewma(xc, span=3000).mean().mean(skipna=True)\n",
    "    X.loc[seg_id, 'exp_Moving_average_6000_mean'] = ewma(xc, span=6000).mean().mean(skipna=True)\n",
    "    X.loc[seg_id, 'exp_Moving_average_30000_mean'] = ewma(xc, span=30000).mean().mean(skipna=True)\n",
    "\n",
    "    # rdg: TODO it seems a parameter to tune\n",
    "    no_of_std = 2\n",
    "    X.loc[seg_id, 'MA_700MA_std_mean'] = xc.rolling(window=700).std().mean()\n",
    "    X.loc[seg_id, 'MA_700MA_BB_high_mean'] = (X.loc[seg_id, 'Moving_average_700_mean']\n",
    "                                              + no_of_std * X.loc[seg_id, 'MA_700MA_std_mean']).mean()\n",
    "    X.loc[seg_id, 'MA_700MA_BB_low_mean'] = (X.loc[seg_id, 'Moving_average_700_mean']\n",
    "                                             - no_of_std * X.loc[seg_id, 'MA_700MA_std_mean']).mean()\n",
    "    X.loc[seg_id, 'MA_400MA_std_mean'] = xc.rolling(window=400).std().mean()\n",
    "    X.loc[seg_id, 'MA_400MA_BB_high_mean'] = (X.loc[seg_id, 'Moving_average_700_mean']\n",
    "                                              + no_of_std * X.loc[seg_id, 'MA_400MA_std_mean']).mean()\n",
    "    X.loc[seg_id, 'MA_400MA_BB_low_mean'] = (X.loc[seg_id, 'Moving_average_700_mean']\n",
    "                                             - no_of_std * X.loc[seg_id, 'MA_400MA_std_mean']).mean()\n",
    "    X.loc[seg_id, 'MA_1000MA_std_mean'] = xc.rolling(window=1000).std().mean()\n",
    "    \n",
    "    X.loc[seg_id, 'iqr'] = np.subtract(*np.percentile(xc, [75, 25]))\n",
    "    X.loc[seg_id, 'q999'] = np.quantile(xc, 0.999)\n",
    "    X.loc[seg_id, 'q001'] = np.quantile(xc, 0.001)\n",
    "    X.loc[seg_id, 'ave10'] = stats.trim_mean(xc, 0.1)\n",
    "\n",
    "    X.loc[seg_id, 'freq_cross_first_50000'] = freq_from_crossings(xc.values[:50000], fs)\n",
    "    X.loc[seg_id, 'freq_cross_last_50000'] = freq_from_crossings(xc.values[-50000:], fs)\n",
    "    X.loc[seg_id, 'freq_cross_first_10000'] = freq_from_crossings(xc.values[:10000], fs)\n",
    "    X.loc[seg_id, 'freq_cross_last_10000'] = freq_from_crossings(xc.values[-10000:], fs)\n",
    "    \n",
    "    for windows in [10, 100, 1000]:\n",
    "        x_roll_std = xc.rolling(windows).std().dropna().values\n",
    "        x_roll_mean = xc.rolling(windows).mean().dropna().values\n",
    "        \n",
    "        X.loc[seg_id, 'ave_roll_std_' + str(windows)] = x_roll_std.mean()\n",
    "        X.loc[seg_id, 'std_roll_std_' + str(windows)] = x_roll_std.std()\n",
    "        X.loc[seg_id, 'max_roll_std_' + str(windows)] = x_roll_std.max()\n",
    "        X.loc[seg_id, 'min_roll_std_' + str(windows)] = x_roll_std.min()\n",
    "        X.loc[seg_id, 'q01_roll_std_' + str(windows)] = np.quantile(x_roll_std, 0.01)\n",
    "        X.loc[seg_id, 'q05_roll_std_' + str(windows)] = np.quantile(x_roll_std, 0.05)\n",
    "        X.loc[seg_id, 'q95_roll_std_' + str(windows)] = np.quantile(x_roll_std, 0.95)\n",
    "        X.loc[seg_id, 'q99_roll_std_' + str(windows)] = np.quantile(x_roll_std, 0.99)\n",
    "        X.loc[seg_id, 'av_change_abs_roll_std_' + str(windows)] = np.mean(np.abs(np.diff(x_roll_std)))\n",
    "        X.loc[seg_id, 'av_change_rate_roll_std_' + str(windows)] = change_rate(pd.Series(x_roll_std), \n",
    "                                                                               method='original')\n",
    "        X.loc[seg_id, 'av_change_rate_roll_std_' + str(windows) + 'v2'] = change_rate(pd.Series(x_roll_std),\n",
    "                                                                                      method='modified')\n",
    "        X.loc[seg_id, 'abs_max_roll_std_' + str(windows)] = np.abs(x_roll_std).max()\n",
    "        X.loc[seg_id, 'ave_roll_mean_' + str(windows)] = x_roll_mean.mean()\n",
    "        X.loc[seg_id, 'std_roll_mean_' + str(windows)] = x_roll_mean.std()\n",
    "        X.loc[seg_id, 'max_roll_mean_' + str(windows)] = x_roll_mean.max()\n",
    "        X.loc[seg_id, 'min_roll_mean_' + str(windows)] = x_roll_mean.min()\n",
    "        X.loc[seg_id, 'q01_roll_mean_' + str(windows)] = np.quantile(x_roll_mean, 0.01)\n",
    "        X.loc[seg_id, 'q05_roll_mean_' + str(windows)] = np.quantile(x_roll_mean, 0.05)\n",
    "        X.loc[seg_id, 'q95_roll_mean_' + str(windows)] = np.quantile(x_roll_mean, 0.95)\n",
    "        X.loc[seg_id, 'q99_roll_mean_' + str(windows)] = np.quantile(x_roll_mean, 0.99)\n",
    "        X.loc[seg_id, 'av_change_abs_roll_mean_' + str(windows)] = np.mean(np.abs(np.diff(x_roll_mean)))\n",
    "        X.loc[seg_id, 'av_change_rate_roll_mean_' + str(windows)] = change_rate(pd.Series(x_roll_mean),\n",
    "                                                                               method = 'original')\n",
    "        X.loc[seg_id, 'av_change_rate_roll_mean_' + str(windows) + '_v2'] = change_rate(pd.Series(x_roll_mean),\n",
    "                                                                                       method = 'modified')\n",
    "        X.loc[seg_id, 'abs_max_roll_mean_' + str(windows)] = np.abs(x_roll_mean).max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling the training set\n",
    "Since the testign set is build of sequences of 150.000 samples, let's chop the training set on comparable chunks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a training file with simple derived features\n",
    "segment_size = 150000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_segment_start_ids(sampling_method):\n",
    "    if sampling_method == 'uniform':\n",
    "        # With this approach we obtain 4194 segments\n",
    "        num_segments_training = int(np.floor(train.shape[0] / segment_size))\n",
    "        segment_start_ids = [i * segment_size for i in range(num_segments_training)]\n",
    "    elif sampling_method == 'uniform_no_jump':\n",
    "        # With this approach we obtain 4178 segments (99.5% of 'uniform')\n",
    "        already_sampled = np.full(train.shape[0], False)\n",
    "        num_segments_training = int(np.floor(train.shape[0] / segment_size))\n",
    "        time_to_failure_jumps = np.diff(train['time_to_failure'].values)\n",
    "        num_good_segments_found = 0\n",
    "        segment_start_ids = []\n",
    "        for i in range(num_segments_training):\n",
    "            idx = i * segment_size\n",
    "            # Detect if there is a discontinuity on the time_to_failure signal within the segment\n",
    "            max_jump = np.max(time_to_failure_jumps[idx:idx + segment_size])\n",
    "            if max_jump < 5:\n",
    "                segment_start_ids.append(idx)\n",
    "                num_good_segments_found += 1\n",
    "            else:\n",
    "                print(f'Rejected candidate segment since max_jump={max_jump}')\n",
    "        segment_start_ids.sort()\n",
    "    elif sampling_method == 'random_no_jump':\n",
    "        # With this approach we obtain 4194 segments\n",
    "        num_segments_training = int(np.floor(train.shape[0] / segment_size)) #arbitrary choice\n",
    "        time_to_failure_jumps = np.diff(train['time_to_failure'].values)\n",
    "        num_good_segments_found = 0\n",
    "        segment_start_ids = []\n",
    "        while num_segments_training != num_good_segments_found:\n",
    "            # Generate a random sampling position\n",
    "            idx = random.randint(0, train.shape[0] - segment_size - 1)\n",
    "            # Detect if there is a discontinuity on the time_to_failure signal within the segment\n",
    "            max_jump = np.max(time_to_failure_jumps[idx:idx + segment_size])\n",
    "            if max_jump < 5:\n",
    "                segment_start_ids.append(idx)\n",
    "                num_good_segments_found += 1\n",
    "            else:\n",
    "                print(f'Rejected candidate segment since max_jump={max_jump}')\n",
    "        segment_start_ids.sort()\n",
    "    else:\n",
    "        raise NameError('Method does not exist')\n",
    "    return segment_start_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, we have two strategies implemented.\n",
    "- Uniform sampling: Here the training set is built from consecutive chunks of data. No special care is taken into what's the content of each segment.\n",
    "- Uniform sampling with rejection: Here the training set is built from consecutive chunks of data. There is a control to avoid having in a segment the jump of the `time_to_failure` signal from zero to a high value.\n",
    "- Random sampling with rejection: Here the training set is built from sampling randomly the data. There is a control to avoid having in a segment the jump of the `time_to_failure` signal from zero to a high value. The segments will most likely overlap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize what happens to the distribution of the `time_to_failure` on teh different samplign strategies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f'Generating uniformly sampled training set')\n",
    "segment_start_ids_uniform = generate_segment_start_ids('uniform')\n",
    "\n",
    "print(f'Generating uniformly sampled training set excluding discontinuities in time_to_failure.')\n",
    "segment_start_ids_uniform_no_jump = generate_segment_start_ids('uniform_no_jump')\n",
    "\n",
    "print(f'Generating randomly sampled training set excluding discontinuities in time_to_failure.')\n",
    "print(f'This method may yield overlaping segments')\n",
    "segment_start_ids_random_no_jump = generate_segment_start_ids('random_no_jump')\n",
    "\n",
    "\n",
    "y_tr_samples_uniform = train['time_to_failure'].values[np.array(\n",
    "    segment_start_ids_uniform) + segment_size - 1]\n",
    "y_tr_samples_uniform_no_jump = train['time_to_failure'].values[\n",
    "    np.array(segment_start_ids_uniform_no_jump) + segment_size - 1]\n",
    "y_tr_samples_random_no_jump = train['time_to_failure'].values[\n",
    "    np.array(segment_start_ids_random_no_jump) + segment_size - 1]\n",
    "\n",
    "plt.subplots(figsize=(16, 16))\n",
    "plt.subplot(3, 3, 1)\n",
    "plt.plot(segment_start_ids_uniform)\n",
    "plt.title('segment_start_ids_uniform')\n",
    "plt.subplot(3, 3, 2)\n",
    "plt.plot(segment_start_ids_uniform_no_jump)\n",
    "plt.title('segment_start_ids_uniform_no_jump')\n",
    "plt.subplot(3, 3, 3)\n",
    "plt.plot(segment_start_ids_random_no_jump)\n",
    "plt.title('segment_start_ids_random_no_jump')\n",
    "\n",
    "plt.subplot(3, 3, 4)\n",
    "plt.plot(y_tr_samples_uniform)\n",
    "plt.title('time_to_failure values (all)')\n",
    "plt.subplot(3, 3, 5)\n",
    "plt.plot(y_tr_samples_uniform_no_jump)\n",
    "plt.title('time_to_failure values (uniform sampling no jump)')\n",
    "plt.subplot(3, 3, 6)\n",
    "plt.plot(y_tr_samples_random_no_jump)\n",
    "plt.title('time_to_failure values (random sampling no jump)')\n",
    "\n",
    "plt.subplot(3, 3, 7)\n",
    "plt.hist(y_tr_samples_uniform, bins='auto', alpha=0.5, density=True)\n",
    "plt.hist(train['time_to_failure'].values[::50], bins='auto', alpha=0.5, density=True)\n",
    "plt.title('With discontinuities (contiguous)')\n",
    "plt.legend(['Sampled', 'All data'])\n",
    "\n",
    "plt.subplot(3, 3, 8)\n",
    "plt.hist(y_tr_samples_uniform_no_jump, bins='auto', alpha=0.5, density=True)\n",
    "plt.hist(train['time_to_failure'].values[::50], bins='auto', alpha=0.5, density=True)\n",
    "plt.title('Discarding discontinuities (contiguous)')\n",
    "plt.legend(['Sampled', 'All data'])\n",
    "\n",
    "plt.subplot(3, 3, 9)\n",
    "plt.hist(y_tr_samples_random_no_jump, bins='auto', alpha=0.5, density=True)\n",
    "plt.title('Discarding discontinuities (rand)')\n",
    "plt.hist(train['time_to_failure'].values[::50], bins='auto', alpha=0.5, density=True)\n",
    "plt.legend(['Sampled', 'All data'])\n",
    "plt.show()\n",
    "\n",
    "del segment_start_ids_uniform\n",
    "del segment_start_ids_uniform_no_jump\n",
    "del segment_start_ids_random_no_jump\n",
    "del y_tr_samples_uniform\n",
    "del y_tr_samples_uniform_no_jump\n",
    "del y_tr_samples_random_no_jump"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we choose one sampling method (`uniform`) and run with it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_start_ids = generate_segment_start_ids('uniform_no_jump')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr = pd.DataFrame(index=range(len(segment_start_ids)), dtype=np.float64)\n",
    "y_tr = pd.DataFrame(index=range(len(segment_start_ids)), dtype=np.float64, columns=['time_to_failure'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute all features for all segments (WARNING: it takes time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for idx in tqdm_notebook(range(len(segment_start_ids))):        \n",
    "    seg_id = segment_start_ids[idx]\n",
    "    seg = train.iloc[seg_id:seg_id + segment_size]\n",
    "    create_features(idx, seg, X_tr)\n",
    "    y_tr.loc[idx, 'time_to_failure'] = seg['time_to_failure'].values[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{X_tr.shape[0]} samples in new train data and {X_tr.shape[1]} columns.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look of how some of these features look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(44, 24))\n",
    "cols = list(np.abs(X_tr.corrwith(y_tr['time_to_failure'])).sort_values(ascending=False).head(24).index)\n",
    "for i, col in enumerate(cols):\n",
    "    ax1 = plt.subplot(6, 4, i + 1)\n",
    "    plt.plot(X_tr[col], color='blue')\n",
    "    plt.title(col)\n",
    "    ax1.set_ylabel(col, color='b')\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    plt.plot(y_tr, color='g')\n",
    "    ax2.set_ylabel('time_to_failure', color='g')\n",
    "    plt.legend([col, 'time_to_failure'])\n",
    "    plt.grid(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, there are features that exploded, which show as `np.inf` in the array. Let's just replace them by the mean values. TODO: check if this is the appropiate strategy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "means_dict = {}\n",
    "for col in X_tr.columns:\n",
    "    if X_tr[col].isnull().any():\n",
    "        print(col)\n",
    "        mean_value = X_tr.loc[X_tr[col] != -np.inf, col].mean()\n",
    "        X_tr.loc[X_tr[col] == -np.inf, col] = mean_value\n",
    "        X_tr[col] = X_tr[col].fillna(mean_value)\n",
    "        means_dict[col] = mean_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('../input/sample_submission.csv', index_col='seg_id')\n",
    "X_test = pd.DataFrame(columns=X_tr.columns, dtype=np.float64, index=submission.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(22, 16))\n",
    "for i, seg_id in enumerate(tqdm_notebook(X_test.index)):\n",
    "    seg = pd.read_csv('../input/test/' + seg_id + '.csv')\n",
    "    create_features(seg_id, seg, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "for col in X_test.columns:\n",
    "    if X_test[col].isnull().any():\n",
    "        X_test.loc[X_test[col] == -np.inf, col] = means_dict[col]\n",
    "        X_test[col] = X_test[col].fillna(means_dict[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaler = StandardScaler()\n",
    "#scaler.fit(X_tr)\n",
    "#X_train_scaled = pd.DataFrame(scaler.transform(X_tr), columns=X_tr.columns)\n",
    "#X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only real difference is scaling the lot in one go rather than scling train and then using this to scale test\n",
    "alldata = pd.concat([X_tr, X_test])\n",
    "scaler = StandardScaler()\n",
    "alldata = pd.DataFrame(scaler.fit_transform(alldata), columns=alldata.columns)\n",
    "X_train_scaled = alldata[:X_tr.shape[0]]\n",
    "X_test_scaled = alldata[X_tr.shape[0]:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_cv(estimator, grid, features, target, num_folds):\n",
    "    \"\"\"Return the best hyperparameters combination in grid.\"\"\"\n",
    "    t0 = time.time()\n",
    "    reg = GridSearchCV(estimator, grid, cv=num_folds, scoring='neg_mean_absolute_error', n_jobs=-1, verbose=10)\n",
    "    reg.fit(features, target)\n",
    "    \n",
    "    t0 = time.time() - t0\n",
    "    print(\"Best CV score: {:.4f}, time: {:.1f}s\".format(-reg.best_score_, t0))\n",
    "    print(reg.best_params_)\n",
    "    return reg.best_params_, reg.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X, X_test, y, folds, params=None, model_type='lgb',\n",
    "                model=None, show_scatter=False):\n",
    "\n",
    "    oof = np.zeros(len(X))\n",
    "    prediction = np.zeros(len(X_test))\n",
    "    scores = []\n",
    "    n_fold = folds.get_n_splits()\n",
    "    \n",
    "    feature_importance = pd.DataFrame()\n",
    "    for fold_n, (train_index, valid_index) in enumerate(folds.split(X)):\n",
    "        print('Fold', fold_n, 'started at', time.ctime())\n",
    "        X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n",
    "        y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n",
    "\n",
    "        if model_type == 'nn':\n",
    "            model = Sequential()\n",
    "            model.add(layers.Dense(1024, input_dim=216, activation=tf.nn.relu))\n",
    "            model.add(layers.Dropout(0.8))\n",
    "            model.add(layers.Dense(256, activation=tf.nn.relu))\n",
    "            model.add(layers.Dropout(0.8))\n",
    "            model.add(layers.Dense(256, activation=tf.nn.relu))\n",
    "            model.add(layers.Dropout(0.8))\n",
    "            model.add(layers.Dense(16, activation=tf.nn.relu))\n",
    "            model.add(layers.Dense(1))\n",
    "\n",
    "            model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
    "            EPOCHS = 1000\n",
    "            early_stop = callbacks.EarlyStopping(monitor='mean_absolute_error', patience=100)\n",
    "\n",
    "            history = model.fit(\n",
    "                X_train,\n",
    "                y_train,\n",
    "                epochs=EPOCHS,\n",
    "                validation_data = (X_valid, y_valid), \n",
    "                verbose=0,\n",
    "                callbacks=[early_stop, PrintDot()])\n",
    "            hist = pd.DataFrame(history.history)\n",
    "            val_score = hist['val_mean_absolute_error'].iloc[-1]\n",
    "            print(f'val_score={val_score}')\n",
    "            plot_history(history)\n",
    "        \n",
    "            y_pred_valid = model.predict(X_valid).reshape(-1,)\n",
    "            y_pred = model.predict(X_test).reshape(-1,)\n",
    "\n",
    "            score = mean_absolute_error(y_valid, y_pred_valid)\n",
    "            print(f'Fold {fold_n}. MAE: {score:.4f}.')\n",
    "        \n",
    "        if model_type == 'lgb':\n",
    "            model = lgb.LGBMRegressor(**params, n_estimators = 50000, n_jobs = 32)\n",
    "            model.fit(X_train, y_train, \n",
    "                      eval_set=[(X_train, y_train), (X_valid, y_valid)],\n",
    "                      eval_metric='mae',\n",
    "                      verbose=10000,\n",
    "                      early_stopping_rounds=2000)\n",
    "            \n",
    "            y_pred_valid = model.predict(X_valid)\n",
    "            y_pred = model.predict(X_test, num_iteration=model.best_iteration_)\n",
    "            \n",
    "        if model_type == 'xgb':\n",
    "            train_data = xgb.DMatrix(data=X_train, label=y_train, feature_names=X.columns)\n",
    "            valid_data = xgb.DMatrix(data=X_valid, label=y_valid, feature_names=X.columns)\n",
    "\n",
    "            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]\n",
    "            model = xgb.train(dtrain=train_data,\n",
    "                              num_boost_round=20000,\n",
    "                              evals=watchlist,\n",
    "                              early_stopping_rounds=200,\n",
    "                              verbose_eval=500,\n",
    "                              params=params)\n",
    "\n",
    "            y_pred_valid = model.predict(xgb.DMatrix(X_valid, feature_names=X.columns),\n",
    "                                         ntree_limit=model.best_ntree_limit)\n",
    "            y_pred = model.predict(xgb.DMatrix(X_test, feature_names=X.columns),\n",
    "                                   ntree_limit=model.best_ntree_limit)\n",
    "        \n",
    "        if model_type == 'sklearn':\n",
    "            model = model\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            y_pred_valid = model.predict(X_valid).reshape(-1,)\n",
    "            score = mean_absolute_error(y_valid, y_pred_valid)\n",
    "            y_pred = model.predict(X_test).reshape(-1,)\n",
    "            print(f'Fold {fold_n}. MAE: {score:.4f}.')\n",
    "            print('')\n",
    "        \n",
    "        if model_type == 'cat':\n",
    "            model = CatBoostRegressor(iterations=20000,  eval_metric='MAE', task_type='GPU', **params)\n",
    "            model.fit(X_train, y_train, eval_set=(X_valid, y_valid), cat_features=[], use_best_model=True,\n",
    "                      verbose=False)\n",
    "\n",
    "            y_pred_valid = model.predict(X_valid)\n",
    "            y_pred = model.predict(X_test)\n",
    "        \n",
    "        if model_type == 'gdi':\n",
    "            y_pred_valid = gpi(X_valid).values\n",
    "            y_pred = gpi(X_test).values\n",
    "        \n",
    "        oof[valid_index] = y_pred_valid.reshape(-1,)\n",
    "        scores.append(mean_absolute_error(y_valid, y_pred_valid))\n",
    "        \n",
    "        prediction += y_pred    \n",
    "        \n",
    "        if model_type == 'lgb':\n",
    "            # feature importance\n",
    "            fold_importance = pd.DataFrame()\n",
    "            fold_importance['feature'] = X.columns\n",
    "            fold_importance['importance'] = model.feature_importances_\n",
    "            fold_importance['fold'] = fold_n + 1\n",
    "            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n",
    "\n",
    "    prediction /= n_fold\n",
    "    \n",
    "    if show_scatter:\n",
    "        fig, axis = plt.subplots(1, 2, figsize=(12,5))\n",
    "        ax1, ax2 = axis\n",
    "        ax1.set_xlabel('actual')\n",
    "        ax1.set_ylabel('predicted')\n",
    "        ax2.set_xlabel('train index')\n",
    "        ax2.set_ylabel('time to failure')\n",
    "        \n",
    "        ax1.scatter(y, oof, color='brown')\n",
    "        ax1.plot([(0, 0), (20, 20)], [(0, 0), (20, 20)], color='blue')\n",
    "\n",
    "        ax2.plot(y, color='blue', label='y_train')\n",
    "        ax2.plot(oof, color='orange')\n",
    "    \n",
    "    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n",
    "    \n",
    "    if model_type == 'lgb':\n",
    "        feature_importance['importance'] /= n_fold\n",
    "        return oof, prediction, np.mean(scores), np.std(scores), feature_importance\n",
    "    else:\n",
    "        return oof, prediction, np.mean(scores), np.std(scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fold_features = 5\n",
    "folds_features = KFold(n_splits=n_fold_features, shuffle=True, random_state=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'num_leaves': 54,\n",
    "    'min_data_in_leaf': 79,\n",
    "    'objective': 'huber',\n",
    "    'max_depth': -1,\n",
    "    'learning_rate': 0.01,\n",
    "    'boosting': 'gbdt',\n",
    "    'bagging_freq': 5,\n",
    "    'bagging_fraction': 0.8126672064208567,\n",
    "    'bagging_seed': 11,\n",
    "    'metric': 'mae',\n",
    "    'verbosity': -1,\n",
    "    'reg_alpha': 0.1302650970728192,\n",
    "    'reg_lambda': 0.3603427518866501\n",
    "}\n",
    "oof_lgb, prediction_lgb, score_mean_lgb, score_std_lgb, feature_importance_lgb = train_model(\n",
    "    X=X_train_scaled,\n",
    "    X_test=X_test_scaled,\n",
    "    y=y_tr,\n",
    "    folds=folds_features,\n",
    "    params=params,\n",
    "    model_type='lgb',\n",
    "    show_scatter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After building a simple mode, let's see which are the important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_num_features = 216\n",
    "cols_feat = (feature_importance_lgb[['feature', 'importance']]\n",
    "        .groupby('feature')\n",
    "        .mean()\n",
    "        .sort_values(by='importance', ascending=False)[:max_num_features].index)\n",
    "best_features = feature_importance_lgb.loc[feature_importance_lgb.feature.isin(cols_feat)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,26))\n",
    "sns.barplot(x='importance', y='feature', data=feature_importance_lgb.sort_values(by='importance', ascending=False))\n",
    "plt.title('LightGBM Features (averaged over folds)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Clearly, come features do not seem to bring any value to the mix. Let's visualize it carefully in case there is something wrong with them (or even if they make sense)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_features_list = cols_feat.tolist()\n",
    "X_train_scaled_best = X_train_scaled[best_features_list]\n",
    "X_test_scaled_best = X_test_scaled[best_features_list]\n",
    "n_fold_models = 5\n",
    "folds_models = KFold(n_splits=n_fold_models, shuffle=True, random_state=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "Let's try a few different models and submit the one with the best validation score. The predicted values in the following plots are using a out-of-fold scheme."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LGBM (Gradient Boosting)\n",
    "Gradient boosting that uses tree based learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first search for the best parameters for this model. It is not possible to use `GridSearchCV` with early stopping (lightgbm), so I am using a custom function for random search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_params = {\n",
    "    'objective': 'huber',\n",
    "    'boosting': 'gbdt',\n",
    "    'verbosity': -1,\n",
    "    'random_seed': 19,\n",
    "    'n_estimators': 50000,\n",
    "    'metric': 'mae',\n",
    "    'bagging_seed': 11\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'num_leaves': list(range(8, 92, 4)),\n",
    "    'min_data_in_leaf': [10, 20, 40, 60, 100],\n",
    "    'max_depth': [3, 4, 5, 6, 8, 12, 16, -1],\n",
    "    'learning_rate': [0.1, 0.05, 0.01, 0.005],\n",
    "    'bagging_freq': [3, 4, 5, 6, 7],\n",
    "    'bagging_fraction': np.linspace(0.6, 0.95, 10),\n",
    "    'reg_alpha': np.linspace(0.1, 0.95, 10),\n",
    "    'reg_lambda': np.linspace(0.1, 0.95, 10)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_size = 1\n",
    "for param in param_grid:\n",
    "    grid_size *= len(param_grid[param])\n",
    "print(f'The search grid has {grid_size} elements')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_score = 9999\n",
    "dataset = lgb.Dataset(X_train_scaled_best, label=y_tr)  # no need to scale features\n",
    "\n",
    "scores_val_mean = []\n",
    "scores_val_std = []\n",
    "for i in tqdm_notebook(range(200)):\n",
    "    params = {k: random.choice(v) for k, v in param_grid.items()}\n",
    "    params.update(fixed_params)\n",
    "    result = lgb.cv(params,\n",
    "                    dataset,\n",
    "                    nfold=n_fold_models,\n",
    "                    early_stopping_rounds=200,\n",
    "                    stratified=False)\n",
    "    \n",
    "    print(f\"Iteration {i} finished with mae={result['l1-mean'][-1]:.4f} and std={result['l1-stdv'][-1]:.4f}\")\n",
    "    scores_val_mean.append(result['l1-mean'][-1])\n",
    "    scores_val_std.append(result['l1-stdv'][-1])\n",
    "    \n",
    "    if result['l1-mean'][-1] < best_score:\n",
    "        best_score = result['l1-mean'][-1]\n",
    "        best_params = params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 6))\n",
    "plt.scatter(scores_val_mean, scores_val_std, color='blue')\n",
    "plt.scatter(best_score, best_score_std, color='gold')\n",
    "plt.xlabel('scores_val_mean')\n",
    "plt.ylabel('scores_val_std')\n",
    "plt.title('Validation score mean/std scatter plot')\n",
    "plt.grid()\n",
    "plt.legend(['All parameters', 'Best'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"best_score={best_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'objective': 'huber',\n",
    "    'boosting': 'gbdt',\n",
    "    'verbosity': -1,\n",
    "    'num_leaves': 8, #54,\n",
    "    'min_data_in_leaf': 100, #79,\n",
    "    'max_depth': 6, #-1,\n",
    "    'learning_rate': 0.01,\n",
    "    'bagging_freq': 3, #5,\n",
    "    'bagging_fraction': 0.7166666666666667, #0.8126672064208567,\n",
    "    'bagging_seed': 11,\n",
    "    'metric': 'mae',\n",
    "    'reg_alpha': 0.19444444444444445, #0.1302650970728192,\n",
    "    'reg_lambda': 0.8555555555555555 #0.3603427518866501\n",
    "}\n",
    "oof_lgb, prediction_lgb, score_mean_lgb, score_std_lgb, feature_importance = train_model(X=X_train_scaled_best,\n",
    "                                                                                         X_test=X_test_scaled_best,\n",
    "                                                                                         y=y_tr,\n",
    "                                                                                         folds=folds_models,\n",
    "                                                                                         params=params,\n",
    "                                                                                         model_type='lgb',\n",
    "                                                                                         show_scatter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost (Gradient Boosting)\n",
    "Gradient boosting that uses tree based learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first search for the best parameters for this model. It is not possible to use `GridSearchCV` with early stopping, so I am making my own gird search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_params = {\n",
    "    'objective': 'reg:linear',\n",
    "    'eval_metric': 'mae',\n",
    "    'silent': True,\n",
    "    'nthread': 32\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'eta': [0.01, 0.015, 0.02, 0.025, 0.03, 0.1, 1, 5, 10], #Andrew uses 0.03\n",
    "    'max_depth': [2, 4, 5, 6, 7, 8, 9, 10, 16, 32, 64, 128],\n",
    "    'subsample': [0.1, 0.25, 0.5, 0.75, 0.9]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_size = 1\n",
    "for param in param_grid:\n",
    "    grid_size *= len(param_grid[param])\n",
    "print(f'The search grid has {grid_size} elements')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random search on the parameter grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_score = 9999\n",
    "\n",
    "dataset = xgb.DMatrix(data=X_train_scaled_best, label=y_tr, feature_names=X_train_scaled_best.columns)\n",
    "scores_val_mean = []\n",
    "scores_val_std = []\n",
    "for i in tqdm_notebook(range(500)):\n",
    "    params = {k: random.choice(v) for k, v in param_grid.items()}\n",
    "    params.update(fixed_params)\n",
    "    result = xgb.cv(params,\n",
    "                    dataset,\n",
    "                    nfold=n_fold_models,\n",
    "                    num_boost_round=20000,\n",
    "                    early_stopping_rounds=200,\n",
    "                    stratified=False)\n",
    "    \n",
    "    print(f\"Iteration {i} finished with mae={result['test-mae-mean'].iloc[-1]:.4f} and std={result['test-mae-std'].iloc[-1]:.4f}\")\n",
    "    scores_val_mean.append(result['test-mae-mean'].iloc[-1])\n",
    "    scores_val_std.append(result['test-mae-std'].iloc[-1])\n",
    "          \n",
    "    if result['test-mae-mean'].iloc[-1] < best_score:\n",
    "        best_score = result['test-mae-mean'].iloc[-1]\n",
    "        best_score_std = result['test-mae-std'].iloc[-1]\n",
    "        best_params = params        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 6))\n",
    "plt.scatter(scores_val_mean, scores_val_std, color='blue')\n",
    "plt.scatter(best_score, best_score_std, color='gold')\n",
    "plt.xlim([2, 3])\n",
    "plt.ylim([0, 0.1])\n",
    "plt.xlabel('scores_val_mean')\n",
    "plt.ylabel('scores_val_std')\n",
    "plt.title('Validation score mean/std scatter plot')\n",
    "plt.grid()\n",
    "plt.legend(['All parameters', 'Best'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"best_score={best_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "xgb_params = {\n",
    "    'eta': 0.01, #Andrew uses 0.03\n",
    "    'max_depth': 6, #Andrew uses 10\n",
    "    'subsample': 0.5, #Andrew uses 0.9\n",
    "    'objective': 'reg:linear',\n",
    "    'eval_metric': 'mae',\n",
    "    'silent': True,\n",
    "    'nthread': 32\n",
    "} # CV mean score: 2.0801, std: 0.0711.\n",
    "oof_xgb, prediction_xgb, score_mean_xgb, score_std_xgb = train_model(X=X_train_scaled_best,\n",
    "                                                                     X_test=X_test_scaled_best,\n",
    "                                                                     y=y_tr,\n",
    "                                                                     folds=folds_models,\n",
    "                                                                     params=xgb_params,\n",
    "                                                                     model_type='xgb',\n",
    "                                                                     show_scatter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM\n",
    "Similar to NuSVC, for regression, uses a parameter nu to control the number of support vectors. However, unlike NuSVC, where nu replaces C, here nu replaces the parameter epsilon of epsilon-SVR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first search for the best parameters ($\\nu$ and $C$) for this model using `GridSearchCV`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = [{'nu': np.linspace(0.15, 0.99, 50),\n",
    "         'C': np.linspace(0.15, 5, 50)}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "target = y_tr.values.flatten()\n",
    "params, cv_results = grid_search_cv(NuSVR(gamma='scale', tol=0.01), grid, X_train_scaled_best, target, n_fold_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "scores_mean = cv_results['mean_test_score'].reshape(len(grid[0]['nu']), len(grid[0]['C']))\n",
    "scores_std = cv_results['std_test_score'].reshape(len(grid[0]['nu']), len(grid[0]['C']))\n",
    "\n",
    "plt.subplots(figsize=(16, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(scores_mean, interpolation='nearest', cmap=plt.cm.hot)\n",
    "plt.xlabel('nu')\n",
    "plt.ylabel('C')\n",
    "plt.colorbar()\n",
    "plt.xticks(np.arange(len(grid[0]['nu'])), grid[0]['nu'], rotation=45)\n",
    "plt.yticks(np.arange(len(grid[0]['C'])), grid[0]['C'])\n",
    "plt.title('Validation mean score')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(scores_std, interpolation='nearest', cmap=plt.cm.hot)\n",
    "plt.xlabel('nu')\n",
    "plt.ylabel('C')\n",
    "plt.colorbar()\n",
    "plt.xticks(np.arange(len(grid[0]['nu'])), grid[0]['nu'], rotation=45)\n",
    "plt.yticks(np.arange(len(grid[0]['C'])), grid[0]['C'])\n",
    "plt.title('Validation std score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can put the best parameters in builidng the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#model = NuSVR(gamma='scale', nu=0.9, C=10.0, tol=0.01) #original values\n",
    "model = NuSVR(gamma='scale', nu=0.63, C=0.4469387755102041, tol=0.01)\n",
    "oof_svr, prediction_svr, score_mean_svr, score_std_svr = train_model(X=X_train_scaled_best,\n",
    "                                                                     X_test=X_test_scaled_best,\n",
    "                                                                     y=y_tr,\n",
    "                                                                     folds=folds_models,\n",
    "                                                                     params=None,\n",
    "                                                                     model_type='sklearn',\n",
    "                                                                     model=model,\n",
    "                                                                     show_scatter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_params = {\n",
    "    'loss_function':'MAE'\n",
    "}\n",
    "oof_cat, prediction_cat, score_mean_cat, score_std_cat = train_model(X=X_train_scaled_best,\n",
    "                                                                     X_test=X_test_scaled_best,\n",
    "                                                                     y=y_tr,\n",
    "                                                                     folds=folds_models,\n",
    "                                                                     params=cat_params,\n",
    "                                                                     model_type='cat',\n",
    "                                                                     show_scatter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel Ridge\n",
    "This model combines regularized linear regression with a given kernel (radial basis in this case)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first search for the best parameters ($\\alpha$ and $\\gamma$) for this model using `GridSearchCV`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = [{\n",
    "    'alpha': np.linspace(0.001, 10, 50),\n",
    "    'gamma': np.linspace(0.00001, 0.03, 50)\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "target = y_tr.values.flatten()\n",
    "params, cv_results = grid_search_cv(KernelRidge(kernel='rbf'), grid, X_train_scaled_best, target, n_fold_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_mean = cv_results['mean_test_score'].reshape(len(grid[0]['alpha']), len(grid[0]['gamma']))\n",
    "scores_std = cv_results['std_test_score'].reshape(len(grid[0]['alpha']), len(grid[0]['gamma']))\n",
    "\n",
    "plt.subplots(figsize=(16, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(scores_mean, interpolation='nearest', cmap=plt.cm.hot)\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('gamma')\n",
    "plt.colorbar()\n",
    "plt.xticks(np.arange(len(grid[0]['alpha'])), grid[0]['alpha'], rotation=45)\n",
    "plt.yticks(np.arange(len(grid[0]['gamma'])), grid[0]['gamma'])\n",
    "plt.title('Validation accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(scores_std, interpolation='nearest', cmap=plt.cm.hot)\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('gamma')\n",
    "plt.colorbar()\n",
    "plt.xticks(np.arange(len(grid[0]['alpha'])), grid[0]['alpha'], rotation=45)\n",
    "plt.yticks(np.arange(len(grid[0]['gamma'])), grid[0]['gamma'])\n",
    "plt.title('Validation accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can put the best parameters in builidng the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = KernelRidge(kernel='rbf', alpha=0.1, gamma=0.01) #Original parameters\n",
    "model = KernelRidge(kernel='rbf', alpha=2.4497346938775513, gamma=0.0018461224489795917)\n",
    "oof_r, prediction_r, score_mean_r, score_std_r = train_model(X=X_train_scaled_best,\n",
    "                                                             X_test=X_test_scaled_best,\n",
    "                                                             y=y_tr,\n",
    "                                                             folds=folds_models,\n",
    "                                                             params=None,\n",
    "                                                             model_type='sklearn',\n",
    "                                                             model=model,\n",
    "                                                             show_scatter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Genetic Program Model\n",
    "Genetic programming model from https://www.kaggle.com/scirpus/andrews-script-plus-a-genetic-program-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are no tuning here. We took at as face value.\n",
    "oof_gdi, prediction_gdi, score_mean_gdi, score_std_gdi = train_model(X=X_train_scaled_best,\n",
    "                                                                     X_test=X_test_scaled_best,\n",
    "                                                                     y=y_tr,\n",
    "                                                                     folds=folds_models,\n",
    "                                                                     model_type='gdi',\n",
    "                                                                     show_scatter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "This regressor fits many decision trees with different subsets of the original data and average the predictions between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = [{\n",
    "    'max_depth': [8, 10, 12],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'min_samples_leaf': [2, 4, 8],\n",
    "    'min_samples_split': [2, 4, 8]\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = y_tr.values.flatten()\n",
    "params, cv_results = grid_search_cv(RandomForestRegressor(criterion='mae', n_estimators=200, n_jobs=-1),\n",
    "                                    grid,\n",
    "                                    X_train_scaled_best,\n",
    "                                    target,\n",
    "                                    n_fold_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_params = {\n",
    "    'max_depth': 12, #8,\n",
    "    'max_features': 'log2', #'auto',\n",
    "    'min_samples_leaf': 2,\n",
    "    'min_samples_split': 4 #6\n",
    "} #CV mean score: 2.0448, std: 0.0793.\n",
    "model = RandomForestRegressor(criterion='mae', n_estimators=200, n_jobs=-1, **rf_params)\n",
    "oof_rf, prediction_rf, score_mean_rf, score_std_rf = train_model(X=X_train_scaled_best,\n",
    "                                                                 X_test=X_test_scaled_best,\n",
    "                                                                 y=y_tr,\n",
    "                                                                 folds=folds_models,\n",
    "                                                                 params=rf_params,\n",
    "                                                                 model_type='sklearn',\n",
    "                                                                 model=model,\n",
    "                                                                 show_scatter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extremely Randomized Trees\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = [{\n",
    "    'max_depth': [5, 8, 10, 12],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'min_samples_leaf': [2, 4, 8],\n",
    "    'min_samples_split': [2, 4, 8, 16]\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = y_tr.values.flatten()\n",
    "params, cv_results = grid_search_cv(ExtraTreesRegressor(criterion='mae', n_estimators=200, n_jobs=-1),\n",
    "                                    grid,\n",
    "                                    X_train_scaled_best,\n",
    "                                    target,\n",
    "                                    n_fold_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_params = {\n",
    "    'max_depth': 12,\n",
    "    'max_features': 'sqrt',\n",
    "    'min_samples_leaf': 2,\n",
    "    'min_samples_split': 4\n",
    "}\n",
    "model = ExtraTreesRegressor(criterion='mae', n_estimators=200, n_jobs=-1, **ex_params)\n",
    "oof_ex, prediction_ex, score_mean_ex, score_std_ex = train_model(X=X_train_scaled_best,\n",
    "                                                                 X_test=X_test_scaled_best,\n",
    "                                                                 y=y_tr,\n",
    "                                                                 folds=folds_models,\n",
    "                                                                 params=ex_params,\n",
    "                                                                 model_type='sklearn',\n",
    "                                                                 model=model,\n",
    "                                                                 show_scatter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ada Boost\n",
    "AdaBoost begins by fitting a base estimator on the original dataset and then fits additional copies on the same dataset. At each iteration (estimator), the weights of instances are adjusted according to the error of the last prediction. It's similar to the next model, but gradient boosting fits additional estimator copies on the current error and not on the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = [{\n",
    "    'learning_rate': np.linspace(0.0001, 0.1, 5)\n",
    "}]\n",
    "#base = Ridge(alpha=10)\n",
    "base = KernelRidge(kernel='rbf', alpha=2.4497346938775513, gamma=0.0018461224489795917)\n",
    "params = grid_search_cv(AdaBoostRegressor(base_estimator=base, n_estimators=100),\n",
    "                        grid,\n",
    "                        X_train_scaled,\n",
    "                        target,\n",
    "                        n_fold_models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_params = {\n",
    "    'learning_rate': 0.001\n",
    "}\n",
    "model = AdaBoostRegressor(base_estimator=base, n_estimators=100, **ada_params)\n",
    "oof_ada, prediction_ada, score_mean_ada, score_std_ada = train_model(X=X_train_scaled_best,\n",
    "                                                                     X_test=X_test_scaled_best,\n",
    "                                                                     y=y_tr,\n",
    "                                                                     folds=folds_models,\n",
    "                                                                     params=ada_params,\n",
    "                                                                     model_type='sklearn',\n",
    "                                                                     model=model,\n",
    "                                                                     show_scatter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NeuralNet\n",
    "This is a simple FC model mimiquing the previous genetic algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display training progress by printing a single dot for each completed epoch\n",
    "class PrintDot(callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        if epoch % 100 == 0:\n",
    "            print('')\n",
    "        else:\n",
    "            print('.', end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    hist = pd.DataFrame(history.history)\n",
    "    hist['epoch'] = history.epoch\n",
    "  \n",
    "    plt.figure()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Mean Abs Error')\n",
    "    plt.plot(hist['epoch'], hist['mean_absolute_error'],\n",
    "             label='Train Error')\n",
    "    plt.plot(hist['epoch'], hist['val_mean_absolute_error'],\n",
    "             label = 'Val Error')\n",
    "    plt.ylim([0,5])\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'dropout': [0.4, 0.5, 0.7, 0.8, 0.9],\n",
    "    'num_layers' : [2, 4, 6],\n",
    "    'num_neurons' : [64, 128, 256]\n",
    "}\n",
    "grid = ParameterGrid(param_grid)\n",
    "results_nn = pd.DataFrame(columns=['dropout', 'num_layers', 'num_neurons', 'val_score'])\n",
    "\n",
    "for params in tqdm_notebook(grid):\n",
    "    dropout = params['dropout']\n",
    "    num_layers = params['num_layers']\n",
    "    num_neurons = params['num_neurons']\n",
    "    print(f\"Training model with {params}\")\n",
    "    model = Sequential()\n",
    "    model.add(layers.Dense(1024, input_dim=216, activation=tf.nn.relu))\n",
    "    model.add(layers.Dropout(dropout))\n",
    "    for l in range(num_layers):\n",
    "        model.add(layers.Dense(num_neurons, input_dim=216, activation=tf.nn.relu))\n",
    "        model.add(layers.Dropout(dropout))\n",
    "    model.add(layers.Dense(1))\n",
    "\n",
    "    model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
    "\n",
    "    EPOCHS = 1000\n",
    "    early_stop = callbacks.EarlyStopping(monitor='mean_absolute_error', patience=100)\n",
    "    history = model.fit(\n",
    "        X_train_scaled_best,\n",
    "        y_tr,\n",
    "        epochs=EPOCHS,\n",
    "        validation_split=0.5,\n",
    "        verbose=0,\n",
    "        callbacks=[early_stop, PrintDot()])\n",
    "    hist = pd.DataFrame(history.history)\n",
    "    val_score = hist['val_mean_absolute_error'].iloc[-1]\n",
    "    print(f'val_score={val_score}')\n",
    "    plot_history(history)\n",
    "\n",
    "    results_nn = results_nn.append({'dropout': dropout,\n",
    "                                    'num_layers' : num_layers,\n",
    "                                    'num_neurons' : num_neurons,\n",
    "                                    'val_score' : val_score},\n",
    "                                   ignore_index=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(results_nn.index, results_nn['val_score'])\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = backend.get_session()\n",
    "sess.close()\n",
    "backend.clear_session()\n",
    "gc.collect()\n",
    "tf.keras.backend.clear_session()\n",
    "sess = tf.keras.backend.get_session()\n",
    "sess.close()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout = 0.4\n",
    "num_layers = 3\n",
    "num_neurons = 64\n",
    "model = Sequential()\n",
    "model.add(layers.Dense(1024, input_dim=216, activation=tf.nn.relu))\n",
    "model.add(layers.Dropout(dropout))\n",
    "for l in range(num_layers):\n",
    "    model.add(layers.Dense(num_neurons, input_dim=216, activation=tf.nn.relu))\n",
    "    model.add(layers.Dropout(dropout))\n",
    "model.add(layers.Dense(1))\n",
    "\n",
    "model = multi_gpu_model(model, gpus=2, cpu_merge=False)\n",
    "model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
    "EPOCHS = 1000\n",
    "early_stop = callbacks.EarlyStopping(monitor='mean_absolute_error', patience=100)\n",
    "history = model.fit(\n",
    "    X_train_scaled_best,\n",
    "    y_tr,\n",
    "    validation_split=0.5,\n",
    "    epochs=EPOCHS,\n",
    "    verbose=0,\n",
    "    callbacks=[early_stop, PrintDot()])\n",
    "hist = pd.DataFrame(history.history)\n",
    "val_score = hist['val_mean_absolute_error'].iloc[-1]\n",
    "print(f'val_score={val_score}')\n",
    "plot_history(history)\n",
    "\n",
    "results_nn = results_nn.append({'dropout': dropout,\n",
    "                                'num_layers' : num_layers,\n",
    "                                'num_neurons' : num_neurons,\n",
    "                                'val_score' : val_score},\n",
    "                               ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axis = plt.subplots(1, 2, figsize=(12,5))\n",
    "ax1, ax2 = axis\n",
    "ax1.set_xlabel('actual')\n",
    "ax1.set_ylabel('predicted')\n",
    "ax2.set_xlabel('train index')\n",
    "ax2.set_ylabel('time to failure')\n",
    "        \n",
    "ax1.scatter(y_tr, oof_nn, color='brown')\n",
    "ax1.plot([(0, 0), (20, 20)], [(0, 0), (20, 20)], color='blue')\n",
    "\n",
    "ax2.plot(y_tr, color='blue', label='y_train')\n",
    "ax2.plot(oof_nn, color='orange')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_nn, prediction_nn, score_mean_nn, score_std_nn = train_model(X=X_train_scaled_best,\n",
    "                                                                 X_test=X_test_scaled_best,\n",
    "                                                                 y=y_tr,\n",
    "                                                                 folds=folds_models,\n",
    "                                                                 params=params,\n",
    "                                                                 model_type='nn',\n",
    "                                                                 show_scatter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking and blending\n",
    "And now let's try stacking :) We can use the same function for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stack = np.vstack([oof_lgb, oof_xgb, oof_svr, oof_r, oof_cat, oof_gdi, oof_rf, oof_ex,\n",
    "                         oof_ada]).transpose()\n",
    "train_stack = pd.DataFrame(train_stack, columns = ['lgb', 'xgb', 'svr', 'r', 'cat', 'gdi', 'rf', 'ex', 'ada'])\n",
    "test_stack = np.vstack([prediction_lgb, prediction_xgb, prediction_svr, prediction_r,\n",
    "                        prediction_cat, prediction_gdi, prediction_rf, prediction_ex, prediction_ada]).transpose()\n",
    "test_stack = pd.DataFrame(test_stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_params = {\n",
    "    'objective': 'huber',\n",
    "    'boosting': 'gbdt',\n",
    "    'verbosity': -1,\n",
    "    'random_seed': 19,\n",
    "    'n_estimators': 50000,\n",
    "    'metric': 'mae',\n",
    "    'bagging_seed': 11\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'num_leaves': list(range(8, 92, 4)),\n",
    "    'min_data_in_leaf': [10, 20, 40, 60, 100],\n",
    "    'max_depth': [3, 4, 5, 6, 8, 12, 16, -1],\n",
    "    'learning_rate': [0.1, 0.05, 0.01, 0.005],\n",
    "    'bagging_freq': [3, 4, 5, 6, 7],\n",
    "    'bagging_fraction': np.linspace(0.6, 0.95, 10),\n",
    "    'reg_alpha': np.linspace(0.1, 0.95, 10),\n",
    "    'reg_lambda': np.linspace(0.1, 0.95, 10)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_score = 9999\n",
    "dataset = lgb.Dataset(train_stack, label=y_tr)  # no need to scale features\n",
    "\n",
    "scores_val_mean = []\n",
    "scores_val_std = []\n",
    "for i in tqdm_notebook(range(200)):\n",
    "    params = {k: random.choice(v) for k, v in param_grid.items()}\n",
    "    params.update(fixed_params)\n",
    "    result = lgb.cv(params,\n",
    "                    dataset,\n",
    "                    nfold=n_fold_models,\n",
    "                    early_stopping_rounds=200,\n",
    "                    stratified=False)\n",
    "    \n",
    "    print(f\"Iteration {i} finished with mae={result['l1-mean'][-1]:.4f} and std={result['l1-stdv'][-1]:.4f}\")\n",
    "    scores_val_mean.append(result['l1-mean'][-1])\n",
    "    scores_val_std.append(result['l1-stdv'][-1])\n",
    "    \n",
    "    if result['l1-mean'][-1] < best_score:\n",
    "        best_score = result['l1-mean'][-1]\n",
    "        best_params = params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 6))\n",
    "plt.scatter(scores_val_mean, scores_val_std, color='blue')\n",
    "plt.scatter(best_score, best_score_std, color='gold')\n",
    "plt.xlabel('scores_val_mean')\n",
    "plt.ylabel('scores_val_std')\n",
    "plt.title('Validation score mean/std scatter plot')\n",
    "plt.grid()\n",
    "plt.legend(['All parameters', 'Best'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"best_score={best_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'objective': 'huber',\n",
    "    'boosting': 'gbdt',\n",
    "    'verbosity': -1,\n",
    "    'num_leaves': 8, #54,\n",
    "    'min_data_in_leaf': 100, #79,\n",
    "    'max_depth': 5, #-1,\n",
    "    'learning_rate': 0.01, #0.01,\n",
    "    'bagging_freq': 5, #5,\n",
    "    'bagging_fraction': 0.7555555555555555, #0.8126672064208567,\n",
    "    'bagging_seed': 11,\n",
    "    'metric': 'mae',\n",
    "    'reg_alpha': 0.95, #0.1302650970728192,\n",
    "    'reg_lambda': 0.19444444444444445 #0.3603427518866501\n",
    "}\n",
    "oof_lgb_stack, prediction_lgb_stack, score_mean_stack, score_std_stack, feature_importance = train_model(\n",
    "    X=train_stack,\n",
    "    X_test=test_stack,\n",
    "    y=y_tr,\n",
    "    folds=folds_models,\n",
    "    params=params,\n",
    "    model_type='lgb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = feature_importance[['feature', 'importance']].groupby('feature').mean().sort_values(\n",
    "    by='importance', ascending=False).index\n",
    "best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n",
    "plt.figure(figsize=(16, 8));\n",
    "sns.barplot(x='importance', y='feature', data=best_features.sort_values(by='importance', ascending=False));\n",
    "plt.title('LGB Features (avg over folds)');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 18))\n",
    "plt.subplot(4, 3, 1)\n",
    "plt.plot(y_tr, label='y_train')\n",
    "plt.plot(oof_lgb, label='lgb')\n",
    "plt.legend()\n",
    "plt.title('lgb');\n",
    "\n",
    "plt.subplot(4, 3, 2)\n",
    "plt.plot(y_tr, label='y_train')\n",
    "plt.plot(oof_xgb, label='xgb')\n",
    "plt.legend()\n",
    "plt.title('xgb');\n",
    "\n",
    "plt.subplot(4, 3, 3)\n",
    "plt.plot(y_tr, label='y_train')\n",
    "plt.plot(oof_svr, label='svr')\n",
    "plt.legend()\n",
    "plt.title('svr');\n",
    "\n",
    "plt.subplot(4, 3, 4)\n",
    "plt.plot(y_tr, label='y_train')\n",
    "plt.plot(oof_r, label='r')\n",
    "plt.legend()\n",
    "plt.title('r');\n",
    "\n",
    "plt.subplot(4, 3, 5)\n",
    "plt.plot(y_tr, label='y_train')\n",
    "plt.plot(oof_cat, label='cat')\n",
    "plt.legend()\n",
    "plt.title('cat');\n",
    "\n",
    "plt.subplot(4, 3, 6)\n",
    "plt.plot(y_tr, label='y_train')\n",
    "plt.plot(oof_gdi, label='gdi')\n",
    "plt.legend()\n",
    "plt.title('gdi');\n",
    "\n",
    "plt.subplot(4, 3, 7)\n",
    "plt.plot(y_tr, label='y_train')\n",
    "plt.plot(oof_rf, label='rf')\n",
    "plt.legend()\n",
    "plt.title('rf');\n",
    "\n",
    "plt.subplot(4, 3, 8)\n",
    "plt.plot(y_tr, label='y_train')\n",
    "plt.plot(oof_ex, label='ex')\n",
    "plt.legend()\n",
    "plt.title('ex');\n",
    "\n",
    "plt.subplot(4, 3, 9)\n",
    "plt.plot(y_tr, label='y_train')\n",
    "plt.plot(oof_ada, label='ada')\n",
    "plt.legend()\n",
    "plt.title('ada');\n",
    "\n",
    "plt.subplot(4, 3, 11)\n",
    "plt.plot(y_tr, label='y_train')\n",
    "plt.plot(oof_lgb_stack, label='stack')\n",
    "plt.legend()\n",
    "plt.title('stacked');\n",
    "\n",
    "plt.subplot(4, 3, 12)\n",
    "plt.plot(y_tr, label='y_train')\n",
    "plt.plot((oof_lgb + oof_xgb + oof_svr + oof_r + oof_cat + oof_gdi + oof_rf + oof_ex + oof_ada) / 9, label='blend')\n",
    "plt.legend()\n",
    "plt.title('blend');\n",
    "plt.suptitle('Predictions vs actual');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission - Blend model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['time_to_failure'] = (prediction_lgb + prediction_xgb + prediction_svr\n",
    "                                + prediction_cat + prediction_r + prediction_gdi\n",
    "                                + prediction_rf + prediction_ex + prediction_ada) / 9\n",
    "submission.to_csv('../output/submission_rdg_notebook3_blend.csv')\n",
    "score_mean_blend = (score_mean_lgb + score_mean_xgb + score_mean_svr + score_mean_cat\n",
    "                    + score_mean_r + score_mean_gdi + score_mean_rf + score_mean_ex + score_mean_ada) / 9\n",
    "score_std_blend = (score_std_lgb + score_std_xgb + score_std_svr + score_std_cat\n",
    "                    + score_std_r + score_std_gdi + score_std_rf + score_std_ex + score_std_ada) / 9\n",
    "print(f'CV mean score: {score_mean_blend:.4f}, std: {score_std_blend:.4f}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission - Stacked model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['time_to_failure'] = prediction_lgb_stack\n",
    "submission.to_csv('../output/submission_rdg_notebook3_stack.csv')\n",
    "print(f'CV mean score: {score_mean_stack:.4f}, std: {score_std_stack:.4f}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission - LGB model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['time_to_failure'] = prediction_lgb\n",
    "submission.to_csv('../output/submission_rdg_notebook3_lgb.csv')\n",
    "print(f'CV mean score: {score_mean_lgb:.4f}, std: {score_std_lgb:.4f}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission - XGB model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['time_to_failure'] = prediction_xgb\n",
    "submission.to_csv('../output/submission_rdg_notebook3_xgb.csv')\n",
    "print(f'CV mean score: {score_mean_xgb:.4f}, std: {score_std_xgb:.4f}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission - SVR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['time_to_failure'] = prediction_svr\n",
    "submission.to_csv('../output/submission_rdg_notebook3_svr.csv')\n",
    "print(f'CV mean score: {score_mean_svr:.4f}, std: {score_std_svr:.4f}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission - CAT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['time_to_failure'] = prediction_cat\n",
    "submission.to_csv('../output/submission_rdg_notebook3_cat.csv')\n",
    "print(f'CV mean score: {score_mean_cat:.4f}, std: {score_std_cat:.4f}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission - R model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['time_to_failure'] = prediction_r\n",
    "submission.to_csv('../output/submission_rdg_notebook3_r.csv')\n",
    "print(f'CV mean score: {score_mean_r:.4f}, std: {score_std_r:.4f}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission - GDI model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['time_to_failure'] = prediction_gdi\n",
    "submission.to_csv('../output/submission_rdg_notebook3_gdi.csv')\n",
    "print(f'CV mean score: {score_mean_gdi:.4f}, std: {score_std_gdi:.4f}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission - RF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['time_to_failure'] = prediction_rf\n",
    "submission.to_csv('../output/submission_rdg_notebook3_rf.csv')\n",
    "print(f'CV mean score: {score_mean_rf:.4f}, std: {score_std_rf:.4f}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission - EX model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['time_to_failure'] = prediction_ex\n",
    "submission.to_csv('../output/submission_rdg_notebook3_ex.csv')\n",
    "print(f'CV mean score: {score_mean_ex:.4f}, std: {score_std_ex:.4f}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission - ADA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['time_to_failure'] = prediction_ada\n",
    "submission.to_csv('../output/submission_rdg_notebook3_ada.csv')\n",
    "print(f'CV mean score: {score_mean_ada:.4f}, std: {score_std_ada:.4f}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = pd.DataFrame(dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation.loc['lgb', 'mean'] = score_mean_lgb\n",
    "evaluation.loc['lgb', 'std'] = score_std_lgb\n",
    "\n",
    "evaluation.loc['xgb', 'mean'] = score_mean_xgb\n",
    "evaluation.loc['xgb', 'std'] = score_std_xgb\n",
    "\n",
    "evaluation.loc['svr', 'mean'] = score_mean_svr\n",
    "evaluation.loc['svr', 'std'] = score_std_svr\n",
    "\n",
    "evaluation.loc['cat', 'mean'] = score_mean_cat\n",
    "evaluation.loc['cat', 'std'] = score_std_cat\n",
    "\n",
    "evaluation.loc['r', 'mean'] = score_mean_r\n",
    "evaluation.loc['r', 'std'] = score_std_r\n",
    "\n",
    "evaluation.loc['gdi', 'mean'] = score_mean_gdi\n",
    "evaluation.loc['gdi', 'std'] = score_std_gdi\n",
    "\n",
    "evaluation.loc['rf', 'mean'] = score_mean_rf\n",
    "evaluation.loc['rf', 'std'] = score_std_rf\n",
    "\n",
    "evaluation.loc['ex', 'mean'] = score_mean_ex\n",
    "evaluation.loc['ex', 'std'] = score_std_ex\n",
    "\n",
    "evaluation.loc['ada', 'mean'] = score_mean_ada\n",
    "evaluation.loc['ada', 'std'] = score_std_ada\n",
    "\n",
    "evaluation.loc['blend', 'mean'] = score_mean_blend\n",
    "evaluation.loc['blend', 'std'] = score_std_blend\n",
    "\n",
    "evaluation.loc['stack', 'mean'] = score_mean_stack\n",
    "evaluation.loc['stack', 'std'] = score_std_stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "evaluation.to_csv(f\"../output/evaluation_{str(datetime.datetime.now())}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(18, 10))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.bar(np.arange(len(evaluation.index.tolist())), evaluation['mean'], yerr=evaluation['std'], align='center',\n",
    "       alpha=0.5, ecolor='black', capsize=10)\n",
    "plt.ylabel('CV score')\n",
    "plt.xlabel('model')\n",
    "plt.xticks(np.arange(len(evaluation.index.tolist())), evaluation.index.tolist())\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.scatter(evaluation['mean'], evaluation['std'])\n",
    "\n",
    "model_list = evaluation.index.tolist()\n",
    "score_mean_list = evaluation['mean'].tolist()\n",
    "score_std_list = evaluation['std'].tolist()\n",
    "\n",
    "for i in range(len(model_list)):\n",
    "    plt.annotate(model_list[i], (score_mean_list[i]+0.001, score_std_list[i]))\n",
    "    \n",
    "plt.ylabel('std')\n",
    "plt.xlabel('mean')\n",
    "plt.grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
