{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Master jupyter notebook for LANL - SlimBros Team"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correctly predicting earthquakes is very important for preventing deaths and damage to infrastructure. In this competition we try to predict time left to the next laboratory earthquake based on seismic signal data. Training data represents one huge signal, but in test data we have many separate chunks, for each of which we need to predict time to failure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries\n",
    "Let's import everything we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import eli5\n",
    "import csv\n",
    "import dill\n",
    "import time\n",
    "import random\n",
    "import datetime\n",
    "import warnings\n",
    "import feather\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn import svm, neighbors, linear_model\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, AdaBoostRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import NuSVR, SVR\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.linear_model import Ridge, RidgeCV\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold, GridSearchCV, cross_val_score, ParameterGrid, train_test_split\n",
    "from utils import generate_segment_start_ids, compare_methods\n",
    "from features import gpi, create_all_features_extended\n",
    "from features import gpi_new, gpii_new, gpiii_new\n",
    "\n",
    "#Configure the environment\n",
    "%matplotlib inline\n",
    "pd.options.display.precision = 15\n",
    "warnings.filterwarnings('ignore')\n",
    "random.seed(1013)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load/compute the necessary features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_features = False \n",
    "# The computed features are saved in an hdf file along with the time_to_failure to \n",
    "# save the time spend reading the training data and the feature computation\n",
    "#train_data_format = 'csv'\n",
    "train_data_format = 'feather'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_data(file_format):\n",
    "    \"\"\"Load the training dataset.\"\"\"\n",
    "    print(f\"Loading data from {file_format} file:\", end=\"\")\n",
    "    if file_format.lower() == 'feather':\n",
    "        train_df = feather.read_dataframe('../input/train.feather')\n",
    "    else:\n",
    "        train_df = pd.read_csv('../input/train.csv', dtype={'acoustic_data': np.int16,\n",
    "                                                            'time_to_failure': np.float32})\n",
    "        feather.write_dataframe(train_df, '../input/train.feather')\n",
    "    print(\"Done\")\n",
    "    return train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from feather file:Done\n"
     ]
    }
   ],
   "source": [
    "train = load_train_data(train_data_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 16 quakes on the training set.\n"
     ]
    }
   ],
   "source": [
    "time_to_failure_delta = np.diff(train['time_to_failure'])\n",
    "init_times = np.where(time_to_failure_delta > 5)[0].tolist()\n",
    "print(f\"There are {len(init_times)} quakes on the training set.\")\n",
    "init_times = [0] + init_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'start_idx': init_times, 'end_idx': init_times[1:] + [len(time_to_failure_delta)]}\n",
    "quakes = pd.DataFrame(data=d)\n",
    "quakes.insert(2, 'valid', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's discard any quake that looks weird"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature generation\n",
    "- Usual aggregations: mean, std, min and max;\n",
    "- Average difference between the consequitive values in absolute and percent values;\n",
    "- Absolute min and max vallues;\n",
    "- Aforementioned aggregations for first and last 10000 and 50000 values - I think these data should be useful;\n",
    "- Max value to min value and their differencem also count of values bigger that 500 (arbitrary threshold);\n",
    "- Quantile features from this kernel: https://www.kaggle.com/andrekos/basic-feature-benchmark-with-quantiles\n",
    "- Trend features from this kernel: https://www.kaggle.com/jsaguiar/baseline-with-abs-and-trend-features\n",
    "- Rolling features from this kernel: https://www.kaggle.com/wimwim/rolling-quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_files_present = (os.path.isfile('../tmp_results/Xv2_tr.hdf') and \n",
    "                       os.path.isfile('../tmp_results/Xv2_test.hdf') and \n",
    "                       os.path.isfile('../tmp_results/yv2_tr.hdf') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dill.dump_session('notebook_env.db')\n",
    "#dill.load_session('notebook_env.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saved_files_present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading hdf files:Done\n"
     ]
    }
   ],
   "source": [
    "if (not compute_features) and saved_files_present:\n",
    "    print(f\"Reading hdf files:\", end=\"\")\n",
    "    X_tr = pd.read_hdf('../tmp_results/Xv2_tr.hdf', 'data')\n",
    "    X_test = pd.read_hdf('../tmp_results/Xv2_test.hdf', 'data')\n",
    "    y_tr = pd.read_hdf('../tmp_results/yv2_tr.hdf', 'data')  \n",
    "    print(\"Done\")\n",
    "else:\n",
    "    fs = 4000000 #Sampling frequency of the raw signal\n",
    "\n",
    "    #Compute features for the training data\n",
    "    segment_size = 150000\n",
    "    segment_start_ids = generate_segment_start_ids('uniform_no_jump', segment_size, train)\n",
    "    X_tr = pd.DataFrame(index=range(len(segment_start_ids)), dtype=np.float64)\n",
    "    y_tr = pd.DataFrame(index=range(len(segment_start_ids)), dtype=np.float64, columns=['time_to_failure'])\n",
    "    for idx in tqdm_notebook(range(len(segment_start_ids))):        \n",
    "        seg_id = segment_start_ids[idx]\n",
    "        seg = train.iloc[seg_id:seg_id + segment_size]\n",
    "        create_all_features_extended(idx, seg, X_tr, fs)\n",
    "        y_tr.loc[idx, 'time_to_failure'] = seg['time_to_failure'].values[-1]\n",
    "    # Sanity check\n",
    "    means_dict = {}\n",
    "    for col in X_tr.columns:\n",
    "        if X_tr[col].isnull().any():\n",
    "            print(col)\n",
    "            mean_value = X_tr.loc[X_tr[col] != -np.inf, col].mean()\n",
    "            X_tr.loc[X_tr[col] == -np.inf, col] = mean_value\n",
    "            X_tr[col] = X_tr[col].fillna(mean_value)\n",
    "            means_dict[col] = mean_value\n",
    "\n",
    "    #Compute features for the test data\n",
    "    submission = pd.read_csv('../input/sample_submission.csv', index_col='seg_id')\n",
    "    X_test = pd.DataFrame(columns=X_tr.columns, dtype=np.float64, index=submission.index)\n",
    "    for i, seg_id in enumerate(tqdm_notebook(X_test.index)):\n",
    "        seg = pd.read_csv('../input/test/' + seg_id + '.csv')\n",
    "        create_all_features_extended(seg_id, seg, X_test, fs)\n",
    "\n",
    "    # Sanity check\n",
    "    for col in X_test.columns:\n",
    "        if X_test[col].isnull().any():\n",
    "            X_test.loc[X_test[col] == -np.inf, col] = means_dict[col]\n",
    "            X_test[col] = X_test[col].fillna(means_dict[col])\n",
    "            \n",
    "    X_tr.to_hdf('../tmp_results/Xv2_tr.hdf', 'data')\n",
    "    X_test.to_hdf('../tmp_results/Xv2_test.hdf', 'data')\n",
    "    y_tr.to_hdf('../tmp_results/yv2_tr.hdf', 'data')\n",
    "    \n",
    "    del segment_start_ids\n",
    "    del means_dict\n",
    "    del submission\n",
    "    \n",
    "    print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldata = pd.concat([X_tr, X_test])\n",
    "scaler = StandardScaler()\n",
    "alldata = pd.DataFrame(scaler.fit_transform(alldata), columns=alldata.columns)\n",
    "X_train_scaled = alldata[:X_tr.shape[0]]\n",
    "X_test_scaled = alldata[X_tr.shape[0]:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X, X_test, y, folds, params=None, model_type='lgb',\n",
    "                model=None, show_scatter=False, force_positive=False):\n",
    "\n",
    "    oof = np.zeros(len(X))\n",
    "    prediction = np.zeros(len(X_test))\n",
    "    scores = []\n",
    "    n_fold = folds.get_n_splits()\n",
    "    feature_importance = pd.DataFrame()\n",
    "    for fold_n, (train_index, valid_index) in enumerate(folds.split(X)):\n",
    "        print('Fold', fold_n, 'started at', time.ctime())\n",
    "        X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n",
    "        y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n",
    "\n",
    "        if model_type == 'nn':\n",
    "            dropout = params['dropout']\n",
    "            num_layers = params['num_layers']\n",
    "            num_neurons = params['num_neurons']\n",
    "            activation_function = params['activation_function']\n",
    "            model = tf.keras.Sequential()\n",
    "            model.add(tf.keras.layers.Dense(1024, input_dim=216, activation=activation_function))\n",
    "            model.add(tf.keras.layers.BatchNormalization())\n",
    "            model.add(tf.keras.layers.Dropout(dropout))\n",
    "            for l in range(num_layers):\n",
    "                model.add(tf.keras.layers.Dense(num_neurons, activation=activation_function))\n",
    "                model.add(tf.keras.layers.BatchNormalization())\n",
    "                model.add(tf.keras.layers.Dropout(dropout))\n",
    "            model.add(tf.keras.layers.Dense(1))\n",
    "            model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
    "            EPOCHS = 1000\n",
    "            early_stop = tf.keras.callbacks.EarlyStopping(monitor='mean_absolute_error', patience=100)\n",
    "\n",
    "            history = model.fit(\n",
    "                X_train,\n",
    "                y_train,\n",
    "                epochs=EPOCHS,\n",
    "                validation_data = (X_valid, y_valid), \n",
    "                verbose=0,\n",
    "                callbacks=[early_stop, PrintDot()])\n",
    "            hist = pd.DataFrame(history.history)\n",
    "            val_score = hist['val_mean_absolute_error'].iloc[-1]\n",
    "            print(f'val_score={val_score}')\n",
    "            plot_history(history)\n",
    "        \n",
    "            y_pred_valid = model.predict(X_valid).reshape(-1,)\n",
    "            if force_positive:\n",
    "                y_pred_valid = y_pred_valid.clip(min=0)\n",
    "          \n",
    "            y_pred = model.predict(X_test).reshape(-1,)\n",
    "\n",
    "            score = mean_absolute_error(y_valid, y_pred_valid)\n",
    "            print(f'Fold {fold_n}. MAE: {score:.4f}.')\n",
    "        \n",
    "        if model_type == 'lgb':\n",
    "            model = lgb.LGBMRegressor(**params, n_estimators = 50000, n_jobs = -1)\n",
    "            model.fit(X_train, y_train, \n",
    "                      eval_set=[(X_train, y_train), (X_valid, y_valid)],\n",
    "                      eval_metric='mae',\n",
    "                      verbose=10000,\n",
    "                      early_stopping_rounds=2000)\n",
    "            \n",
    "            y_pred_valid = model.predict(X_valid)\n",
    "            if force_positive:\n",
    "                y_pred_valid = y_pred_valid.clip(min=0)\n",
    "          \n",
    "            y_pred = model.predict(X_test, num_iteration=model.best_iteration_)\n",
    "            \n",
    "        if model_type == 'xgb':\n",
    "            train_data = xgb.DMatrix(data=X_train, label=y_train, feature_names=X.columns)\n",
    "            valid_data = xgb.DMatrix(data=X_valid, label=y_valid, feature_names=X.columns)\n",
    "\n",
    "            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]\n",
    "            model = xgb.train(dtrain=train_data,\n",
    "                              num_boost_round=20000,\n",
    "                              evals=watchlist,\n",
    "                              early_stopping_rounds=200,\n",
    "                              verbose_eval=500,\n",
    "                              params=params)\n",
    "\n",
    "            y_pred_valid = model.predict(xgb.DMatrix(X_valid, feature_names=X.columns),\n",
    "                                         ntree_limit=model.best_ntree_limit)\n",
    "            if force_positive:\n",
    "                y_pred_valid = y_pred_valid.clip(min=0)\n",
    "          \n",
    "            y_pred = model.predict(xgb.DMatrix(X_test, feature_names=X.columns),\n",
    "                                   ntree_limit=model.best_ntree_limit)\n",
    "        \n",
    "        if model_type == 'sklearn':\n",
    "            model = model\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred_valid = model.predict(X_valid).reshape(-1,)\n",
    "            if force_positive:\n",
    "                y_pred_valid = y_pred_valid.clip(min=0)\n",
    "                \n",
    "            score = mean_absolute_error(y_valid, y_pred_valid)\n",
    "            y_pred = model.predict(X_test).reshape(-1,)\n",
    "            print(f'Fold {fold_n}. MAE: {score:.4f}.')\n",
    "            print('')\n",
    "        \n",
    "        if model_type == 'cat':\n",
    "            model = CatBoostRegressor(iterations=20000,  eval_metric='MAE', task_type='GPU', **params)\n",
    "            model.fit(X_train,\n",
    "                      y_train,\n",
    "                      eval_set=(X_valid, y_valid),\n",
    "                      cat_features=[],\n",
    "                      use_best_model=True,\n",
    "                      verbose=False)\n",
    "\n",
    "            y_pred_valid = model.predict(X_valid)\n",
    "            y_pred = model.predict(X_test)\n",
    "\n",
    "            if force_positive:\n",
    "                y_pred_valid = y_pred_valid.clip(min=0)\n",
    "\n",
    "        if model_type == 'gpi':\n",
    "            y_pred_valid = gpi(X_valid, activation=params['activation']).values\n",
    "            y_pred = gpi(X_test, activation=params['activation']).values\n",
    "            if force_positive:\n",
    "                y_pred_valid = y_pred_valid.clip(min=0)\n",
    "\n",
    "        if model_type == 'gpi_new':\n",
    "            y_pred_valid = gpi_new(X_valid, activation=params['activation']).values\n",
    "            y_pred = gpi_new(X_test, activation=params['activation']).values\n",
    "            if force_positive:\n",
    "                y_pred_valid = y_pred_valid.clip(min=0)\n",
    "            \n",
    "        if model_type == 'gpii_new':\n",
    "            y_pred_valid = gpii_new(X_valid, activation=params['activation']).values\n",
    "            y_pred = gpii_new(X_test, activation=params['activation']).values\n",
    "            if force_positive:\n",
    "                y_pred_valid = y_pred_valid.clip(min=0)\n",
    "                \n",
    "        if model_type == 'gpiii_new':\n",
    "            y_pred_valid = gpiii_new(X_valid, activation=params['activation']).values\n",
    "            y_pred = gpiii_new(X_test, activation=params['activation']).values\n",
    "            if force_positive:\n",
    "                y_pred_valid = y_pred_valid.clip(min=0)\n",
    "            \n",
    "        oof[valid_index] = y_pred_valid.reshape(-1,)\n",
    "        scores.append(mean_absolute_error(y_valid, y_pred_valid))\n",
    "        \n",
    "        prediction += y_pred    \n",
    "        \n",
    "        if model_type == 'lgb':\n",
    "            # feature importance\n",
    "            fold_importance = pd.DataFrame()\n",
    "            fold_importance['feature'] = X.columns\n",
    "            fold_importance['importance'] = model.feature_importances_\n",
    "            fold_importance['fold'] = fold_n + 1\n",
    "            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n",
    "\n",
    "    prediction /= n_fold\n",
    "    \n",
    "    if force_positive:\n",
    "        prediction = prediction.clip(min=0)    \n",
    "    \n",
    "    if show_scatter:\n",
    "        fig, axis = plt.subplots(1, 2, figsize=(12,5))\n",
    "        ax1, ax2 = axis\n",
    "        ax1.set_xlabel('actual')\n",
    "        ax1.set_ylabel('predicted')\n",
    "        ax2.set_xlabel('train index')\n",
    "        ax2.set_ylabel('time to failure')\n",
    "        \n",
    "        ax1.scatter(y, oof, color='brown')\n",
    "        ax1.plot([(0, 0), (20, 20)], [(0, 0), (20, 20)], color='blue')\n",
    "\n",
    "        ax2.plot(y, color='blue', label='y_train')\n",
    "        ax2.plot(oof, color='orange')\n",
    "    \n",
    "    print(f'CV mean score: {np.mean(scores):.4f}, std: {np.std(scores):.4f}.')\n",
    "    \n",
    "    if model_type == 'lgb':\n",
    "        feature_importance['importance'] /= n_fold\n",
    "        return oof, prediction, scores, feature_importance\n",
    "    else:\n",
    "        return oof, prediction, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fold = 5\n",
    "folds_models = KFold(n_splits=n_fold, shuffle=True, random_state=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a few different models and submit the one with the best validation score. The predicted values in the following plots are using a out-of-fold scheme."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LGBM (Gradient Boosting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_lgb_huber = {\n",
    "    \"objective\": \"huber\",\n",
    "    \"boosting\": \"gbdt\",\n",
    "    \"verbosity\": -1,\n",
    "    \"num_leaves\": 12,\n",
    "    \"min_data_in_leaf\": 40,\n",
    "    \"max_depth\": 8,\n",
    "    \"learning_rate\": 0.005,\n",
    "    \"bagging_freq\": 4,\n",
    "    \"bagging_fraction\": 0.6,\n",
    "    \"bagging_seed\": 11,\n",
    "    \"random_seed\": 19,\n",
    "    \"metric\": \"mae\",\n",
    "    \"reg_alpha\": 0.47777777777777775,\n",
    "    \"reg_lambda\": 0.47777777777777775\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_lgb_gamma = {\n",
    "    'num_leaves': 36,\n",
    "    'min_data_in_leaf': 10,\n",
    "    'max_depth': 12,\n",
    "    'learning_rate': 0.005,\n",
    "    'bagging_freq': 3,\n",
    "    'bagging_fraction': 0.7166666666666667,\n",
    "    'reg_alpha': 0.28888888888888886,\n",
    "    'reg_lambda': 0.95,\n",
    "    'objective': 'gamma',\n",
    "    'boosting': 'gbdt',\n",
    "    'verbosity': -1,\n",
    "    'random_seed': 19,\n",
    "    'metric': 'mae',\n",
    "    'bagging_seed': 11\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_lgb_fair = {\n",
    "    'num_leaves': 8,\n",
    "    'min_data_in_leaf': 60,\n",
    "    'max_depth': 3,\n",
    "    'learning_rate': 0.01,\n",
    "    'bagging_freq': 5,\n",
    "    'bagging_fraction': 0.6388888888888888,\n",
    "    'reg_alpha': 0.19444444444444445,\n",
    "    'reg_lambda': 0.95,\n",
    "    'objective': 'fair',\n",
    "    'boosting': 'gbdt',\n",
    "    'verbosity': -1,\n",
    "    'random_seed': 19,\n",
    "    'metric': 'mae',\n",
    "    'bagging_seed': 11\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_lgb_mae = {\n",
    "    \"num_leaves\": 8,\n",
    "    \"min_data_in_leaf\": 100,\n",
    "    \"max_depth\": 16,\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"bagging_freq\": 3,\n",
    "    \"bagging_fraction\": 0.6777777777777778,\n",
    "    \"reg_alpha\": 0.19444444444444445,\n",
    "    \"reg_lambda\": 0.1,\n",
    "    \"objective\": \"mae\",\n",
    "    \"boosting\": \"gbdt\",\n",
    "    \"verbosity\": -1,\n",
    "    \"random_seed\": 19,\n",
    "    \"metric\": \"mae\",\n",
    "    \"bagging_seed\": 11}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_lgb_regression = {\n",
    "    \"num_leaves\": 80,\n",
    "    \"min_data_in_leaf\": 100,\n",
    "    \"max_depth\": 8,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"bagging_freq\": 5,\n",
    "    \"bagging_fraction\": 0.7166666666666667,\n",
    "    \"reg_alpha\": 0.28888888888888886,\n",
    "    \"reg_lambda\": 0.6666666666666666,\n",
    "    \"objective\": \"regression\",\n",
    "    \"boosting\": \"gbdt\",\n",
    "    \"verbosity\": -1,\n",
    "    \"random_seed\": 19,\n",
    "    \"metric\": \"mae\",\n",
    "    \"bagging_seed\": 11\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_lgb_hyperopt = {\n",
    "    \"bagging_fraction\": 0.84,\n",
    "    \"boosting\": \"gbdt\",\n",
    "    \"feature_fraction\": 0.55,\n",
    "    \"lambda_l1\": 2.0256449252260045,\n",
    "    \"lambda_l2\": 0.5147476786636518,\n",
    "    \"learning_rate\": 0.0662641996692177,\n",
    "    \"max_bin\": 131,\n",
    "    \"max_depth\": 24,\n",
    "    \"metric\": \"MAE\",\n",
    "    \"min_data_in_bin\": 48,\n",
    "    \"min_data_in_leaf\": 234,\n",
    "    \"min_gain_to_split\": 1.81,\n",
    "    \"num_leaves\": 981,\n",
    "    \"objective\": \"fair\",\n",
    "    \"subsample\": 0.9630227452314452\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_lgb = best_params_lgb_huber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 started at Fri May 10 17:26:11 2019\n",
      "Training until validation scores don't improve for 2000 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[5148]\ttraining's l1: 1.22941\tvalid_1's l1: 2.01089\n",
      "Fold 1 started at Fri May 10 17:28:00 2019\n",
      "Training until validation scores don't improve for 2000 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[2716]\ttraining's l1: 1.57238\tvalid_1's l1: 2.00756\n",
      "Fold 2 started at Fri May 10 17:29:13 2019\n",
      "Training until validation scores don't improve for 2000 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[2844]\ttraining's l1: 1.52788\tvalid_1's l1: 2.09075\n",
      "Fold 3 started at Fri May 10 17:30:29 2019\n",
      "Training until validation scores don't improve for 2000 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[2804]\ttraining's l1: 1.58911\tvalid_1's l1: 1.85877\n",
      "Fold 4 started at Fri May 10 17:31:43 2019\n",
      "Training until validation scores don't improve for 2000 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[2608]\ttraining's l1: 1.57541\tvalid_1's l1: 2.05721\n",
      "CV mean score: 2.0050, std: 0.0794.\n"
     ]
    }
   ],
   "source": [
    "oof_lgb, prediction_lgb, scores_lgb, feature_importance_lgb = train_model(X=X_train_scaled,\n",
    "                                                                          X_test=X_test_scaled,\n",
    "                                                                          y=y_tr,\n",
    "                                                                          folds=folds_models,\n",
    "                                                                          params=params_lgb,\n",
    "                                                                          model_type='lgb',\n",
    "                                                                          show_scatter=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 started at Fri May 10 17:35:41 2019\n",
      "Training until validation scores don't improve for 2000 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[65]\ttraining's l1: 1.70334\tvalid_1's l1: 2.03212\n",
      "Fold 1 started at Fri May 10 17:35:45 2019\n",
      "Training until validation scores don't improve for 2000 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[44]\ttraining's l1: 1.79251\tvalid_1's l1: 2.03055\n",
      "Fold 2 started at Fri May 10 17:35:49 2019\n",
      "Training until validation scores don't improve for 2000 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[81]\ttraining's l1: 1.63362\tvalid_1's l1: 2.08423\n",
      "Fold 3 started at Fri May 10 17:35:53 2019\n",
      "Training until validation scores don't improve for 2000 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[52]\ttraining's l1: 1.79384\tvalid_1's l1: 1.88475\n",
      "Fold 4 started at Fri May 10 17:35:57 2019\n",
      "Training until validation scores don't improve for 2000 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[58]\ttraining's l1: 1.7284\tvalid_1's l1: 2.04483\n",
      "CV mean score: 2.0153, std: 0.0681.\n"
     ]
    }
   ],
   "source": [
    "oof_lgb, prediction_lgb, scores_lgb, feature_importance_lgb = train_model(X=X_train_scaled,\n",
    "                                                                          X_test=X_test_scaled,\n",
    "                                                                          y=y_tr,\n",
    "                                                                          folds=folds_models,\n",
    "                                                                          params=best_params_lgb_hyperopt,\n",
    "                                                                          model_type='lgb',\n",
    "                                                                          show_scatter=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = feature_importance_lgb[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n",
    "    by=\"importance\", ascending=False)[:50].index\n",
    "best_features = feature_importance_lgb.loc[feature_importance_lgb.feature.isin(cols)]\n",
    "plt.figure(figsize=(16, 12));\n",
    "sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\n",
    "plt.title('LGB Features (avg over folds)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost (Gradient Boosting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "params_xgb = {\n",
    "    'eta': 0.01, #Andrew uses 0.03\n",
    "    'max_depth': 6, #Andrew uses 10\n",
    "    'subsample': 0.5, #Andrew uses 0.9\n",
    "    'objective': 'reg:linear',\n",
    "    'eval_metric': 'mae',\n",
    "    'silent': True,\n",
    "    'nthread': 32\n",
    "} # CV mean score: 2.0801, std: 0.0711.\n",
    "oof_xgb, prediction_xgb, scores_xgb = train_model(X=X_train_scaled,\n",
    "                                                  X_test=X_test_scaled,\n",
    "                                                  y=y_tr,\n",
    "                                                  folds=folds_models,\n",
    "                                                  params=params_xgb,\n",
    "                                                  model_type='xgb',\n",
    "                                                  show_scatter=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM\n",
    "Similar to NuSVC, for regression, uses a parameter nu to control the number of support vectors. However, unlike NuSVC, where nu replaces C, here nu replaces the parameter epsilon of epsilon-SVR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#model = NuSVR(gamma='scale', nu=0.9, C=10.0, tol=0.01) #original values\n",
    "model = NuSVR(gamma='scale', nu=0.63, C=0.4469387755102041, tol=0.01)\n",
    "oof_svr, prediction_svr, scores_svr = train_model(X=X_train_scaled,\n",
    "                                      X_test=X_test_scaled,\n",
    "                                      y=y_tr,\n",
    "                                      folds=folds_models,\n",
    "                                      params=None,\n",
    "                                      model_type='sklearn',\n",
    "                                      model=model,\n",
    "                                      show_scatter=False)\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_cat = {\n",
    "    'loss_function':'MAE'\n",
    "}\n",
    "oof_cat, prediction_cat, scores_cat = train_model(X=X_train_scaled,\n",
    "                                                  X_test=X_test_scaled,\n",
    "                                                  y=y_tr,\n",
    "                                                  folds=folds_models,\n",
    "                                                  params=params_cat,\n",
    "                                                  model_type='cat',\n",
    "                                                  show_scatter=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel Ridge\n",
    "This model combines regularized linear regression with a given kernel (radial basis in this case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = KernelRidge(kernel='rbf', alpha=0.1, gamma=0.01) #Original parameters\n",
    "model = KernelRidge(kernel='rbf', alpha=2.4497346938775513, gamma=0.0018461224489795917)\n",
    "oof_r, prediction_r, scores_r = train_model(X=X_train_scaled,\n",
    "                                            X_test=X_test_scaled,\n",
    "                                            y=y_tr,\n",
    "                                            folds=folds_models,\n",
    "                                            params=None,\n",
    "                                            model_type='sklearn',\n",
    "                                            model=model,\n",
    "                                            show_scatter=False)\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Genetic Program Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_gpi = {\n",
    "    'activation':'atan'\n",
    "}\n",
    "oof_gpi, prediction_gpi, scores_gpi = train_model(X=X_train_scaled,\n",
    "                                                  X_test=X_test_scaled,\n",
    "                                                  y=y_tr,\n",
    "                                                  folds=folds_models,\n",
    "                                                  model_type='gpi',\n",
    "                                                  params=params_gpi,\n",
    "                                                  show_scatter=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_gpi_new = {\n",
    "    'activation':'atan'\n",
    "}\n",
    "oof_gpi_new, prediction_gpi_new, scores_gpi_new = train_model(X_train_scaled,\n",
    "                                                              X_test_scaled,\n",
    "                                                              y_tr,\n",
    "                                                              params=params_gpi_new,\n",
    "                                                              folds=folds_models,\n",
    "                                                              model_type='gpi_new')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_gpii_new = {\n",
    "    'activation':'atan'\n",
    "}\n",
    "oof_gpii_new, prediction_gpii_new, scores_gpii_new = train_model(X_train_scaled,\n",
    "                                                                 X_test_scaled,\n",
    "                                                                 y_tr,\n",
    "                                                                 params=params_gpii_new,\n",
    "                                                                 folds=folds_models,\n",
    "                                                                 model_type='gpii_new')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_gpiii_new = {\n",
    "    'activation':'atan'\n",
    "}\n",
    "oof_gpiii_new, prediction_gpiii_new, scores_gpiii_new = train_model(X_train_scaled,\n",
    "                                                                    X_test_scaled,\n",
    "                                                                    y_tr,\n",
    "                                                                    params=params_gpiii_new,\n",
    "                                                                    folds=folds_models,\n",
    "                                                                    model_type='gpiii_new')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "This regressor fits many decision trees with different subsets of the original data and average the predictions between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_rf = {\n",
    "    'max_depth': 12, #8,\n",
    "    'max_features': 'log2', #'auto',\n",
    "    'min_samples_leaf': 2,\n",
    "    'min_samples_split': 4 #6\n",
    "} #CV mean score: 2.0448, std: 0.0793.\n",
    "model = RandomForestRegressor(criterion='mae', n_estimators=200, n_jobs=-1, **params_rf)\n",
    "oof_rf, prediction_rf, scores_rf = train_model(X=X_train_scaled,\n",
    "                                               X_test=X_test_scaled,\n",
    "                                               y=y_tr,\n",
    "                                               folds=folds_models,\n",
    "                                               params=params_rf,\n",
    "                                               model_type='sklearn',\n",
    "                                               model=model,\n",
    "                                               show_scatter=False)\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extremely Randomized Trees\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_ex = {\n",
    "    'max_depth': 12,\n",
    "    'max_features': 'sqrt',\n",
    "    'min_samples_leaf': 2,\n",
    "    'min_samples_split': 4\n",
    "}\n",
    "model = ExtraTreesRegressor(criterion='mae', n_estimators=200, n_jobs=-1, **params_ex)\n",
    "oof_ex, prediction_ex, scores_ex = train_model(X=X_train_scaled,\n",
    "                                               X_test=X_test_scaled,\n",
    "                                               y=y_tr,\n",
    "                                               folds=folds_models,\n",
    "                                               params=params_ex,\n",
    "                                               model_type='sklearn',\n",
    "                                               model=model,\n",
    "                                               show_scatter=False)\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ada Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_ada = {\n",
    "    'learning_rate': 0.001\n",
    "}\n",
    "base = KernelRidge(kernel='rbf', alpha=2.4497346938775513, gamma=0.0018461224489795917)\n",
    "model = AdaBoostRegressor(base_estimator=base, n_estimators=100, **params_ada)\n",
    "oof_ada, prediction_ada, scores_ada = train_model(X=X_train_scaled,\n",
    "                                                  X_test=X_test_scaled,\n",
    "                                                  y=y_tr,\n",
    "                                                  folds=folds_models,\n",
    "                                                  params=params_ada,\n",
    "                                                  model_type='sklearn',\n",
    "                                                  model=model,\n",
    "                                                  show_scatter=False)\n",
    "del base\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = neighbors.KNeighborsRegressor()\n",
    "#parameter_grid = {'n_neighbors': [10, 20, 25, 30, 35, 50]}\n",
    "#grid_search = GridSearchCV(model, param_grid=parameter_grid,\n",
    "#                           cv=folds_models,\n",
    "#                           scoring='neg_mean_absolute_error',\n",
    "#                           n_jobs=-1)\n",
    "#grid_search.fit(X_train_scaled, y_tr)\n",
    "#print('Best score: {}'.format(grid_search.best_score_))\n",
    "#print('Best parameters: {}'.format(grid_search.best_params_))\n",
    "#model = neighbors.KNeighborsRegressor(**grid_search.best_params_)\n",
    "params_knn = {\n",
    "    'n_neighbors': 10\n",
    "}\n",
    "model = neighbors.KNeighborsRegressor(n_neighbors=30)\n",
    "oof_knn, prediction_knn, scores_knn = train_model(X=X_train_scaled,\n",
    "                                                  X_test=X_test_scaled,\n",
    "                                                  y=y_tr,\n",
    "                                                  folds=folds_models,\n",
    "                                                  params=params_knn,\n",
    "                                                  model_type='sklearn',\n",
    "                                                  model=model,\n",
    "                                                  show_scatter=False)\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 started at Fri May 10 17:41:17 2019\n"
     ]
    }
   ],
   "source": [
    "params_gbr = {\n",
    "    'max_depth': 12,\n",
    "    'max_features': 'sqrt',\n",
    "    'min_samples_leaf': 2,\n",
    "    'min_samples_split': 4\n",
    "}\n",
    "model = GradientBoostingRegressor(criterion='mae', n_estimators=200, **params_gbr)\n",
    "oof_gbr, prediction_gbr, scores_gbr = train_model(X=X_train_scaled,\n",
    "                                                  X_test=X_test_scaled,\n",
    "                                                  y=y_tr,\n",
    "                                                  folds=folds_models,\n",
    "                                                  params=params_gbr,\n",
    "                                                  model_type='sklearn',\n",
    "                                                  model=model,\n",
    "                                                  show_scatter=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8));\n",
    "scores_df = pd.DataFrame()\n",
    "try:\n",
    "    scores_df['LGBMRegressor'] = scores_lgb\n",
    "except NameError:\n",
    "    print('LGBMRegressor not computed')\n",
    "try:\n",
    "    scores_df['XGBRegressor'] = scores_xgb\n",
    "except NameError:\n",
    "    print('XGBRegressor not computed')\n",
    "try:\n",
    "    scores_df['NuSVR'] = scores_svr\n",
    "except NameError:\n",
    "    print('NuSVR not computed')\n",
    "try:\n",
    "    scores_df['CatBoostRegressor'] = scores_cat\n",
    "except NameError:\n",
    "    print('CatBoostRegressor not computed')\n",
    "try:\n",
    "    scores_df['Ridge'] = scores_r\n",
    "except NameError:\n",
    "    print('Ridge not computed')\n",
    "try:\n",
    "    scores_df['GPI'] = scores_gpi\n",
    "except NameError:\n",
    "    print('GPI not computed')\n",
    "try:\n",
    "    scores_df['RandomForestRegressor'] = scores_rf\n",
    "except NameError:\n",
    "    print('RandomForestRegressor not computed')\n",
    "try:\n",
    "    scores_df['ExtraTreesRegressor'] = scores_ex\n",
    "except NameError:\n",
    "    print('ExtraTreesRegressor not computed')\n",
    "try:\n",
    "    scores_df['AdaBoostRegressor'] = scores_ada\n",
    "except NameError:\n",
    "    print('AdaBoostRegressor not computed')\n",
    "try:\n",
    "    scores_df['KNN'] = scores_knn\n",
    "except NameError:\n",
    "    print('KNN not computed')\n",
    "try:\n",
    "    scores_df['GradientBoostingRegressor'] = scores_knn\n",
    "except NameError:\n",
    "    print('GradientBoostingRegressor not computed')\n",
    "\n",
    "ax = sns.boxplot(data=scores_df.reindex(scores_df.mean().sort_values().index, axis=1));\n",
    "#ax.set(yscale=\"log\")\n",
    "\n",
    "plt.xticks(rotation=45);\n",
    "plt.xlabel('Method');\n",
    "plt.ylabel('Score');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ELI5 and permutation importance\n",
    "ELI5 is a package with provides explanations for ML models. It can do this not only for linear models, but also for tree based like Random Forest or lightgbm.\n",
    "\n",
    "**Important notice**: running eli5 on all features takes a lot of time, so I run the cell below in `version 14` and printed the top-50 features. In the following versions I'll use these 50 columns and use eli5 to find top-40 of them so that it takes less time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_scaled, y_tr, test_size=0.2)\n",
    "model = lgb.LGBMRegressor(**params_lgb, n_estimators = 50000, n_jobs = -1, verbose=-1)\n",
    "model.fit(X_train,\n",
    "          y_train, \n",
    "          eval_set=[(X_train, y_train), (X_valid, y_valid)],\n",
    "          eval_metric='mae',\n",
    "          verbose=10000,\n",
    "          early_stopping_rounds=200)\n",
    "\n",
    "perm = eli5.sklearn.PermutationImportance(model, random_state=1).fit(X_train, y_train)\n",
    "eli5.show_weights(perm, top=50, feature_names=X_train_scaled.columns.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_num_features = 40\n",
    "top_features = [i for i in eli5.formatters.as_dataframe.explain_weights_df(model).feature if 'BIAS' not in i][:top_num_features]\n",
    "oof_lgb_top, prediction_lgb_top, scores_lgb_top, feature_importance_lgb_top = train_model(\n",
    "    X=X_train_scaled[top_features],\n",
    "    X_test=X_test_scaled[top_features],\n",
    "    y=y_tr,\n",
    "    folds=folds_models,\n",
    "    params=params_lgb,\n",
    "    model_type='lgb',\n",
    "    show_scatter=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping highly correlated features\n",
    "\n",
    "Due to the huge number of features there are certainly some highly correlated features, let's try droping them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled_uncorr = X_train_scaled.copy()\n",
    "X_test_scaled_uncorr = X_test_scaled.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://chrisalbon.com/machine_learning/feature_selection/drop_highly_correlated_features/\n",
    "corr_matrix = X_train_scaled_uncorr.corr().abs()\n",
    "# Select upper triangle of correlation matrix\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "# Find index of feature columns with correlation greater than 0.99\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.99)]\n",
    "X_train_scaled_uncorr = X_train_scaled_uncorr.drop(to_drop, axis=1)\n",
    "X_test_scaled_uncorr = X_test_scaled_uncorr.drop(to_drop, axis=1)\n",
    "\n",
    "print(X_train_scaled.shape)\n",
    "print(X_train_scaled_uncorr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_lgb_uncorr, prediction_lgb_uncorr, scores_lgb_uncorr, feature_importance_lgb_uncorr = train_model(\n",
    "    X=X_train_scaled_uncorr,\n",
    "    X_test=X_test_scaled_uncorr,\n",
    "    y=y_tr,\n",
    "    folds=folds_models,\n",
    "    params=params_lgb,\n",
    "    model_type='lgb',\n",
    "    show_scatter=False)\n",
    "cols = feature_importance_lgb_uncorr[['feature', 'importance']].groupby('feature').mean().sort_values(\n",
    "    by='importance', ascending=False).index\n",
    "best_features = feature_importance_lgb_uncorr.loc[feature_importance_lgb_uncorr.feature.isin(cols)]\n",
    "plt.figure(figsize=(16, 20));\n",
    "sns.barplot(x='importance', y='feature', data=best_features.sort_values(by='importance', ascending=False));\n",
    "plt.title('LGB Features (avg over folds)');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_scaled_uncorr, y_tr, test_size=0.2)\n",
    "model = lgb.LGBMRegressor(**params_lgb, n_estimators = 50000, n_jobs = -1, verbose=-1)\n",
    "model.fit(X_train,\n",
    "          y_train, \n",
    "          eval_set=[(X_train, y_train), (X_valid, y_valid)],\n",
    "          eval_metric='mae',\n",
    "          verbose=10000,\n",
    "          early_stopping_rounds=200)\n",
    "\n",
    "perm = eli5.sklearn.PermutationImportance(model, random_state=1).fit(X_train, y_train)\n",
    "#eli5.show_weights(perm, top=50, feature_names=X_train_scaled_uncorr.columns.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_lgb = {'num_leaves': 36,\n",
    "              'min_data_in_leaf': 20,\n",
    "              'max_depth': 4,\n",
    "              'learning_rate': 0.005,\n",
    "              'bagging_freq': 5,\n",
    "              'bagging_fraction': 0.6,\n",
    "              'reg_alpha': 0.3833333333333333,\n",
    "              'reg_lambda': 0.5722222222222222,\n",
    "              'objective': 'huber',\n",
    "              'boosting': 'gbdt',\n",
    "              'verbosity': -1, 'random_seed': 19,\n",
    "              'metric': 'mae',\n",
    "              'bagging_seed': 11}\n",
    "top_num_features = 40\n",
    "top_features = [i for i in eli5.formatters.as_dataframe.explain_weights_df(model).feature if 'BIAS' not in i][:top_num_features]\n",
    "oof_lgb_uncorr_top, prediction_lgb_uncorr_top, scores_lgb_uncorr_top, feature_importance_lgb_uncorr_top = train_model(\n",
    "    X=X_train_scaled_uncorr[top_features],\n",
    "    X_test=X_test_scaled_uncorr[top_features],\n",
    "    y=y_tr,\n",
    "    folds=folds_models,\n",
    "    params=params_lgb,\n",
    "    model_type='lgb',\n",
    "    show_scatter=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = pd.DataFrame(dtype=np.float64)\n",
    "submission = pd.read_csv('../input/sample_submission.csv', index_col='seg_id')\n",
    "submission['time_to_failure'] = prediction_lgb_uncorr_top\n",
    "submission.to_csv('../output/submission_lgb_uncorr_top.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fixed_params = {\n",
    "    'objective': 'huber',\n",
    "    'boosting': 'gbdt',\n",
    "    'verbosity': -1,\n",
    "    'random_seed': 19,\n",
    "    'n_estimators': 50000,\n",
    "    'metric': 'mae',\n",
    "    'bagging_seed': 11\n",
    "}\n",
    "\n",
    "param_grid = {\n",
    "    'num_leaves': list(range(8, 92, 4)),\n",
    "    'min_data_in_leaf': [10, 20, 40, 60, 100],\n",
    "    'max_depth': [3, 4, 5, 6, 8, 12, 16, -1],\n",
    "    'learning_rate': [0.1, 0.05, 0.01, 0.005],\n",
    "    'bagging_freq': [3, 4, 5, 6, 7],\n",
    "    'bagging_fraction': np.linspace(0.6, 0.95, 10),\n",
    "    'reg_alpha': np.linspace(0.1, 0.95, 10),\n",
    "    'reg_lambda': np.linspace(0.1, 0.95, 10)\n",
    "}\n",
    "\n",
    "grid_size = 1\n",
    "for param in param_grid:\n",
    "    grid_size *= len(param_grid[param])\n",
    "print(f'The search grid has {grid_size} elements')\n",
    "\n",
    "best_score = 9999\n",
    "dataset = lgb.Dataset(X_train_scaled_uncorr[top_features], label=y_tr)  # no need to scale features\n",
    "\n",
    "scores_val_mean = []\n",
    "scores_val_std = []\n",
    "for i in tqdm_notebook(range(2000)):\n",
    "    params = {k: random.choice(v) for k, v in param_grid.items()}\n",
    "    params.update(fixed_params)\n",
    "    result = lgb.cv(params,\n",
    "                    dataset,\n",
    "                    nfold=n_fold,\n",
    "                    early_stopping_rounds=200,\n",
    "                    stratified=False)\n",
    "\n",
    "    print(\n",
    "        f\"Iteration {i} finished with mae={result['l1-mean'][-1]:.4f} and std={result['l1-stdv'][-1]:.4f}\")\n",
    "    scores_val_mean.append(result['l1-mean'][-1])\n",
    "    scores_val_std.append(result['l1-stdv'][-1])\n",
    "\n",
    "    if result['l1-mean'][-1] < best_score:\n",
    "        best_score = result['l1-mean'][-1]\n",
    "        best_score_std = result['l1-stdv'][-1]\n",
    "        best_params = params\n",
    "\n",
    "\n",
    "plt.figure(figsize=(16, 6))\n",
    "plt.scatter(scores_val_mean, scores_val_std, color='blue')\n",
    "plt.scatter(best_score, best_score_std, color='gold')\n",
    "plt.xlabel('scores_val_mean')\n",
    "plt.ylabel('scores_val_std')\n",
    "plt.title('Validation score mean/std scatter plot')\n",
    "plt.grid()\n",
    "plt.legend(['All parameters', 'Best'])\n",
    "\n",
    "print(f\"best_score={best_score}\")\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8));\n",
    "scores_df = pd.DataFrame()\n",
    "\n",
    "try:\n",
    "    scores_df['LGB'] = scores_lgb\n",
    "except NameError:\n",
    "    print('LGB not computed')\n",
    "try:\n",
    "    scores_df['LGB_top'] = scores_lgb_top\n",
    "except NameError:\n",
    "    print('LGB_top not computed')\n",
    "try:\n",
    "    scores_df['LGB_uncorr'] = scores_lgb_uncorr\n",
    "except NameError:\n",
    "    print('LGB_uncorr not computed')\n",
    "try:\n",
    "    scores_df['LGB_uncorr_top'] = scores_lgb_uncorr_top\n",
    "except NameError:\n",
    "    print('LGB_uncorr_top not computed')\n",
    "try:\n",
    "    scores_df['XGB'] = scores_xgb\n",
    "except NameError:\n",
    "    print('XGB not computed')\n",
    "try:\n",
    "    scores_df['SVR'] = scores_svr\n",
    "except NameError:\n",
    "    print('SVR not computed')\n",
    "try:\n",
    "    scores_df['CatBoost'] = scores_cat\n",
    "except NameError:\n",
    "    print('CatBoost not computed')\n",
    "try:\n",
    "    scores_df['Ridge'] = scores_r\n",
    "except NameError:\n",
    "    print('Ridge not computed')\n",
    "try:\n",
    "    scores_df['GPI'] = scores_gpi\n",
    "except NameError:\n",
    "    print('GPI not computed')\n",
    "try:\n",
    "    scores_df['GPI_new'] = scores_gpi_new\n",
    "except NameError:\n",
    "    print('GPI_new not computed')\n",
    "try:\n",
    "    scores_df['GPII_new'] = scores_gpii_new\n",
    "except NameError:\n",
    "    print('GPII_new not computed')    \n",
    "try:\n",
    "    scores_df['GPIII_new'] = scores_gpiii_new\n",
    "except NameError:\n",
    "    print('GPIII_new not computed')     \n",
    "try:\n",
    "    scores_df['RandomForest'] = scores_rf\n",
    "except NameError:\n",
    "    print('RandomForest not computed')     \n",
    "try:\n",
    "    scores_df['ExtraTrees'] = scores_ex\n",
    "except NameError:\n",
    "    print('ExtraTrees not computed') \n",
    "try:\n",
    "    scores_df['KNN'] = scores_knn\n",
    "except NameError:\n",
    "    print('KNN not computed') \n",
    "try:\n",
    "    scores_df['GBR'] = scores_knn\n",
    "except NameError:\n",
    "    print('GBR not computed')\n",
    "    \n",
    "ax = sns.boxplot(data=scores_df.reindex(scores_df.mean().sort_values().index, axis=1));\n",
    "#ax.set(yscale=\"log\")\n",
    "\n",
    "plt.xticks(rotation=45);\n",
    "plt.xlabel('Method');\n",
    "plt.ylabel('Score');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_blend_gp = (prediction_gpi +\n",
    "                    prediction_gpi_new + \n",
    "                    prediction_gpii_new + \n",
    "                    prediction_gpiii_new) / 4\n",
    "oof_blend_gp = (oof_gpi +\n",
    "             oof_gpi_new + \n",
    "             oof_gpii_new + \n",
    "             oof_gpiii_new) / 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = pd.DataFrame(dtype=np.float64)\n",
    "submission = pd.read_csv('../input/sample_submission.csv', index_col='seg_id')\n",
    "submission['time_to_failure'] = prediction_blend_gp\n",
    "submission.to_csv('../output/submission_blend_gp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axis = plt.subplots(1, 2, figsize=(12,5))\n",
    "ax1, ax2 = axis\n",
    "ax1.set_xlabel('actual')\n",
    "ax1.set_ylabel('predicted')\n",
    "ax2.set_xlabel('train index')\n",
    "ax2.set_ylabel('time to failure')\n",
    "ax1.scatter(y_tr, oof_lgb_uncorr_top, color='brown')\n",
    "ax1.plot([(0, 0), (20, 20)], [(0, 0), (20, 20)], color='blue')\n",
    "ax2.plot(y_tr, color='blue', label='y_train')\n",
    "ax2.plot(oof_lgb_uncorr_top, color='orange')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axis = plt.subplots(1, 2, figsize=(12,5))\n",
    "ax1, ax2 = axis\n",
    "ax1.set_xlabel('actual')\n",
    "ax1.set_ylabel('predicted')\n",
    "ax2.set_xlabel('train index')\n",
    "ax2.set_ylabel('time to failure')\n",
    "ax1.scatter(y_tr, oof_blend, color='brown')\n",
    "ax1.plot([(0, 0), (20, 20)], [(0, 0), (20, 20)], color='blue')\n",
    "ax2.plot(y_tr, color='blue', label='y_train')\n",
    "ax2.plot(oof_blend, color='orange')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
