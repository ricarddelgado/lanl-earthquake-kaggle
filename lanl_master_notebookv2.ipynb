{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Master jupyter notebook for LANL - SlimBros Team"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correctly predicting earthquakes is very important for preventing deaths and damage to infrastructure. In this competition we try to predict time left to the next laboratory earthquake based on seismic signal data. Training data represents one huge signal, but in test data we have many separate chunks, for each of which we need to predict time to failure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries\n",
    "Let's import everything we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import eli5\n",
    "import csv\n",
    "import time\n",
    "import random\n",
    "import datetime\n",
    "import warnings\n",
    "import feather\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn import svm, neighbors, linear_model\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, AdaBoostRegressor\n",
    "from sklearn.svm import NuSVR, SVR\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.linear_model import Ridge, RidgeCV\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold, GridSearchCV, cross_val_score, ParameterGrid, train_test_split\n",
    "from utils import generate_segment_start_ids, compare_methods\n",
    "from features import gpi, create_all_features_extended\n",
    "from features import gpi_new, gpii_new, gpiii_new\n",
    "\n",
    "#Configure the environment\n",
    "%matplotlib inline\n",
    "pd.options.display.precision = 15\n",
    "warnings.filterwarnings('ignore')\n",
    "random.seed(1013)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load/compute the necessary features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_features = True \n",
    "# The computed features are saved in an hdf file along with the time_to_failure to \n",
    "# save the time spend reading the training data and the feature computation\n",
    "#train_data_format = 'csv'\n",
    "train_data_format = 'feather'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_data(file_format):\n",
    "    \"\"\"Load the training dataset.\"\"\"\n",
    "    print(f\"Loading data from {file_format} file:\", end=\"\")\n",
    "    if file_format.lower() == 'feather':\n",
    "        train_df = feather.read_dataframe('../input/train.feather')\n",
    "    else:\n",
    "        train_df = pd.read_csv('../input/train.csv', dtype={'acoustic_data': np.int16,\n",
    "                                                            'time_to_failure': np.float32})\n",
    "        feather.write_dataframe(train_df, '../input/train.feather')\n",
    "    print(\"Done\")\n",
    "    return train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = load_train_data(train_data_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acoustic_data_small = train['acoustic_data'].values[::50]\n",
    "train_time_to_failure_small = train['time_to_failure'].values[::50]\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(14, 8))\n",
    "plt.title('Trends of acoustic_data and time_to_failure. 2% of the data (sampled)')\n",
    "plt.plot(train_acoustic_data_small, color='b')\n",
    "ax1.set_ylabel('acoustic_data', color='b')\n",
    "ax2 = ax1.twinx()\n",
    "plt.plot(train_time_to_failure_small, color='g')\n",
    "ax2.set_ylabel('time_to_failure', color='g')\n",
    "\n",
    "del train_acoustic_data_small\n",
    "del train_time_to_failure_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_to_failure_delta = np.diff(train['time_to_failure'])\n",
    "init_times = np.where(time_to_failure_delta > 5)[0].tolist()\n",
    "print(f\"There are {len(init_times)} quakes on the training set.\")\n",
    "init_times = [0] + init_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'start_idx': init_times, 'end_idx': init_times[1:] + [len(time_to_failure_delta)]}\n",
    "quakes = pd.DataFrame(data=d)\n",
    "quakes.insert(2, 'valid', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's discard any quake that looks weird"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#quakes['valid'][1] = False\n",
    "#quakes['valid'][2] = False\n",
    "#quakes['valid'][7] = False\n",
    "#quakes['valid'][14] = False\n",
    "#quakes['valid'][15] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature generation\n",
    "- Usual aggregations: mean, std, min and max;\n",
    "- Average difference between the consequitive values in absolute and percent values;\n",
    "- Absolute min and max vallues;\n",
    "- Aforementioned aggregations for first and last 10000 and 50000 values - I think these data should be useful;\n",
    "- Max value to min value and their differencem also count of values bigger that 500 (arbitrary threshold);\n",
    "- Quantile features from this kernel: https://www.kaggle.com/andrekos/basic-feature-benchmark-with-quantiles\n",
    "- Trend features from this kernel: https://www.kaggle.com/jsaguiar/baseline-with-abs-and-trend-features\n",
    "- Rolling features from this kernel: https://www.kaggle.com/wimwim/rolling-quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_files_present = (os.path.isfile('../tmp_results/X_tr.hdf') and \n",
    "                       os.path.isfile('../tmp_results/X_test.hdf') and \n",
    "                       os.path.isfile('../tmp_results/y_tr.hdf') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (not compute_features) and saved_files_present:\n",
    "    print(f\"Reading hdf files:\", end=\"\")\n",
    "    X_tr = pd.read_hdf('../tmp_results/X_tr.hdf', 'data')\n",
    "    X_test = pd.read_hdf('../tmp_results/X_test.hdf', 'data')\n",
    "    y_tr = pd.read_hdf('../tmp_results/y_tr.hdf', 'data')  \n",
    "    print(\"Done\")\n",
    "else:\n",
    "    fs = 4000000 #Sampling frequency of the raw signal\n",
    "\n",
    "    #Compute features for the training data\n",
    "    segment_size = 150000\n",
    "    segment_start_ids = generate_segment_start_ids('uniform_no_jump', segment_size, train)\n",
    "    X_tr = pd.DataFrame(index=range(len(segment_start_ids)), dtype=np.float64)\n",
    "    y_tr = pd.DataFrame(index=range(len(segment_start_ids)), dtype=np.float64, columns=['time_to_failure'])\n",
    "    for idx in tqdm_notebook(range(len(segment_start_ids))):        \n",
    "        seg_id = segment_start_ids[idx]\n",
    "        seg = train.iloc[seg_id:seg_id + segment_size]\n",
    "        create_all_features_extended(idx, seg, X_tr, fs)\n",
    "        y_tr.loc[idx, 'time_to_failure'] = seg['time_to_failure'].values[-1]\n",
    "    # Sanity check\n",
    "    means_dict = {}\n",
    "    for col in X_tr.columns:\n",
    "        if X_tr[col].isnull().any():\n",
    "            print(col)\n",
    "            mean_value = X_tr.loc[X_tr[col] != -np.inf, col].mean()\n",
    "            X_tr.loc[X_tr[col] == -np.inf, col] = mean_value\n",
    "            X_tr[col] = X_tr[col].fillna(mean_value)\n",
    "            means_dict[col] = mean_value\n",
    "\n",
    "    #Compute features for the test data\n",
    "    submission = pd.read_csv('../input/sample_submission.csv', index_col='seg_id')\n",
    "    X_test = pd.DataFrame(columns=X_tr.columns, dtype=np.float64, index=submission.index)\n",
    "    for i, seg_id in enumerate(tqdm_notebook(X_test.index)):\n",
    "        seg = pd.read_csv('../input/test/' + seg_id + '.csv')\n",
    "        create_all_features_extended(seg_id, seg, X_test, fs)\n",
    "\n",
    "    # Sanity check\n",
    "    for col in X_test.columns:\n",
    "        if X_test[col].isnull().any():\n",
    "            X_test.loc[X_test[col] == -np.inf, col] = means_dict[col]\n",
    "            X_test[col] = X_test[col].fillna(means_dict[col])\n",
    "            \n",
    "    X_tr.to_hdf('../tmp_results/X_tr.hdf', 'data')\n",
    "    X_test.to_hdf('../tmp_results/X_test.hdf', 'data')\n",
    "    y_tr.to_hdf('../tmp_results/y_tr.hdf', 'data')\n",
    "    \n",
    "    del segment_start_ids\n",
    "    del means_dict\n",
    "    del submission\n",
    "    \n",
    "    print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldata = pd.concat([X_tr, X_test])\n",
    "scaler = StandardScaler()\n",
    "alldata = pd.DataFrame(scaler.fit_transform(alldata), columns=alldata.columns)\n",
    "X_train_scaled = alldata[:X_tr.shape[0]]\n",
    "X_test_scaled = alldata[X_tr.shape[0]:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X, X_test, y, folds, params=None, model_type='lgb',\n",
    "                model=None, show_scatter=False, force_positive=False):\n",
    "\n",
    "    oof = np.zeros(len(X))\n",
    "    prediction = np.zeros(len(X_test))\n",
    "    scores = []\n",
    "    n_fold = folds.get_n_splits()\n",
    "    feature_importance = pd.DataFrame()\n",
    "    for fold_n, (train_index, valid_index) in enumerate(folds.split(X)):\n",
    "        print('Fold', fold_n, 'started at', time.ctime())\n",
    "        X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n",
    "        y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n",
    "\n",
    "        if model_type == 'nn':\n",
    "            dropout = params['dropout']\n",
    "            num_layers = params['num_layers']\n",
    "            num_neurons = params['num_neurons']\n",
    "            activation_function = params['activation_function']\n",
    "            model = tf.keras.Sequential()\n",
    "            model.add(tf.keras.layers.Dense(1024, input_dim=216, activation=activation_function))\n",
    "            model.add(tf.keras.layers.BatchNormalization())\n",
    "            model.add(tf.keras.layers.Dropout(dropout))\n",
    "            for l in range(num_layers):\n",
    "                model.add(tf.keras.layers.Dense(num_neurons, activation=activation_function))\n",
    "                model.add(tf.keras.layers.BatchNormalization())\n",
    "                model.add(tf.keras.layers.Dropout(dropout))\n",
    "            model.add(tf.keras.layers.Dense(1))\n",
    "            model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
    "            EPOCHS = 1000\n",
    "            early_stop = tf.keras.callbacks.EarlyStopping(monitor='mean_absolute_error', patience=100)\n",
    "\n",
    "            history = model.fit(\n",
    "                X_train,\n",
    "                y_train,\n",
    "                epochs=EPOCHS,\n",
    "                validation_data = (X_valid, y_valid), \n",
    "                verbose=0,\n",
    "                callbacks=[early_stop, PrintDot()])\n",
    "            hist = pd.DataFrame(history.history)\n",
    "            val_score = hist['val_mean_absolute_error'].iloc[-1]\n",
    "            print(f'val_score={val_score}')\n",
    "            plot_history(history)\n",
    "        \n",
    "            y_pred_valid = model.predict(X_valid).reshape(-1,)\n",
    "            if force_positive:\n",
    "                y_pred_valid = y_pred_valid.clip(min=0)\n",
    "          \n",
    "            y_pred = model.predict(X_test).reshape(-1,)\n",
    "\n",
    "            score = mean_absolute_error(y_valid, y_pred_valid)\n",
    "            print(f'Fold {fold_n}. MAE: {score:.4f}.')\n",
    "        \n",
    "        if model_type == 'lgb':\n",
    "            model = lgb.LGBMRegressor(**params, n_estimators = 50000, n_jobs = -1)\n",
    "            model.fit(X_train, y_train, \n",
    "                      eval_set=[(X_train, y_train), (X_valid, y_valid)],\n",
    "                      eval_metric='mae',\n",
    "                      verbose=10000,\n",
    "                      early_stopping_rounds=2000)\n",
    "            \n",
    "            y_pred_valid = model.predict(X_valid)\n",
    "            if force_positive:\n",
    "                y_pred_valid = y_pred_valid.clip(min=0)\n",
    "          \n",
    "            y_pred = model.predict(X_test, num_iteration=model.best_iteration_)\n",
    "            \n",
    "        if model_type == 'xgb':\n",
    "            train_data = xgb.DMatrix(data=X_train, label=y_train, feature_names=X.columns)\n",
    "            valid_data = xgb.DMatrix(data=X_valid, label=y_valid, feature_names=X.columns)\n",
    "\n",
    "            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]\n",
    "            model = xgb.train(dtrain=train_data,\n",
    "                              num_boost_round=20000,\n",
    "                              evals=watchlist,\n",
    "                              early_stopping_rounds=200,\n",
    "                              verbose_eval=500,\n",
    "                              params=params)\n",
    "\n",
    "            y_pred_valid = model.predict(xgb.DMatrix(X_valid, feature_names=X.columns),\n",
    "                                         ntree_limit=model.best_ntree_limit)\n",
    "            if force_positive:\n",
    "                y_pred_valid = y_pred_valid.clip(min=0)\n",
    "          \n",
    "            y_pred = model.predict(xgb.DMatrix(X_test, feature_names=X.columns),\n",
    "                                   ntree_limit=model.best_ntree_limit)\n",
    "        \n",
    "        if model_type == 'sklearn':\n",
    "            model = model\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred_valid = model.predict(X_valid).reshape(-1,)\n",
    "            if force_positive:\n",
    "                y_pred_valid = y_pred_valid.clip(min=0)\n",
    "                \n",
    "            score = mean_absolute_error(y_valid, y_pred_valid)\n",
    "            y_pred = model.predict(X_test).reshape(-1,)\n",
    "            print(f'Fold {fold_n}. MAE: {score:.4f}.')\n",
    "            print('')\n",
    "        \n",
    "        if model_type == 'cat':\n",
    "            model = CatBoostRegressor(iterations=20000,  eval_metric='MAE', task_type='GPU', **params)\n",
    "            model.fit(X_train,\n",
    "                      y_train,\n",
    "                      eval_set=(X_valid, y_valid),\n",
    "                      cat_features=[],\n",
    "                      use_best_model=True,\n",
    "                      verbose=False)\n",
    "\n",
    "            y_pred_valid = model.predict(X_valid)\n",
    "            y_pred = model.predict(X_test)\n",
    "\n",
    "            if force_positive:\n",
    "                y_pred_valid = y_pred_valid.clip(min=0)\n",
    "\n",
    "        if model_type == 'gpi':\n",
    "            y_pred_valid = gpi(X_valid, activation=params['activation']).values\n",
    "            y_pred = gpi(X_test, activation=params['activation']).values\n",
    "            \n",
    "            if force_positive:\n",
    "                y_pred_valid = y_pred_valid.clip(min=0)\n",
    "\n",
    "        oof[valid_index] = y_pred_valid.reshape(-1,)\n",
    "        scores.append(mean_absolute_error(y_valid, y_pred_valid))\n",
    "        \n",
    "        prediction += y_pred    \n",
    "        \n",
    "        if model_type == 'lgb':\n",
    "            # feature importance\n",
    "            fold_importance = pd.DataFrame()\n",
    "            fold_importance['feature'] = X.columns\n",
    "            fold_importance['importance'] = model.feature_importances_\n",
    "            fold_importance['fold'] = fold_n + 1\n",
    "            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n",
    "\n",
    "    prediction /= n_fold\n",
    "    \n",
    "    if force_positive:\n",
    "        prediction = prediction.clip(min=0)    \n",
    "    \n",
    "    if show_scatter:\n",
    "        fig, axis = plt.subplots(1, 2, figsize=(12,5))\n",
    "        ax1, ax2 = axis\n",
    "        ax1.set_xlabel('actual')\n",
    "        ax1.set_ylabel('predicted')\n",
    "        ax2.set_xlabel('train index')\n",
    "        ax2.set_ylabel('time to failure')\n",
    "        \n",
    "        ax1.scatter(y, oof, color='brown')\n",
    "        ax1.plot([(0, 0), (20, 20)], [(0, 0), (20, 20)], color='blue')\n",
    "\n",
    "        ax2.plot(y, color='blue', label='y_train')\n",
    "        ax2.plot(oof, color='orange')\n",
    "    \n",
    "    print(f'CV mean score: {np.mean(scores):.4f}, std: {np.std(scores):.4f}.')\n",
    "    \n",
    "    if model_type == 'lgb':\n",
    "        feature_importance['importance'] /= n_fold\n",
    "        return oof, prediction, scores, feature_importance\n",
    "    else:\n",
    "        return oof, prediction, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fold = 5\n",
    "folds_models = KFold(n_splits=n_fold, shuffle=True, random_state=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a few different models and submit the one with the best validation score. The predicted values in the following plots are using a out-of-fold scheme."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LGBM (Gradient Boosting)\n",
    "Gradient boosting that uses tree based learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To determine the best parameters, we have done a random search for different objective functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Huber\n",
    "![Huber](./hyperparam_search/lgbm/huber.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_lgb_huber = {\n",
    "    \"objective\": \"huber\",\n",
    "    \"boosting\": \"gbdt\",\n",
    "    \"verbosity\": -1,\n",
    "    \"num_leaves\": 12,\n",
    "    \"min_data_in_leaf\": 40,\n",
    "    \"max_depth\": 8,\n",
    "    \"learning_rate\": 0.005,\n",
    "    \"bagging_freq\": 4,\n",
    "    \"bagging_fraction\": 0.6,\n",
    "    \"bagging_seed\": 11,\n",
    "    \"random_seed\": 19,\n",
    "    \"metric\": \"mae\",\n",
    "    \"reg_alpha\": 0.47777777777777775,\n",
    "    \"reg_lambda\": 0.47777777777777775\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Gamma\n",
    "![Gamma](./hyperparam_search/lgbm/gamma.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_lgb_gamma = {\n",
    "    'num_leaves': 36,\n",
    "    'min_data_in_leaf': 10,\n",
    "    'max_depth': 12,\n",
    "    'learning_rate': 0.005,\n",
    "    'bagging_freq': 3,\n",
    "    'bagging_fraction': 0.7166666666666667,\n",
    "    'reg_alpha': 0.28888888888888886,\n",
    "    'reg_lambda': 0.95,\n",
    "    'objective': 'gamma',\n",
    "    'boosting': 'gbdt',\n",
    "    'verbosity': -1,\n",
    "    'random_seed': 19,\n",
    "    'metric': 'mae',\n",
    "    'bagging_seed': 11\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Fair\n",
    "![Fair](./hyperparam_search/lgbm/fair.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_lgb_fair = {\n",
    "    'num_leaves': 8,\n",
    "    'min_data_in_leaf': 60,\n",
    "    'max_depth': 3,\n",
    "    'learning_rate': 0.01,\n",
    "    'bagging_freq': 5,\n",
    "    'bagging_fraction': 0.6388888888888888,\n",
    "    'reg_alpha': 0.19444444444444445,\n",
    "    'reg_lambda': 0.95,\n",
    "    'objective': 'fair',\n",
    "    'boosting': 'gbdt',\n",
    "    'verbosity': -1,\n",
    "    'random_seed': 19,\n",
    "    'metric': 'mae',\n",
    "    'bagging_seed': 11\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- MAE\n",
    "![MAE](./hyperparam_search/lgbm/mae.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_lgb_mae = {\n",
    "    \"num_leaves\": 8,\n",
    "    \"min_data_in_leaf\": 100,\n",
    "    \"max_depth\": 16,\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"bagging_freq\": 3,\n",
    "    \"bagging_fraction\": 0.6777777777777778,\n",
    "    \"reg_alpha\": 0.19444444444444445,\n",
    "    \"reg_lambda\": 0.1,\n",
    "    \"objective\": \"mae\",\n",
    "    \"boosting\": \"gbdt\",\n",
    "    \"verbosity\": -1,\n",
    "    \"random_seed\": 19,\n",
    "    \"metric\": \"mae\",\n",
    "    \"bagging_seed\": 11}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Regression\n",
    "![MAE](./hyperparam_search/lgbm/regression.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_lgb_regression = {\n",
    "    \"num_leaves\": 80,\n",
    "    \"min_data_in_leaf\": 100,\n",
    "    \"max_depth\": 8,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"bagging_freq\": 5,\n",
    "    \"bagging_fraction\": 0.7166666666666667,\n",
    "    \"reg_alpha\": 0.28888888888888886,\n",
    "    \"reg_lambda\": 0.6666666666666666,\n",
    "    \"objective\": \"regression\",\n",
    "    \"boosting\": \"gbdt\",\n",
    "    \"verbosity\": -1, \"random_seed\": 19,\n",
    "    \"metric\": \"mae\",\n",
    "    \"bagging_seed\": 11\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_lgb = best_params_lgb_huber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "oof_lgb, prediction_lgb, scores_lgb, feature_importance_lgb = train_model(X=X_train_scaled,\n",
    "                                                                          X_test=X_test_scaled,\n",
    "                                                                          y=y_tr,\n",
    "                                                                          folds=folds_models,\n",
    "                                                                          params=params_lgb,\n",
    "                                                                          model_type='lgb',\n",
    "                                                                          show_scatter=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = feature_importance_lgb[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n",
    "    by=\"importance\", ascending=False)[:50].index\n",
    "best_features = feature_importance_lgb.loc[feature_importance.feature.isin(cols)]\n",
    "plt.figure(figsize=(16, 12));\n",
    "sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\n",
    "plt.title('LGB Features (avg over folds)');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = pd.DataFrame(dtype=np.float64)\n",
    "submission = pd.read_csv('../input/sample_submission.csv', index_col='seg_id')\n",
    "submission['time_to_failure'] = prediction_lgb\n",
    "submission.to_csv('../output/submission_lgb.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost (Gradient Boosting)\n",
    "Gradient boosting that uses tree based learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "params_xgb = {\n",
    "    'eta': 0.01, #Andrew uses 0.03\n",
    "    'max_depth': 6, #Andrew uses 10\n",
    "    'subsample': 0.5, #Andrew uses 0.9\n",
    "    'objective': 'reg:linear',\n",
    "    'eval_metric': 'mae',\n",
    "    'silent': True,\n",
    "    'nthread': 32\n",
    "} # CV mean score: 2.0801, std: 0.0711.\n",
    "oof_xgb, prediction_xgb, scores_xgb = train_model(X=X_train_scaled,\n",
    "                                                  X_test=X_test_scaled,\n",
    "                                                  y=y_tr,\n",
    "                                                  folds=folds_models,\n",
    "                                                  params=params_xgb,\n",
    "                                                  model_type='xgb',\n",
    "                                                  show_scatter=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM\n",
    "Similar to NuSVC, for regression, uses a parameter nu to control the number of support vectors. However, unlike NuSVC, where nu replaces C, here nu replaces the parameter epsilon of epsilon-SVR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#model = NuSVR(gamma='scale', nu=0.9, C=10.0, tol=0.01) #original values\n",
    "model = NuSVR(gamma='scale', nu=0.63, C=0.4469387755102041, tol=0.01)\n",
    "oof_svr, prediction_svr, scores_svr = train_model(X=X_train_scaled,\n",
    "                                      X_test=X_test_scaled,\n",
    "                                      y=y_tr,\n",
    "                                      folds=folds_models,\n",
    "                                      params=None,\n",
    "                                      model_type='sklearn',\n",
    "                                      model=model,\n",
    "                                      show_scatter=False)\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_cat = {\n",
    "    'loss_function':'MAE'\n",
    "}\n",
    "oof_cat, prediction_cat, scores_cat = train_model(X=X_train_scaled,\n",
    "                                                  X_test=X_test_scaled,\n",
    "                                                  y=y_tr,\n",
    "                                                  folds=folds_models,\n",
    "                                                  params=params_cat,\n",
    "                                                  model_type='cat',\n",
    "                                                  show_scatter=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel Ridge\n",
    "This model combines regularized linear regression with a given kernel (radial basis in this case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = KernelRidge(kernel='rbf', alpha=0.1, gamma=0.01) #Original parameters\n",
    "model = KernelRidge(kernel='rbf', alpha=2.4497346938775513, gamma=0.0018461224489795917)\n",
    "oof_r, prediction_r, scores_r = train_model(X=X_train_scaled,\n",
    "                                            X_test=X_test_scaled,\n",
    "                                            y=y_tr,\n",
    "                                            folds=folds_models,\n",
    "                                            params=None,\n",
    "                                            model_type='sklearn',\n",
    "                                            model=model,\n",
    "                                            show_scatter=False)\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Genetic Program Model\n",
    "Genetic programming model from https://www.kaggle.com/scirpus/andrews-script-plus-a-genetic-program-model?scriptVersionId=10629478"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_gpi = {\n",
    "    'activation':'tanh'\n",
    "}\n",
    "oof_gpi, prediction_gpi, scores_gpi = train_model(X=X_train_scaled,\n",
    "                                                  X_test=X_test_scaled,\n",
    "                                                  y=y_tr,\n",
    "                                                  folds=folds_models,\n",
    "                                                  model_type='gpi',\n",
    "                                                  params=params_gpi,\n",
    "                                                  show_scatter=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "This regressor fits many decision trees with different subsets of the original data and average the predictions between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_rf = {\n",
    "    'max_depth': 12, #8,\n",
    "    'max_features': 'log2', #'auto',\n",
    "    'min_samples_leaf': 2,\n",
    "    'min_samples_split': 4 #6\n",
    "} #CV mean score: 2.0448, std: 0.0793.\n",
    "model = RandomForestRegressor(criterion='mae', n_estimators=200, n_jobs=-1, **params_rf)\n",
    "oof_rf, prediction_rf, scores_rf = train_model(X=X_train_scaled,\n",
    "                                               X_test=X_test_scaled,\n",
    "                                               y=y_tr,\n",
    "                                               folds=folds_models,\n",
    "                                               params=params_rf,\n",
    "                                               model_type='sklearn',\n",
    "                                               model=model,\n",
    "                                               show_scatter=False)\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extremely Randomized Trees\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_ex = {\n",
    "    'max_depth': 12,\n",
    "    'max_features': 'sqrt',\n",
    "    'min_samples_leaf': 2,\n",
    "    'min_samples_split': 4\n",
    "}\n",
    "model = ExtraTreesRegressor(criterion='mae', n_estimators=200, n_jobs=-1, **params_ex)\n",
    "oof_ex, prediction_ex, scores_ex = train_model(X=X_train_scaled,\n",
    "                                               X_test=X_test_scaled,\n",
    "                                               y=y_tr,\n",
    "                                               folds=folds_models,\n",
    "                                               params=params_ex,\n",
    "                                               model_type='sklearn',\n",
    "                                               model=model,\n",
    "                                               show_scatter=False)\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ada Boost\n",
    "AdaBoost begins by fitting a base estimator on the original dataset and then fits additional copies on the same dataset. At each iteration (estimator), the weights of instances are adjusted according to the error of the last prediction. It's similar to the next model, but gradient boosting fits additional estimator copies on the current error and not on the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_ada = {\n",
    "    'learning_rate': 0.001\n",
    "}\n",
    "base = KernelRidge(kernel='rbf', alpha=2.4497346938775513, gamma=0.0018461224489795917)\n",
    "model = AdaBoostRegressor(base_estimator=base, n_estimators=100, **params_ada)\n",
    "oof_ada, prediction_ada, scores_ada = train_model(X=X_train_scaled,\n",
    "                                                  X_test=X_test_scaled,\n",
    "                                                  y=y_tr,\n",
    "                                                  folds=folds_models,\n",
    "                                                  params=params_ada,\n",
    "                                                  model_type='sklearn',\n",
    "                                                  model=model,\n",
    "                                                  show_scatter=False)\n",
    "del base\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = neighbors.KNeighborsRegressor()\n",
    "#parameter_grid = {'n_neighbors': [10, 20, 25, 30, 35, 50]}\n",
    "#grid_search = GridSearchCV(model, param_grid=parameter_grid,\n",
    "#                           cv=folds_models,\n",
    "#                           scoring='neg_mean_absolute_error',\n",
    "#                           n_jobs=-1)\n",
    "#grid_search.fit(X_train_scaled, y_tr)\n",
    "#print('Best score: {}'.format(grid_search.best_score_))\n",
    "#print('Best parameters: {}'.format(grid_search.best_params_))\n",
    "#model = neighbors.KNeighborsRegressor(**grid_search.best_params_)\n",
    "model = neighbors.KNeighborsRegressor(n_neighbors=30)\n",
    "oof_knn, prediction_knn, scores_knn = train_model(X=X_train_scaled,\n",
    "                                                  X_test=X_test_scaled,\n",
    "                                                  y=y_tr,\n",
    "                                                  folds=folds_models,\n",
    "                                                  params=params_knn,\n",
    "                                                  model_type='sklearn',\n",
    "                                                  model=model,\n",
    "                                                  show_scatter=False)\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display training progress by printing a single dot for each completed epoch\n",
    "class PrintDot(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        if epoch % 100 == 0:\n",
    "            print('')\n",
    "        else:\n",
    "            print('.', end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    hist = pd.DataFrame(history.history)\n",
    "    hist['epoch'] = history.epoch\n",
    "  \n",
    "    plt.figure()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Mean Abs Error')\n",
    "    plt.plot(hist['epoch'], hist['mean_absolute_error'],\n",
    "             label='Train Error')\n",
    "    plt.plot(hist['epoch'], hist['val_mean_absolute_error'],\n",
    "             label = 'Val Error')\n",
    "    #plt.ylim([0,5])\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FC model (all_features)x256x1\n",
    "The following model is a fully-connected neural network that uses all the features from the training set.\n",
    "\n",
    "Structure:\n",
    "- Input layer\n",
    "- Fully connected layer of 256 neurons with tanh activation\n",
    "- Output layer with linear activation\n",
    "![(all_features)x256x1 model](./figures/all_featuresx256x1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_function = 'tanh'\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(256,\n",
    "                                input_dim=X_train_scaled.shape[-1],\n",
    "                                activation=activation_function))\n",
    "model.add(tf.keras.layers.Dense(1, activation='linear'))\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(lr = 0.001, decay = 0.001 / 32)\n",
    "model.compile(loss='mean_absolute_error', optimizer=optimizer, metrics=['mean_absolute_error'])\n",
    "EPOCHS = 250\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='mean_absolute_error', patience=100)\n",
    "history = model.fit(\n",
    "    X_train_scaled,\n",
    "    y_tr,\n",
    "    validation_split=0.2,\n",
    "    epochs=EPOCHS,\n",
    "    shuffle=True,\n",
    "    verbose=0,\n",
    "    callbacks=[early_stop, PrintDot()])\n",
    "hist = pd.DataFrame(history.history)\n",
    "val_score = hist['val_mean_absolute_error'].iloc[-1]\n",
    "\n",
    "#Show results\n",
    "print(f'val_score={val_score}')\n",
    "plot_history(history)\n",
    "\n",
    "y_pred = model.predict(X_train_scaled).reshape(-1,)\n",
    "fig, axis = plt.subplots(1, 2, figsize=(12,5))\n",
    "ax1, ax2 = axis\n",
    "ax1.set_xlabel('actual')\n",
    "ax1.set_ylabel('predicted')\n",
    "ax2.set_xlabel('train index')\n",
    "ax2.set_ylabel('time to failure')\n",
    "ax1.scatter(y_tr, y_pred, color='brown')\n",
    "ax1.plot([(0, 0), (20, 20)], [(0, 0), (20, 20)], color='blue')\n",
    "ax2.plot(y_tr, color='blue', label='y_train')\n",
    "ax2.plot(y_pred, color='orange')\n",
    "\n",
    "#Save results\n",
    "print(model.summary())\n",
    "tf.keras.utils.plot_model(model, to_file='./figures/all_featuresx256x1.png', show_shapes=True, show_layer_names=True)\n",
    "\n",
    "# Housekeeping\n",
    "del model\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensemble NN model with FC 5x256x1 networks\n",
    "The following model is an ensemble of small fully-connected neural networks. Each small neural network uses 5 features from the training set. The features are selected from exhaustive search using the script `feature_search_nn_5x256x1.py`.\n",
    "\n",
    "Structure of each NN:\n",
    "- Input layer\n",
    "- Fully connected layer of 5 neurons with tanh activation\n",
    "- Output layer with leaky ReLU\n",
    "![5x256x1.png model](./figures/5x256x1.png)\n",
    "\n",
    "Ensemble:\n",
    "- Uses `num_single_nets` small networks\n",
    "- Weighted average using the inverse of the validation scores (`val_score`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#out_path = '../output/search_gdi_features_nn_5x256x1_tanh_linear/'\n",
    "out_path = '../output/search_all_features_nn_5x1x1_tanh_linear/'\n",
    "all_files = [f for f in os.listdir(out_path) if os.path.isfile(os.path.join(out_path, f))]\n",
    "li = []\n",
    "for f in all_files:\n",
    "    df = pd.read_csv(os.path.join(out_path, f), index_col=None, header=0)\n",
    "    li.append(df)\n",
    "results_nn = pd.concat(li, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_single_nets = 256\n",
    "results_nn_sorted = results_nn.sort_values(by=['val_score']).iloc[0:num_single_nets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_nn_sorted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.zeros((num_single_nets,len(X_test_scaled)))\n",
    "oofs = np.zeros((num_single_nets,len(X_train_scaled)))\n",
    "scores = []\n",
    "for i in tqdm_notebook(range(num_single_nets)):\n",
    "    nn_features_list = [results_nn_sorted['f1'].iloc[i],\n",
    "                        results_nn_sorted['f2'].iloc[i],\n",
    "                        results_nn_sorted['f3'].iloc[i],\n",
    "                        results_nn_sorted['f4'].iloc[i],\n",
    "                        results_nn_sorted['f5'].iloc[i]]\n",
    "    X_train_nn = X_train_scaled[nn_features_list]\n",
    "    X_test_nn = X_test_scaled[nn_features_list]\n",
    "    print(f\"Building FC-NN {i}. Features considered:\")  \n",
    "    print(nn_features_list)\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Dense(1,\n",
    "                                    input_dim=X_train_nn.shape[-1],\n",
    "                                    activation='tanh'))\n",
    "    model.add(tf.keras.layers.Dense(1,\n",
    "                                    activation='linear'))\n",
    "\n",
    "    model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
    "    EPOCHS = 100\n",
    "    early_stop = tf.keras.callbacks.EarlyStopping(monitor='mean_absolute_error', patience=100)\n",
    "    history = model.fit(\n",
    "        X_train_nn,\n",
    "        y_tr,\n",
    "        validation_split=0.2,\n",
    "        epochs=EPOCHS,\n",
    "        shuffle=True,\n",
    "        verbose=0,\n",
    "        callbacks=[early_stop, PrintDot()])\n",
    "    hist = pd.DataFrame(history.history)\n",
    "    val_score = hist['val_mean_absolute_error'].iloc[-1]\n",
    "    scores.append(val_score)\n",
    "    print(f'val_score={val_score}')\n",
    "\n",
    "    oofs[i] = model.predict(X_train_nn).reshape(-1,)\n",
    "    predictions[i] = model.predict(X_test_nn).reshape(-1,)\n",
    "    \n",
    "    #Save results\n",
    "    if i==1:\n",
    "        print(model.summary())\n",
    "        tf.keras.utils.plot_model(model, to_file='./figures/5x256x1.png', show_shapes=True, show_layer_names=True)\n",
    "\n",
    "    #Housekeeping\n",
    "    del model\n",
    "    tf.keras.backend.clear_session()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions = predictions.clip(min=0)\n",
    "oofs = oofs.clip(min=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine estimations\n",
    "model_confidence = 1/np.array(scores)\n",
    "model_confidence /= np.sum(model_confidence)\n",
    "prediction = np.average(predictions.T, weights=model_confidence, axis=1)\n",
    "oof = np.average(oofs.T, weights=model_confidence, axis=1)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(model_confidence)\n",
    "\n",
    "#Show results\n",
    "fig, axis = plt.subplots(1, 2, figsize=(12,5))\n",
    "ax1, ax2 = axis\n",
    "ax1.set_xlabel('actual')\n",
    "ax1.set_ylabel('predicted')\n",
    "ax2.set_xlabel('train index')\n",
    "ax2.set_ylabel('time to failure')\n",
    "ax1.scatter(y_tr, oof, color='brown')\n",
    "ax1.plot([(0, 0), (20, 20)], [(0, 0), (20, 20)], color='blue')\n",
    "ax2.plot(y_tr, color='blue', label='y_train')\n",
    "ax2.plot(oof, color='orange')\n",
    "print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = pd.DataFrame(dtype=np.float64)\n",
    "submission = pd.read_csv('../input/sample_submission.csv', index_col='seg_id')\n",
    "submission['time_to_failure'] = prediction\n",
    "submission.to_csv('../output/submission_nn_ensemble_5x1x1_tanh_Linear.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "params_dnn = {\n",
    "    'dropout': 0.70,\n",
    "    'num_layers': 2,\n",
    "    'num_neurons': 90,\n",
    "    'activation_function': 'tanh'\n",
    "}\n",
    "oof_dnn, prediction_dnn, score_mean_dnn, score_std_dnn = train_model(X=X_train_scaled,\n",
    "                                                                     X_test=X_test_scaled,\n",
    "                                                                     y=y_tr,\n",
    "                                                                     folds=folds_models,\n",
    "                                                                     params=params_dnn,\n",
    "                                                                     model_type='nn',\n",
    "                                                                     show_scatter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8));\n",
    "scores_df = pd.DataFrame()\n",
    "scores_df['LGBMRegressor'] = scores_lgb\n",
    "scores_df['XGBRegressor'] = scores_xgb\n",
    "scores_df['NuSVR'] = scores_svr\n",
    "scores_df['CatBoostRegressor'] = scores_cat\n",
    "scores_df['Ridge'] = scores_r\n",
    "scores_df['GPI'] = scores_gpi\n",
    "scores_df['RandomForestRegressor'] = scores_rf\n",
    "scores_df['ExtraTreesRegressor'] = scores_ex\n",
    "scores_df['AdaBoostRegressor'] = scores_ada\n",
    "scores_df['KNN'] = scores_knn\n",
    "\n",
    "ax = sns.boxplot(data=scores_df.reindex(scores_df.mean().sort_values().index, axis=1));\n",
    "#ax.set(yscale=\"log\")\n",
    "\n",
    "plt.xticks(rotation=45);\n",
    "plt.xlabel('Method');\n",
    "plt.ylabel('Score');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ELI5 and permutation importance\n",
    "ELI5 is a package with provides explanations for ML models. It can do this not only for linear models, but also for tree based like Random Forest or lightgbm.\n",
    "\n",
    "**Important notice**: running eli5 on all features takes a lot of time, so I run the cell below in `version 14` and printed the top-50 features. In the following versions I'll use these 50 columns and use eli5 to find top-40 of them so that it takes less time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_scaled, y_tr, test_size=0.1)\n",
    "model = lgb.LGBMRegressor(**params_lgb, n_estimators = 50000, n_jobs = -1, verbose=-1)\n",
    "model.fit(X_train,\n",
    "          y_train, \n",
    "          eval_set=[(X_train, y_train), (X_valid, y_valid)],\n",
    "          eval_metric='mae',\n",
    "          verbose=10000,\n",
    "          early_stopping_rounds=200)\n",
    "\n",
    "perm = eli5.sklearn.PermutationImportance(model, random_state=1).fit(X_train, y_train)\n",
    "#eli5.show_weights(perm, top=50, feature_names=X_train_scaled.columns.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_num_features = 40\n",
    "top_features = [i for i in eli5.formatters.as_dataframe.explain_weights_df(model).feature if 'BIAS' not in i][:top_num_features]\n",
    "oof_lgb_top, prediction_lgb_top, scores_lgb_top, feature_importance_lgb_top = train_model(\n",
    "    X=X_train_scaled[top_features],\n",
    "    X_test=X_test_scaled[top_features],\n",
    "    y=y_tr,\n",
    "    folds=folds_models,\n",
    "    params=params_lgb,\n",
    "    model_type='lgb',\n",
    "    show_scatter=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping highly correlated features\n",
    "\n",
    "Due to the huge number of features there are certainly some highly correlated features, let's try droping them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled_uncorr = X_train_scaled.copy()\n",
    "X_test_scaled_uncorr = X_test_scaled.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://chrisalbon.com/machine_learning/feature_selection/drop_highly_correlated_features/\n",
    "corr_matrix = X_train_scaled_uncorr.corr().abs()\n",
    "# Select upper triangle of correlation matrix\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "# Find index of feature columns with correlation greater than 0.99\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.99)]\n",
    "X_train_scaled_uncorr = X_train_scaled_uncorr.drop(to_drop, axis=1)\n",
    "X_test_scaled_uncorr = X_test_scaled_uncorr.drop(to_drop, axis=1)\n",
    "\n",
    "print(X_train_scaled.shape)\n",
    "print(X_train_scaled_uncorr.shape)\n",
    "\n",
    "oof_lgb_uncorr, prediction_lgb_uncorr, scores_lgb_uncorr, feature_importance_lgb_uncorr = train_model(\n",
    "    X=X_train_scaled_uncorr,\n",
    "    X_test=X_test_scaled_uncorr,\n",
    "    y=y_tr,\n",
    "    folds=folds_models,\n",
    "    params=params_lgb,\n",
    "    model_type='lgb',\n",
    "    show_scatter=False)\n",
    "cols = feature_importance_lgb_uncorr[['feature', 'importance']].groupby('feature').mean().sort_values(\n",
    "    by='importance', ascending=False).index\n",
    "best_features = feature_importance_lgb_uncorr.loc[feature_importance_lgb_uncorr.feature.isin(cols)]\n",
    "plt.figure(figsize=(16, 20));\n",
    "sns.barplot(x='importance', y='feature', data=best_features.sort_values(by='importance', ascending=False));\n",
    "plt.title('LGB Features (avg over folds)');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_scaled_uncorr, y_tr, test_size=0.1)\n",
    "model = lgb.LGBMRegressor(**params_lgb, n_estimators = 50000, n_jobs = -1, verbose=-1)\n",
    "model.fit(X_train,\n",
    "          y_train, \n",
    "          eval_set=[(X_train, y_train), (X_valid, y_valid)],\n",
    "          eval_metric='mae',\n",
    "          verbose=10000,\n",
    "          early_stopping_rounds=200)\n",
    "\n",
    "perm = eli5.sklearn.PermutationImportance(model, random_state=1).fit(X_train, y_train)\n",
    "#eli5.show_weights(perm, top=50, feature_names=X_train_scaled_uncorr.columns.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_num_features = 40\n",
    "top_features = [i for i in eli5.formatters.as_dataframe.explain_weights_df(model).feature if 'BIAS' not in i][:top_num_features]\n",
    "oof_lgb_uncorr_top, prediction_lgb_uncorr_top, scores_lgb_uncorr_top, feature_importance_lgb_uncorr_top = train_model(\n",
    "    X=X_train_scaled[top_features],\n",
    "    X_test=X_test_scaled[top_features],\n",
    "    y=y_tr,\n",
    "    folds=folds_models,\n",
    "    params=params_lgb,\n",
    "    model_type='lgb',\n",
    "    show_scatter=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8));\n",
    "scores_df = pd.DataFrame()\n",
    "scores_df['LGB'] = scores_lgb\n",
    "scores_df['LGB_top'] = scores_lgb_top\n",
    "scores_df['LGB_uncorr'] = scores_lgb_uncorr\n",
    "scores_df['LGB_uncorr_top'] = scores_lgb_uncorr_top\n",
    "scores_df['XGB'] = scores_xgb\n",
    "scores_df['SVR'] = scores_svr\n",
    "scores_df['CatBoost'] = scores_cat\n",
    "scores_df['Ridge'] = scores_r\n",
    "scores_df['GPI'] = scores_gpi\n",
    "scores_df['RandomForest'] = scores_rf\n",
    "scores_df['ExtraTrees'] = scores_ex\n",
    "scores_df['AdaBoost'] = scores_ada\n",
    "scores_df['KNN'] = scores_knn\n",
    "\n",
    "ax = sns.boxplot(data=scores_df.reindex(scores_df.mean().sort_values().index, axis=1));\n",
    "ax.set(yscale=\"log\")\n",
    "\n",
    "plt.xticks(rotation=45);\n",
    "plt.xlabel('Method');\n",
    "plt.ylabel('Score');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking and blending\n",
    "And now let's try stacking :) We can use the same function for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stack = np.vstack([oof_lgb, oof_xgb, oof_svr, oof_r,\n",
    "                         oof_cat, oof_gdi, oof_rf, oof_ex,\n",
    "                         oof_ada]).transpose()\n",
    "train_stack = pd.DataFrame(train_stack, columns = ['lgb', 'xgb', 'svr', 'r', 'cat', 'gdi', 'rf', 'ex', 'ada'])\n",
    "test_stack = np.vstack([prediction_lgb, prediction_xgb, prediction_svr, prediction_r,\n",
    "                        prediction_cat, prediction_gdi, prediction_rf, prediction_ex,\n",
    "                        prediction_ada]).transpose()\n",
    "test_stack = pd.DataFrame(test_stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_lgb_stack = {\n",
    "    'objective': 'huber',\n",
    "    'boosting': 'gbdt',\n",
    "    'verbosity': -1,\n",
    "    'num_leaves': 8, #54,\n",
    "    'min_data_in_leaf': 100, #79,\n",
    "    'max_depth': 5, #-1,\n",
    "    'learning_rate': 0.01, #0.01,\n",
    "    'bagging_freq': 5, #5,\n",
    "    'bagging_fraction': 0.7555555555555555, #0.8126672064208567,\n",
    "    'bagging_seed': 11,\n",
    "    'metric': 'mae',\n",
    "    'reg_alpha': 0.95, #0.1302650970728192,\n",
    "    'reg_lambda': 0.19444444444444445 #0.3603427518866501\n",
    "}\n",
    "oof_lgb_stack, prediction_lgb_stack, score_mean_lgb_stack, score_std_lgb_stack, feature_importance_lgb_stack = train_model(\n",
    "    X=train_stack,\n",
    "    X_test=test_stack,\n",
    "    y=y_tr,\n",
    "    folds=folds_models,\n",
    "    params=params_lgb_stack,\n",
    "    model_type='lgb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = feature_importance_lgb_stack[['feature', 'importance']].groupby('feature').mean().sort_values(\n",
    "    by='importance', ascending=False).index\n",
    "best_features = feature_importance_lgb_stack.loc[feature_importance_lgb_stack.feature.isin(cols)]\n",
    "plt.figure(figsize=(16, 8));\n",
    "sns.barplot(x='importance', y='feature', data=best_features.sort_values(by='importance', ascending=False));\n",
    "plt.title('LGB Features (avg over folds)');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_blend = (oof_lgb + oof_xgb + oof_svr + oof_r + oof_cat + oof_gdi + oof_rf + oof_ex + oof_ada) / 9\n",
    "prediction_blend = (prediction_lgb + prediction_xgb + prediction_svr \n",
    "                    + prediction_cat + prediction_r + prediction_gdi\n",
    "                    + prediction_rf + prediction_ex + prediction_ada) / 9\n",
    "score_mean_blend = (score_mean_lgb + score_mean_xgb + score_mean_svr + score_mean_cat\n",
    "                    + score_mean_r + score_mean_gdi + score_mean_rf + score_mean_ex + score_mean_ada) / 9\n",
    "score_std_blend = (score_std_lgb + score_std_xgb + score_std_svr + score_std_cat\n",
    "                    + score_std_r + score_std_gdi + score_std_rf + score_std_ex + score_std_ada) / 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = pd.DataFrame(dtype=np.float64)\n",
    "submission = pd.read_csv('../input/sample_submission.csv', index_col='seg_id')\n",
    "my_methods = ['lgb', 'xgb', 'svr', 'r', 'cat', 'gdi', 'rf', 'ex', 'ada', 'lgb_stack', 'blend']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for method in my_methods:\n",
    "    submission['time_to_failure'] = globals()['prediction_' + method]\n",
    "    submission.to_csv('../output/submission_' + method + '.csv')\n",
    "    evaluation.loc[method, 'mean'] = globals()['score_mean_' + method]\n",
    "    evaluation.loc[method, 'std'] = globals()['score_std_' + method]\n",
    "evaluation.to_csv(f\"../output/evaluation_{str(datetime.datetime.now())}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_methods(evaluation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
