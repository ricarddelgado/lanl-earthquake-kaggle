{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Master jupyter notebook for LANL - SlimBros Team"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correctly predicting earthquakes is very important for preventing deaths and damage to infrastructure. In this competition we try to predict time left to the next laboratory earthquake based on seismic signal data. Training data represents one huge signal, but in test data we have many separate chunks, for each of which we need to predict time to failure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries\n",
    "Let's import everything we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import time\n",
    "import random\n",
    "import datetime\n",
    "import warnings\n",
    "import feather\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "#from keras import layers, Sequential, callbacks, backend\n",
    "\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, AdaBoostRegressor\n",
    "from sklearn.svm import NuSVR, SVR\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.linear_model import Ridge, RidgeCV\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold, GridSearchCV, cross_val_score\n",
    "from utils import generate_segment_start_ids, compare_methods\n",
    "from features import gpi, create_all_features\n",
    "\n",
    "#Configure the environment\n",
    "%matplotlib inline\n",
    "pd.options.display.precision = 15\n",
    "warnings.filterwarnings('ignore')\n",
    "random.seed(1013)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load/compute the necessary features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_features = False \n",
    "# The computed features are saved in an hdf file along with the time_to_failure to \n",
    "# save the time spend reading the training data and the feature computation\n",
    "#train_data_format = 'csv'\n",
    "train_data_format = 'feather'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_data(file_format):\n",
    "    \"\"\"Load the training dataset.\"\"\"\n",
    "    print(f\"Loading data from {file_format} file:\", end=\"\")\n",
    "    if file_format.lower() == 'feather':\n",
    "        train_df = feather.read_dataframe('../input/train.feather')\n",
    "    else:\n",
    "        train_df = pd.read_csv('../input/train.csv', dtype={'acoustic_data': np.int16,\n",
    "                                                            'time_to_failure': np.float32})\n",
    "        feather.write_dataframe(train_df, '../input/train.feather')\n",
    "    print(\"Done\")\n",
    "    return train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from feather file:Done\n"
     ]
    }
   ],
   "source": [
    "train = load_train_data(train_data_format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature generation\n",
    "- Usual aggregations: mean, std, min and max;\n",
    "- Average difference between the consequitive values in absolute and percent values;\n",
    "- Absolute min and max vallues;\n",
    "- Aforementioned aggregations for first and last 10000 and 50000 values - I think these data should be useful;\n",
    "- Max value to min value and their differencem also count of values bigger that 500 (arbitrary threshold);\n",
    "- Quantile features from this kernel: https://www.kaggle.com/andrekos/basic-feature-benchmark-with-quantiles\n",
    "- Trend features from this kernel: https://www.kaggle.com/jsaguiar/baseline-with-abs-and-trend-features\n",
    "- Rolling features from this kernel: https://www.kaggle.com/wimwim/rolling-quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_files_present = (os.path.isfile('../tmp_results/X_tr.hdf') and \n",
    "                       os.path.isfile('../tmp_results/X_test.hdf') and \n",
    "                       os.path.isfile('../tmp_results/y_tr.hdf') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading hdf files:Done\n"
     ]
    }
   ],
   "source": [
    "if (not compute_features) and saved_files_present:\n",
    "    print(f\"Reading hdf files:\", end=\"\")\n",
    "    X_tr = pd.read_hdf('../tmp_results/X_tr.hdf', 'data')\n",
    "    X_test = pd.read_hdf('../tmp_results/X_test.hdf', 'data')\n",
    "    y_tr = pd.read_hdf('../tmp_results/y_tr.hdf', 'data')  \n",
    "    print(\"Done\")\n",
    "else:\n",
    "    fs = 4000000 #Sampling frequency of the raw signal\n",
    "\n",
    "    #Compute features for the training data\n",
    "    segment_size = 150000\n",
    "    segment_start_ids = generate_segment_start_ids('uniform_no_jump', segment_size, train)\n",
    "    X_tr = pd.DataFrame(index=range(len(segment_start_ids)), dtype=np.float64)\n",
    "    y_tr = pd.DataFrame(index=range(len(segment_start_ids)), dtype=np.float64, columns=['time_to_failure'])\n",
    "    for idx in tqdm_notebook(range(len(segment_start_ids))):        \n",
    "        seg_id = segment_start_ids[idx]\n",
    "        seg = train.iloc[seg_id:seg_id + segment_size]\n",
    "        create_all_features(idx, seg, X_tr, fs)\n",
    "        y_tr.loc[idx, 'time_to_failure'] = seg['time_to_failure'].values[-1]\n",
    "    # Sanity check\n",
    "    means_dict = {}\n",
    "    for col in X_tr.columns:\n",
    "        if X_tr[col].isnull().any():\n",
    "            print(col)\n",
    "            mean_value = X_tr.loc[X_tr[col] != -np.inf, col].mean()\n",
    "            X_tr.loc[X_tr[col] == -np.inf, col] = mean_value\n",
    "            X_tr[col] = X_tr[col].fillna(mean_value)\n",
    "            means_dict[col] = mean_value\n",
    "\n",
    "    #Compute features for the test data\n",
    "    submission = pd.read_csv('../input/sample_submission.csv', index_col='seg_id')\n",
    "    X_test = pd.DataFrame(columns=X_tr.columns, dtype=np.float64, index=submission.index)\n",
    "    for i, seg_id in enumerate(tqdm_notebook(X_test.index)):\n",
    "        seg = pd.read_csv('../input/test/' + seg_id + '.csv')\n",
    "        create_all_features(seg_id, seg, X_test, fs)\n",
    "\n",
    "    # Sanity check\n",
    "    for col in X_test.columns:\n",
    "        if X_test[col].isnull().any():\n",
    "            X_test.loc[X_test[col] == -np.inf, col] = means_dict[col]\n",
    "            X_test[col] = X_test[col].fillna(means_dict[col])\n",
    "            \n",
    "    X_tr.to_hdf('../tmp_results/X_tr.hdf', 'data')\n",
    "    X_test.to_hdf('../tmp_results/X_test.hdf', 'data')\n",
    "    y_tr.to_hdf('../tmp_results/y_tr.hdf', 'data')\n",
    "    \n",
    "    del segment_start_ids\n",
    "    del means_dict\n",
    "    del submission\n",
    "    \n",
    "    print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldata = pd.concat([X_tr, X_test])\n",
    "scaler = StandardScaler()\n",
    "alldata = pd.DataFrame(scaler.fit_transform(alldata), columns=alldata.columns)\n",
    "X_train_scaled = alldata[:X_tr.shape[0]]\n",
    "X_test_scaled = alldata[X_tr.shape[0]:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X, X_test, y, folds, params=None, model_type='lgb',\n",
    "                model=None, show_scatter=False):\n",
    "\n",
    "    oof = np.zeros(len(X))\n",
    "    prediction = np.zeros(len(X_test))\n",
    "    scores = []\n",
    "    n_fold = folds.get_n_splits()\n",
    "    \n",
    "    feature_importance = pd.DataFrame()\n",
    "    for fold_n, (train_index, valid_index) in enumerate(folds.split(X)):\n",
    "        print('Fold', fold_n, 'started at', time.ctime())\n",
    "        X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n",
    "        y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n",
    "\n",
    "        if model_type == 'nn':\n",
    "            dropout = 0.7\n",
    "            num_layers = 2\n",
    "            num_neurons = 128\n",
    "            model = tf.keras.Sequential()\n",
    "            model.add(tf.keras.layers.Dense(1024, input_dim=216, activation=tf.nn.relu))\n",
    "            model.add(tf.keras.layers.Dropout(dropout))\n",
    "            for l in range(num_layers):\n",
    "                model.add(tf.keras.layers.Dense(num_neurons, activation=tf.nn.relu))\n",
    "                model.add(tf.keras.layers.Dropout(dropout))\n",
    "            model.add(tf.keras.layers.Dense(1))\n",
    "            model = tf.keras.utils.multi_gpu_model(model, gpus=2, cpu_merge=False)\n",
    "            model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
    "            EPOCHS = 1000\n",
    "            early_stop = tf.keras.callbacks.EarlyStopping(monitor='mean_absolute_error', patience=100)\n",
    "\n",
    "            history = model.fit(\n",
    "                X_train,\n",
    "                y_train,\n",
    "                epochs=EPOCHS,\n",
    "                validation_data = (X_valid, y_valid), \n",
    "                verbose=0,\n",
    "                callbacks=[early_stop, PrintDot()])\n",
    "            hist = pd.DataFrame(history.history)\n",
    "            val_score = hist['val_mean_absolute_error'].iloc[-1]\n",
    "            print(f'val_score={val_score}')\n",
    "            plot_history(history)\n",
    "        \n",
    "            y_pred_valid = model.predict(X_valid).reshape(-1,)\n",
    "            y_pred = model.predict(X_test).reshape(-1,)\n",
    "\n",
    "            score = mean_absolute_error(y_valid, y_pred_valid)\n",
    "            print(f'Fold {fold_n}. MAE: {score:.4f}.')\n",
    "        \n",
    "        if model_type == 'lgb':\n",
    "            model = lgb.LGBMRegressor(**params, n_estimators = 50000, n_jobs = 32)\n",
    "            model.fit(X_train, y_train, \n",
    "                      eval_set=[(X_train, y_train), (X_valid, y_valid)],\n",
    "                      eval_metric='mae',\n",
    "                      verbose=10000,\n",
    "                      early_stopping_rounds=2000)\n",
    "            \n",
    "            y_pred_valid = model.predict(X_valid)\n",
    "            y_pred = model.predict(X_test, num_iteration=model.best_iteration_)\n",
    "            \n",
    "        if model_type == 'xgb':\n",
    "            train_data = xgb.DMatrix(data=X_train, label=y_train, feature_names=X.columns)\n",
    "            valid_data = xgb.DMatrix(data=X_valid, label=y_valid, feature_names=X.columns)\n",
    "\n",
    "            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]\n",
    "            model = xgb.train(dtrain=train_data,\n",
    "                              num_boost_round=20000,\n",
    "                              evals=watchlist,\n",
    "                              early_stopping_rounds=200,\n",
    "                              verbose_eval=500,\n",
    "                              params=params)\n",
    "\n",
    "            y_pred_valid = model.predict(xgb.DMatrix(X_valid, feature_names=X.columns),\n",
    "                                         ntree_limit=model.best_ntree_limit)\n",
    "            y_pred = model.predict(xgb.DMatrix(X_test, feature_names=X.columns),\n",
    "                                   ntree_limit=model.best_ntree_limit)\n",
    "        \n",
    "        if model_type == 'sklearn':\n",
    "            model = model\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            y_pred_valid = model.predict(X_valid).reshape(-1,)\n",
    "            score = mean_absolute_error(y_valid, y_pred_valid)\n",
    "            y_pred = model.predict(X_test).reshape(-1,)\n",
    "            print(f'Fold {fold_n}. MAE: {score:.4f}.')\n",
    "            print('')\n",
    "        \n",
    "        if model_type == 'cat':\n",
    "            model = CatBoostRegressor(iterations=20000,  eval_metric='MAE', task_type='GPU', **params)\n",
    "            model.fit(X_train, y_train, eval_set=(X_valid, y_valid), cat_features=[], use_best_model=True,\n",
    "                      verbose=False)\n",
    "\n",
    "            y_pred_valid = model.predict(X_valid)\n",
    "            y_pred = model.predict(X_test)\n",
    "        \n",
    "        if model_type == 'gdi':\n",
    "            y_pred_valid = gpi(X_valid).values\n",
    "            y_pred = gpi(X_test).values\n",
    "        \n",
    "        oof[valid_index] = y_pred_valid.reshape(-1,)\n",
    "        scores.append(mean_absolute_error(y_valid, y_pred_valid))\n",
    "        \n",
    "        prediction += y_pred    \n",
    "        \n",
    "        if model_type == 'lgb':\n",
    "            # feature importance\n",
    "            fold_importance = pd.DataFrame()\n",
    "            fold_importance['feature'] = X.columns\n",
    "            fold_importance['importance'] = model.feature_importances_\n",
    "            fold_importance['fold'] = fold_n + 1\n",
    "            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n",
    "\n",
    "    prediction /= n_fold\n",
    "    \n",
    "    if show_scatter:\n",
    "        fig, axis = plt.subplots(1, 2, figsize=(12,5))\n",
    "        ax1, ax2 = axis\n",
    "        ax1.set_xlabel('actual')\n",
    "        ax1.set_ylabel('predicted')\n",
    "        ax2.set_xlabel('train index')\n",
    "        ax2.set_ylabel('time to failure')\n",
    "        \n",
    "        ax1.scatter(y, oof, color='brown')\n",
    "        ax1.plot([(0, 0), (20, 20)], [(0, 0), (20, 20)], color='blue')\n",
    "\n",
    "        ax2.plot(y, color='blue', label='y_train')\n",
    "        ax2.plot(oof, color='orange')\n",
    "    \n",
    "    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n",
    "    \n",
    "    if model_type == 'lgb':\n",
    "        feature_importance['importance'] /= n_fold\n",
    "        return oof, prediction, np.mean(scores), np.std(scores), feature_importance\n",
    "    else:\n",
    "        return oof, prediction, np.mean(scores), np.std(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fold = 5\n",
    "folds_models = KFold(n_splits=n_fold, shuffle=True, random_state=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a few different models and submit the one with the best validation score. The predicted values in the following plots are using a out-of-fold scheme."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LGBM (Gradient Boosting)\n",
    "Gradient boosting that uses tree based learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "params_lgb = {\n",
    "    'objective': 'huber',\n",
    "    'boosting': 'gbdt',\n",
    "    'verbosity': -1,\n",
    "    'num_leaves': 8, #54,\n",
    "    'min_data_in_leaf': 100, #79,\n",
    "    'max_depth': 6, #-1,\n",
    "    'learning_rate': 0.01,\n",
    "    'bagging_freq': 3, #5,\n",
    "    'bagging_fraction': 0.7166666666666667, #0.8126672064208567,\n",
    "    'bagging_seed': 11,\n",
    "    'metric': 'mae',\n",
    "    'reg_alpha': 0.19444444444444445, #0.1302650970728192,\n",
    "    'reg_lambda': 0.8555555555555555 #0.3603427518866501\n",
    "}\n",
    "oof_lgb, prediction_lgb, score_mean_lgb, score_std_lgb, feature_importance = train_model(X=X_train_scaled,\n",
    "                                                                                         X_test=X_test_scaled,\n",
    "                                                                                         y=y_tr,\n",
    "                                                                                         folds=folds_models,\n",
    "                                                                                         params=params_lgb,\n",
    "                                                                                         model_type='lgb',\n",
    "                                                                                         show_scatter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost (Gradient Boosting)\n",
    "Gradient boosting that uses tree based learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "params_xgb = {\n",
    "    'eta': 0.01, #Andrew uses 0.03\n",
    "    'max_depth': 6, #Andrew uses 10\n",
    "    'subsample': 0.5, #Andrew uses 0.9\n",
    "    'objective': 'reg:linear',\n",
    "    'eval_metric': 'mae',\n",
    "    'silent': True,\n",
    "    'nthread': 32\n",
    "} # CV mean score: 2.0801, std: 0.0711.\n",
    "oof_xgb, prediction_xgb, score_mean_xgb, score_std_xgb = train_model(X=X_train_scaled,\n",
    "                                                                     X_test=X_test_scaled,\n",
    "                                                                     y=y_tr,\n",
    "                                                                     folds=folds_models,\n",
    "                                                                     params=params_xgb,\n",
    "                                                                     model_type='xgb',\n",
    "                                                                     show_scatter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM\n",
    "Similar to NuSVC, for regression, uses a parameter nu to control the number of support vectors. However, unlike NuSVC, where nu replaces C, here nu replaces the parameter epsilon of epsilon-SVR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#model = NuSVR(gamma='scale', nu=0.9, C=10.0, tol=0.01) #original values\n",
    "model = NuSVR(gamma='scale', nu=0.63, C=0.4469387755102041, tol=0.01)\n",
    "oof_svr, prediction_svr, score_mean_svr, score_std_svr = train_model(X=X_train_scaled,\n",
    "                                                                     X_test=X_test_scaled,\n",
    "                                                                     y=y_tr,\n",
    "                                                                     folds=folds_models,\n",
    "                                                                     params=None,\n",
    "                                                                     model_type='sklearn',\n",
    "                                                                     model=model,\n",
    "                                                                     show_scatter=True)\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_cat = {\n",
    "    'loss_function':'MAE'\n",
    "}\n",
    "oof_cat, prediction_cat, score_mean_cat, score_std_cat = train_model(X=X_train_scaled,\n",
    "                                                                     X_test=X_test_scaled,\n",
    "                                                                     y=y_tr,\n",
    "                                                                     folds=folds_models,\n",
    "                                                                     params=params_cat,\n",
    "                                                                     model_type='cat',\n",
    "                                                                     show_scatter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel Ridge\n",
    "This model combines regularized linear regression with a given kernel (radial basis in this case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = KernelRidge(kernel='rbf', alpha=0.1, gamma=0.01) #Original parameters\n",
    "model = KernelRidge(kernel='rbf', alpha=2.4497346938775513, gamma=0.0018461224489795917)\n",
    "oof_r, prediction_r, score_mean_r, score_std_r = train_model(X=X_train_scaled,\n",
    "                                                             X_test=X_test_scaled,\n",
    "                                                             y=y_tr,\n",
    "                                                             folds=folds_models,\n",
    "                                                             params=None,\n",
    "                                                             model_type='sklearn',\n",
    "                                                             model=model,\n",
    "                                                             show_scatter=True)\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Genetic Program Model\n",
    "Genetic programming model from https://www.kaggle.com/scirpus/andrews-script-plus-a-genetic-program-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are no tuning here. We took at as face value.\n",
    "oof_gdi, prediction_gdi, score_mean_gdi, score_std_gdi = train_model(X=X_train_scaled,\n",
    "                                                                     X_test=X_test_scaled,\n",
    "                                                                     y=y_tr,\n",
    "                                                                     folds=folds_models,\n",
    "                                                                     model_type='gdi',\n",
    "                                                                     show_scatter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "This regressor fits many decision trees with different subsets of the original data and average the predictions between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_rf = {\n",
    "    'max_depth': 12, #8,\n",
    "    'max_features': 'log2', #'auto',\n",
    "    'min_samples_leaf': 2,\n",
    "    'min_samples_split': 4 #6\n",
    "} #CV mean score: 2.0448, std: 0.0793.\n",
    "model = RandomForestRegressor(criterion='mae', n_estimators=200, n_jobs=-1, **params_rf)\n",
    "oof_rf, prediction_rf, score_mean_rf, score_std_rf = train_model(X=X_train_scaled,\n",
    "                                                                 X_test=X_test_scaled,\n",
    "                                                                 y=y_tr,\n",
    "                                                                 folds=folds_models,\n",
    "                                                                 params=params_rf,\n",
    "                                                                 model_type='sklearn',\n",
    "                                                                 model=model,\n",
    "                                                                 show_scatter=True)\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extremely Randomized Trees\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_ex = {\n",
    "    'max_depth': 12,\n",
    "    'max_features': 'sqrt',\n",
    "    'min_samples_leaf': 2,\n",
    "    'min_samples_split': 4\n",
    "}\n",
    "model = ExtraTreesRegressor(criterion='mae', n_estimators=200, n_jobs=-1, **params_ex)\n",
    "oof_ex, prediction_ex, score_mean_ex, score_std_ex = train_model(X=X_train_scaled,\n",
    "                                                                 X_test=X_test_scaled,\n",
    "                                                                 y=y_tr,\n",
    "                                                                 folds=folds_models,\n",
    "                                                                 params=params_ex,\n",
    "                                                                 model_type='sklearn',\n",
    "                                                                 model=model,\n",
    "                                                                 show_scatter=True)\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ada Boost\n",
    "AdaBoost begins by fitting a base estimator on the original dataset and then fits additional copies on the same dataset. At each iteration (estimator), the weights of instances are adjusted according to the error of the last prediction. It's similar to the next model, but gradient boosting fits additional estimator copies on the current error and not on the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_ada = {\n",
    "    'learning_rate': 0.001\n",
    "}\n",
    "base = KernelRidge(kernel='rbf', alpha=2.4497346938775513, gamma=0.0018461224489795917)\n",
    "model = AdaBoostRegressor(base_estimator=base, n_estimators=100, **params_ada)\n",
    "oof_ada, prediction_ada, score_mean_ada, score_std_ada = train_model(X=X_train_scaled,\n",
    "                                                                     X_test=X_test_scaled,\n",
    "                                                                     y=y_tr,\n",
    "                                                                     folds=folds_models,\n",
    "                                                                     params=params_ada,\n",
    "                                                                     model_type='sklearn',\n",
    "                                                                     model=model,\n",
    "                                                                     show_scatter=True)\n",
    "del base\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NeuralNet\n",
    "This is a simple FC model mimiquing the previous genetic algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display training progress by printing a single dot for each completed epoch\n",
    "class PrintDot(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        if epoch % 100 == 0:\n",
    "            print('')\n",
    "        else:\n",
    "            print('.', end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    hist = pd.DataFrame(history.history)\n",
    "    hist['epoch'] = history.epoch\n",
    "  \n",
    "    plt.figure()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Mean Abs Error')\n",
    "    plt.plot(hist['epoch'], hist['mean_absolute_error'],\n",
    "             label='Train Error')\n",
    "    plt.plot(hist['epoch'], hist['val_mean_absolute_error'],\n",
    "             label = 'Val Error')\n",
    "    plt.ylim([0,5])\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2089 samples, validate on 2089 samples\n",
      "Epoch 1/1000\n",
      "1856/2089 [=========================>....] - ETA: 0s - loss: 3.8878 - mean_absolute_error: 3.8878\n",
      "2089/2089 [==============================] - 1s 341us/sample - loss: 3.8408 - mean_absolute_error: 3.8408 - val_loss: 2.3732 - val_mean_absolute_error: 2.3732\n",
      "Epoch 2/1000\n",
      "2089/2089 [==============================] - 0s 155us/sample - loss: 3.3488 - mean_absolute_error: 3.3488 - val_loss: 2.2817 - val_mean_absolute_error: 2.2817\n",
      "Epoch 3/1000\n",
      "2089/2089 [==============================] - 0s 156us/sample - loss: 3.2710 - mean_absolute_error: 3.2710 - val_loss: 2.6445 - val_mean_absolute_error: 2.6445\n",
      "Epoch 4/1000\n",
      "2089/2089 [==============================] - 0s 150us/sample - loss: 3.1756 - mean_absolute_error: 3.1756 - val_loss: 2.7095 - val_mean_absolute_error: 2.7095\n",
      "Epoch 5/1000\n",
      "2089/2089 [==============================] - 0s 155us/sample - loss: 3.0286 - mean_absolute_error: 3.0286 - val_loss: 2.3684 - val_mean_absolute_error: 2.3684\n",
      "Epoch 6/1000\n",
      "2089/2089 [==============================] - 0s 154us/sample - loss: 2.9917 - mean_absolute_error: 2.9917 - val_loss: 2.8141 - val_mean_absolute_error: 2.8141\n",
      "Epoch 7/1000\n",
      "2089/2089 [==============================] - 0s 154us/sample - loss: 2.9368 - mean_absolute_error: 2.9368 - val_loss: 2.6049 - val_mean_absolute_error: 2.6049\n",
      "Epoch 8/1000\n",
      "2089/2089 [==============================] - 0s 155us/sample - loss: 2.9074 - mean_absolute_error: 2.9074 - val_loss: 2.5866 - val_mean_absolute_error: 2.5866\n",
      "Epoch 9/1000\n",
      "2089/2089 [==============================] - 0s 154us/sample - loss: 2.8097 - mean_absolute_error: 2.8097 - val_loss: 2.5073 - val_mean_absolute_error: 2.5073\n",
      "Epoch 10/1000\n",
      "2089/2089 [==============================] - 0s 158us/sample - loss: 2.8455 - mean_absolute_error: 2.8455 - val_loss: 2.8247 - val_mean_absolute_error: 2.8247\n",
      "Epoch 11/1000\n",
      "2089/2089 [==============================] - 0s 150us/sample - loss: 2.7845 - mean_absolute_error: 2.7845 - val_loss: 2.6540 - val_mean_absolute_error: 2.6540\n",
      "Epoch 12/1000\n",
      "2089/2089 [==============================] - 0s 157us/sample - loss: 2.8371 - mean_absolute_error: 2.8371 - val_loss: 2.7937 - val_mean_absolute_error: 2.7937\n",
      "Epoch 13/1000\n",
      "2089/2089 [==============================] - 0s 150us/sample - loss: 2.7827 - mean_absolute_error: 2.7827 - val_loss: 2.6345 - val_mean_absolute_error: 2.6345\n",
      "Epoch 14/1000\n",
      "2089/2089 [==============================] - 0s 153us/sample - loss: 2.7076 - mean_absolute_error: 2.7076 - val_loss: 2.6571 - val_mean_absolute_error: 2.6571\n",
      "Epoch 15/1000\n",
      "2089/2089 [==============================] - 0s 154us/sample - loss: 2.7350 - mean_absolute_error: 2.7350 - val_loss: 2.2818 - val_mean_absolute_error: 2.2818\n",
      "Epoch 16/1000\n",
      "2089/2089 [==============================] - 0s 154us/sample - loss: 2.6425 - mean_absolute_error: 2.6425 - val_loss: 2.2438 - val_mean_absolute_error: 2.2438\n",
      "Epoch 17/1000\n",
      "2089/2089 [==============================] - 0s 159us/sample - loss: 2.6912 - mean_absolute_error: 2.6912 - val_loss: 2.1644 - val_mean_absolute_error: 2.1644\n",
      "Epoch 18/1000\n",
      "2089/2089 [==============================] - 0s 156us/sample - loss: 2.5802 - mean_absolute_error: 2.5802 - val_loss: 2.3291 - val_mean_absolute_error: 2.3290\n",
      "Epoch 19/1000\n",
      "2089/2089 [==============================] - 0s 153us/sample - loss: 2.6830 - mean_absolute_error: 2.6830 - val_loss: 2.2719 - val_mean_absolute_error: 2.2719\n",
      "Epoch 20/1000\n",
      "2089/2089 [==============================] - 0s 154us/sample - loss: 2.6052 - mean_absolute_error: 2.6052 - val_loss: 2.2842 - val_mean_absolute_error: 2.2842\n",
      "Epoch 21/1000\n",
      "2089/2089 [==============================] - 0s 154us/sample - loss: 2.5926 - mean_absolute_error: 2.5926 - val_loss: 2.1789 - val_mean_absolute_error: 2.1789\n",
      "Epoch 22/1000\n",
      "2089/2089 [==============================] - 0s 153us/sample - loss: 2.5978 - mean_absolute_error: 2.5978 - val_loss: 2.3577 - val_mean_absolute_error: 2.3577\n",
      "Epoch 23/1000\n",
      "2089/2089 [==============================] - 0s 153us/sample - loss: 2.6532 - mean_absolute_error: 2.6532 - val_loss: 2.0707 - val_mean_absolute_error: 2.0707\n",
      "Epoch 24/1000\n",
      "2089/2089 [==============================] - 0s 153us/sample - loss: 2.5905 - mean_absolute_error: 2.5905 - val_loss: 2.3379 - val_mean_absolute_error: 2.3379\n",
      "Epoch 25/1000\n",
      "2089/2089 [==============================] - 0s 153us/sample - loss: 2.5970 - mean_absolute_error: 2.5970 - val_loss: 2.3292 - val_mean_absolute_error: 2.3292\n",
      "Epoch 26/1000\n",
      "2089/2089 [==============================] - 0s 151us/sample - loss: 2.6011 - mean_absolute_error: 2.6011 - val_loss: 2.4644 - val_mean_absolute_error: 2.4644\n",
      "Epoch 27/1000\n",
      "2089/2089 [==============================] - 0s 153us/sample - loss: 2.5740 - mean_absolute_error: 2.5740 - val_loss: 2.3019 - val_mean_absolute_error: 2.3019\n",
      "Epoch 28/1000\n",
      "2089/2089 [==============================] - 0s 152us/sample - loss: 2.5423 - mean_absolute_error: 2.5423 - val_loss: 2.5216 - val_mean_absolute_error: 2.5216\n",
      "Epoch 29/1000\n",
      "2089/2089 [==============================] - 0s 154us/sample - loss: 2.5552 - mean_absolute_error: 2.5552 - val_loss: 2.2540 - val_mean_absolute_error: 2.2540\n",
      "Epoch 30/1000\n",
      "2089/2089 [==============================] - 0s 154us/sample - loss: 2.5519 - mean_absolute_error: 2.5519 - val_loss: 2.3519 - val_mean_absolute_error: 2.3519\n",
      "Epoch 31/1000\n",
      "2089/2089 [==============================] - 0s 153us/sample - loss: 2.5484 - mean_absolute_error: 2.5484 - val_loss: 2.2867 - val_mean_absolute_error: 2.2867\n",
      "Epoch 32/1000\n",
      "2089/2089 [==============================] - 0s 157us/sample - loss: 2.6014 - mean_absolute_error: 2.6014 - val_loss: 2.5038 - val_mean_absolute_error: 2.5038\n",
      "Epoch 33/1000\n",
      "2089/2089 [==============================] - 0s 154us/sample - loss: 2.4996 - mean_absolute_error: 2.4996 - val_loss: 2.1175 - val_mean_absolute_error: 2.1175\n",
      "Epoch 34/1000\n",
      "2089/2089 [==============================] - 0s 155us/sample - loss: 2.5383 - mean_absolute_error: 2.5383 - val_loss: 2.1178 - val_mean_absolute_error: 2.1178\n",
      "Epoch 35/1000\n",
      "2089/2089 [==============================] - 0s 157us/sample - loss: 2.5298 - mean_absolute_error: 2.5298 - val_loss: 2.1646 - val_mean_absolute_error: 2.1646\n",
      "Epoch 36/1000\n",
      "2089/2089 [==============================] - 0s 154us/sample - loss: 2.5496 - mean_absolute_error: 2.5496 - val_loss: 2.2823 - val_mean_absolute_error: 2.2823\n",
      "Epoch 37/1000\n",
      "2089/2089 [==============================] - 0s 157us/sample - loss: 2.5143 - mean_absolute_error: 2.5143 - val_loss: 2.0659 - val_mean_absolute_error: 2.0659\n",
      "Epoch 38/1000\n",
      " 480/2089 [=====>........................] - ETA: 0s - loss: 2.4600 - mean_absolute_error: 2.4600"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-d906cb0c4a75>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     callbacks=[early_stop, PrintDot()])\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0mhist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mval_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_mean_absolute_error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/user/toolse64/ubuntu/18.04/usr/local/tools/anaconda_2018.12/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    878\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m           validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/local/user/toolse64/ubuntu/18.04/usr/local/tools/anaconda_2018.12/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, mode, validation_in_fit, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/user/toolse64/ubuntu/18.04/usr/local/tools/anaconda_2018.12/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3075\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3076\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3077\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3078\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n",
      "\u001b[0;32m/local/user/toolse64/ubuntu/18.04/usr/local/tools/anaconda_2018.12/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dropout = 0.7\n",
    "num_layers = 2\n",
    "num_neurons = 128\n",
    "#with tf.device('/cpu:0'):\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(1024, input_dim=216, activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dropout(dropout))\n",
    "for l in range(num_layers):\n",
    "    model.add(tf.keras.layers.Dense(num_neurons, activation=tf.nn.relu))\n",
    "    model.add(tf.keras.layers.Dropout(dropout))\n",
    "model.add(tf.keras.layers.Dense(1))\n",
    "#model = tf.keras.utils.multi_gpu_model(model, gpus=1, cpu_relocation=True)\n",
    "model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
    "EPOCHS = 1000\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='mean_absolute_error', patience=100)\n",
    "history = model.fit(\n",
    "    X_train_scaled,\n",
    "    y_tr,\n",
    "    validation_split=0.5,\n",
    "    epochs=EPOCHS,\n",
    "    verbose=1,\n",
    "    callbacks=[early_stop, PrintDot()])\n",
    "hist = pd.DataFrame(history.history)\n",
    "val_score = hist['val_mean_absolute_error'].iloc[-1]\n",
    "print(f'val_score={val_score}')\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "oof_nn, prediction_nn, score_mean_nn, score_std_nn = train_model(X=X_train_scaled,\n",
    "                                                                 X_test=X_test_scaled,\n",
    "                                                                 y=y_tr,\n",
    "                                                                 folds=folds_models,\n",
    "                                                                 model_type='nn',\n",
    "                                                                 show_scatter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN\n",
    "This is a simple CNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking and blending\n",
    "And now let's try stacking :) We can use the same function for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stack = np.vstack([oof_lgb, oof_xgb, oof_svr, oof_r,\n",
    "                         oof_cat, oof_gdi, oof_rf, oof_ex,\n",
    "                         oof_ada]).transpose()\n",
    "train_stack = pd.DataFrame(train_stack, columns = ['lgb', 'xgb', 'svr', 'r', 'cat', 'gdi', 'rf', 'ex', 'ada'])\n",
    "test_stack = np.vstack([prediction_lgb, prediction_xgb, prediction_svr, prediction_r,\n",
    "                        prediction_cat, prediction_gdi, prediction_rf, prediction_ex,\n",
    "                        prediction_ada]).transpose()\n",
    "test_stack = pd.DataFrame(test_stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_lgb_stack = {\n",
    "    'objective': 'huber',\n",
    "    'boosting': 'gbdt',\n",
    "    'verbosity': -1,\n",
    "    'num_leaves': 8, #54,\n",
    "    'min_data_in_leaf': 100, #79,\n",
    "    'max_depth': 5, #-1,\n",
    "    'learning_rate': 0.01, #0.01,\n",
    "    'bagging_freq': 5, #5,\n",
    "    'bagging_fraction': 0.7555555555555555, #0.8126672064208567,\n",
    "    'bagging_seed': 11,\n",
    "    'metric': 'mae',\n",
    "    'reg_alpha': 0.95, #0.1302650970728192,\n",
    "    'reg_lambda': 0.19444444444444445 #0.3603427518866501\n",
    "}\n",
    "oof_lgb_stack, prediction_lgb_stack, score_mean_lgb_stack, score_std_lgb_stack, feature_importance_lgb_stack = train_model(\n",
    "    X=train_stack,\n",
    "    X_test=test_stack,\n",
    "    y=y_tr,\n",
    "    folds=folds_models,\n",
    "    params=params_lgb_stack,\n",
    "    model_type='lgb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = feature_importance_lgb_stack[['feature', 'importance']].groupby('feature').mean().sort_values(\n",
    "    by='importance', ascending=False).index\n",
    "best_features = feature_importance_lgb_stack.loc[feature_importance_lgb_stack.feature.isin(cols)]\n",
    "plt.figure(figsize=(16, 8));\n",
    "sns.barplot(x='importance', y='feature', data=best_features.sort_values(by='importance', ascending=False));\n",
    "plt.title('LGB Features (avg over folds)');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_blend = (oof_lgb + oof_xgb + oof_svr + oof_r + oof_cat + oof_gdi + oof_rf + oof_ex + oof_ada) / 9\n",
    "prediction_blend = (prediction_lgb + prediction_xgb + prediction_svr \n",
    "                    + prediction_cat + prediction_r + prediction_gdi\n",
    "                    + prediction_rf + prediction_ex + prediction_ada) / 9\n",
    "score_mean_blend = (score_mean_lgb + score_mean_xgb + score_mean_svr + score_mean_cat\n",
    "                    + score_mean_r + score_mean_gdi + score_mean_rf + score_mean_ex + score_mean_ada) / 9\n",
    "score_std_blend = (score_std_lgb + score_std_xgb + score_std_svr + score_std_cat\n",
    "                    + score_std_r + score_std_gdi + score_std_rf + score_std_ex + score_std_ada) / 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = pd.DataFrame(dtype=np.float64)\n",
    "submission = pd.read_csv('../input/sample_submission.csv', index_col='seg_id')\n",
    "my_methods = ['lgb', 'xgb', 'svr', 'r', 'cat', 'gdi', 'rf', 'ex', 'ada', 'lgb_stack', 'blend']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for method in my_methods:\n",
    "    submission['time_to_failure'] = globals()['prediction_' + method]\n",
    "    submission.to_csv('../output/submission_' + method + '.csv')\n",
    "    evaluation.loc[method, 'mean'] = globals()['score_mean_' + method]\n",
    "    evaluation.loc[method, 'std'] = globals()['score_std_' + method]\n",
    "evaluation.to_csv(f\"../output/evaluation_{str(datetime.datetime.now())}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_methods(evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
